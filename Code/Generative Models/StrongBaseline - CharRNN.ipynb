{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["##Importing necessary libraries"],"metadata":{"id":"ySRi7JXyrNf5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohBMM5TJD1cU"},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F"]},{"cell_type":"markdown","source":["Mounting Google Drive"],"metadata":{"id":"wJ53NrQhrJiv"}},{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0gb1nr7v993","executionInfo":{"status":"ok","timestamp":1669394989937,"user_tz":300,"elapsed":28269,"user":{"displayName":"Namita Shukla","userId":"15938957227034652948"}},"outputId":"36a64ed9-445f-40b6-dddf-43928388f0b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["##Data Formatting - Inputting just slash separated haikus for our Char RNN"],"metadata":{"id":"OIEPds7gzdrt"}},{"cell_type":"code","source":["# importing library\n","import pandas as pd\n","  \n","# Then loading csv file\n","df = pd.read_csv('/content/drive/MyDrive/CIS530-Project/Data/haikus.csv')"],"metadata":{"id":"dy_v0Y0pvuhX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"_trrTwopwX9p","executionInfo":{"status":"ok","timestamp":1669312398537,"user_tz":300,"elapsed":6,"user":{"displayName":"Namita Shukla","userId":"02059142805893535133"}},"outputId":"c77b64e6-f122-4765-af20-d6247a0ed295"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                         0                                 1  \\\n","0          Memorial Day --                 a shadow for each   \n","1            spring rain -              as the doctor speaks   \n","2        spring moonset --                   a rice ball for   \n","3          sunny afternoon                an old man lingers   \n","4            cinco de mayo                       horses roll   \n","...                    ...                               ...   \n","143132  I'm not asking did            you say it nor clarify   \n","143133     You are truly a               moron or a liar I'm   \n","143134  Ain't no selfie on   this earth that's gonna make me   \n","143135    is doing a great          job turning Independents   \n","143136    Wanted to send a         quick follow up on if the   \n","\n","                              2       source 0_syllables 1_syllables  \\\n","0                   white cross  tempslibres           5           5   \n","1             i think of lilacs  tempslibres         2,3           5   \n","2                     breakfast  tempslibres         3,4           4   \n","3              near the mailbox  tempslibres           5           5   \n","4               in the shallows  tempslibres           5           3   \n","...                         ...          ...         ...         ...   \n","143132    what you said neither       twaiku           5           7   \n","143133   inclined to think both       twaiku           5           7   \n","143134         like Theresa May       twaiku           5           7   \n","143135           into Democrats       twaiku           5           7   \n","143136  blood is loud Talk soon       twaiku           5           7   \n","\n","       2_syllables  \n","0                2  \n","1                5  \n","2                2  \n","3                4  \n","4                4  \n","...            ...  \n","143132           5  \n","143133           5  \n","143134           5  \n","143135           5  \n","143136           5  \n","\n","[143137 rows x 7 columns]"],"text/html":["\n","  <div id=\"df-3f85d3e9-59ec-4c9e-b749-1977ed4306ea\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>source</th>\n","      <th>0_syllables</th>\n","      <th>1_syllables</th>\n","      <th>2_syllables</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Memorial Day --</td>\n","      <td>a shadow for each</td>\n","      <td>white cross</td>\n","      <td>tempslibres</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>spring rain -</td>\n","      <td>as the doctor speaks</td>\n","      <td>i think of lilacs</td>\n","      <td>tempslibres</td>\n","      <td>2,3</td>\n","      <td>5</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>spring moonset --</td>\n","      <td>a rice ball for</td>\n","      <td>breakfast</td>\n","      <td>tempslibres</td>\n","      <td>3,4</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sunny afternoon</td>\n","      <td>an old man lingers</td>\n","      <td>near the mailbox</td>\n","      <td>tempslibres</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>cinco de mayo</td>\n","      <td>horses roll</td>\n","      <td>in the shallows</td>\n","      <td>tempslibres</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>143132</th>\n","      <td>I'm not asking did</td>\n","      <td>you say it nor clarify</td>\n","      <td>what you said neither</td>\n","      <td>twaiku</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>143133</th>\n","      <td>You are truly a</td>\n","      <td>moron or a liar I'm</td>\n","      <td>inclined to think both</td>\n","      <td>twaiku</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>143134</th>\n","      <td>Ain't no selfie on</td>\n","      <td>this earth that's gonna make me</td>\n","      <td>like Theresa May</td>\n","      <td>twaiku</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>143135</th>\n","      <td>is doing a great</td>\n","      <td>job turning Independents</td>\n","      <td>into Democrats</td>\n","      <td>twaiku</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>143136</th>\n","      <td>Wanted to send a</td>\n","      <td>quick follow up on if the</td>\n","      <td>blood is loud Talk soon</td>\n","      <td>twaiku</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>143137 rows × 7 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f85d3e9-59ec-4c9e-b749-1977ed4306ea')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3f85d3e9-59ec-4c9e-b749-1977ed4306ea button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3f85d3e9-59ec-4c9e-b749-1977ed4306ea');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["cols = ['0','1','2']\n","newcol = ['/'.join(i) for i in df[cols].astype(str).values]\n","df = df.assign(haiku=newcol).drop(cols, 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JxI2QNEOwkyu","executionInfo":{"status":"ok","timestamp":1669312401544,"user_tz":300,"elapsed":677,"user":{"displayName":"Namita Shukla","userId":"02059142805893535133"}},"outputId":"f9ca24de-3730-4723-cf84-10c233c63b67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]}]},{"cell_type":"code","source":["print(df['haiku'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fUfBiwevxOeJ","executionInfo":{"status":"ok","timestamp":1669312475527,"user_tz":300,"elapsed":312,"user":{"displayName":"Namita Shukla","userId":"02059142805893535133"}},"outputId":"f3c78928-5e04-42e2-831d-445aff5faaac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Memorial Day --/a shadow for each/white cross$\n"]}]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"LoNgKsHqxgNk","executionInfo":{"status":"ok","timestamp":1669312417311,"user_tz":300,"elapsed":333,"user":{"displayName":"Namita Shukla","userId":"02059142805893535133"}},"outputId":"7ccecf5e-4a85-46d4-d00c-29a7ccda76e6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             source 0_syllables 1_syllables 2_syllables  \\\n","0       tempslibres           5           5           2   \n","1       tempslibres         2,3           5           5   \n","2       tempslibres         3,4           4           2   \n","3       tempslibres           5           5           4   \n","4       tempslibres           5           3           4   \n","...             ...         ...         ...         ...   \n","143132       twaiku           5           7           5   \n","143133       twaiku           5           7           5   \n","143134       twaiku           5           7           5   \n","143135       twaiku           5           7           5   \n","143136       twaiku           5           7           5   \n","\n","                                                    haiku  \n","0           Memorial Day --/a shadow for each/white cross  \n","1       spring rain -/as the doctor speaks/i think of ...  \n","2             spring moonset --/a rice ball for/breakfast  \n","3       sunny afternoon/an old man lingers/near the ma...  \n","4               cinco de mayo/horses roll/in the shallows  \n","...                                                   ...  \n","143132  I'm not asking did/ you say it nor clarify/wha...  \n","143133  You are truly a/ moron or a liar I'm/inclined ...  \n","143134  Ain't no selfie on/ this earth that's gonna ma...  \n","143135  is doing a great/ job turning Independents/int...  \n","143136  Wanted to send a/ quick follow up on if the/bl...  \n","\n","[143137 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-1de99823-55c8-46d6-b2e0-9e27bfd068ab\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>0_syllables</th>\n","      <th>1_syllables</th>\n","      <th>2_syllables</th>\n","      <th>haiku</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tempslibres</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>Memorial Day --/a shadow for each/white cross</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>tempslibres</td>\n","      <td>2,3</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>spring rain -/as the doctor speaks/i think of ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>tempslibres</td>\n","      <td>3,4</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>spring moonset --/a rice ball for/breakfast</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>tempslibres</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>sunny afternoon/an old man lingers/near the ma...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>tempslibres</td>\n","      <td>5</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>cinco de mayo/horses roll/in the shallows</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>143132</th>\n","      <td>twaiku</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>I'm not asking did/ you say it nor clarify/wha...</td>\n","    </tr>\n","    <tr>\n","      <th>143133</th>\n","      <td>twaiku</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>You are truly a/ moron or a liar I'm/inclined ...</td>\n","    </tr>\n","    <tr>\n","      <th>143134</th>\n","      <td>twaiku</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>Ain't no selfie on/ this earth that's gonna ma...</td>\n","    </tr>\n","    <tr>\n","      <th>143135</th>\n","      <td>twaiku</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>is doing a great/ job turning Independents/int...</td>\n","    </tr>\n","    <tr>\n","      <th>143136</th>\n","      <td>twaiku</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>Wanted to send a/ quick follow up on if the/bl...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>143137 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1de99823-55c8-46d6-b2e0-9e27bfd068ab')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1de99823-55c8-46d6-b2e0-9e27bfd068ab button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1de99823-55c8-46d6-b2e0-9e27bfd068ab');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["df['haiku'] = df['haiku'] + '$'"],"metadata":{"id":"LgMaCmS_xo-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['haiku'].to_csv(r'/content/just_haikus.txt', header=None, index=None, sep='\\n', mode='a')"],"metadata":{"id":"Cc0dIAMsEaI6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Combining our two datasets into /$ format"],"metadata":{"id":"V_T51Ow_2E_t"}},{"cell_type":"code","source":["data = data2 = \"\"\n","  \n","# Reading data from file1\n","with open('/content/drive/MyDrive/CIS530-Project/Data/just_haikus.txt') as fp:\n","    data = fp.read()\n","  \n","# Reading data from file2\n","with open('/content/drive/MyDrive/CIS530-Project/Data/lines.txt') as fp:\n","    data2 = fp.read()\n","  \n","# Merging 2 files\n","# To add the data of file2\n","# from next line\n","data += \"\\n\"\n","data += data2\n","  \n","with open ('/content/drive/MyDrive/CIS530-Project/Data/combined_just_haikus.txt', 'w') as fp:\n","    fp.write(data)"],"metadata":{"id":"esbPE7B3zrT2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CHAR RNN  - main implementation starts from here"],"metadata":{"id":"A4uuSg-z5cWz"}},{"cell_type":"code","source":["with open('/content/drive/MyDrive/CIS530-Project/Data/combined_just_haikus.txt', 'r') as f:\n","    text = f.read()"],"metadata":{"id":"WBuMJTwitUIU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text[0:200]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"JBIiNdJO03Gn","executionInfo":{"status":"ok","timestamp":1669395023392,"user_tz":300,"elapsed":370,"user":{"displayName":"Namita Shukla","userId":"15938957227034652948"}},"outputId":"a5c4f471-c180-455b-e459-eae2c836a2ba"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Memorial Day --/a shadow for each/white cross$\\nspring rain -/as the doctor speaks/i think of lilacs$\\nspring moonset --/a rice ball for/breakfast$\\nsunny afternoon/an old man lingers/near the mailbox$\\nc'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# encode the text and map each character to an integer and vice versa\n","# 1. int2char, which maps integers to characters\n","# 2. char2int, which maps characters to unique integers\n","chars = tuple(set(text))\n","int2char = dict(enumerate(chars))\n","char2int = {ch: ii for ii, ch in int2char.items()}\n","# encode the text\n","encoded = np.array([char2int[ch] for ch in text])"],"metadata":{"id":"Kw85zpspEwPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def one_hot_encode(arr, n_labels):\n","    \n","    # Initialize the the encoded array\n","    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n","    \n","    # Fill the appropriate elements with ones\n","    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n","    \n","    # Finally reshape it to get back to the original array\n","    one_hot = one_hot.reshape((*arr.shape, n_labels))\n","    \n","    return one_hot"],"metadata":{"id":"H4soJfeUEy8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check that the function works as expected\n","test_seq = np.array([[0, 5, 1]])\n","one_hot = one_hot_encode(test_seq, 8)\n","print(one_hot)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JyHYaBIjE05C","executionInfo":{"status":"ok","timestamp":1669395054285,"user_tz":300,"elapsed":5,"user":{"displayName":"Namita Shukla","userId":"15938957227034652948"}},"outputId":"5e128824-a264-48c6-d38e-b8bb7a6c69b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0. 0.]\n","  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"]}]},{"cell_type":"code","source":["def get_batches(arr, batch_size, seq_length):\n","    '''Create a generator that returns batches of size\n","       batch_size x seq_length from arr.\n","       \n","       Arguments\n","       ---------\n","       arr: Array you want to make batches from\n","       batch_size: Batch size, the number of sequences per batch\n","       seq_length: Number of encoded chars in a sequence\n","    '''\n","    \n","    batch_size_total = batch_size * seq_length\n","    # total number of batches we can make\n","    n_batches = len(arr)//batch_size_total\n","    \n","    # Keep only enough characters to make full batches\n","    arr = arr[:n_batches * batch_size_total]\n","    # Reshape into batch_size rows\n","    arr = arr.reshape((batch_size, -1))\n","    \n","    # iterate through the array, one sequence at a time\n","    for n in range(0, arr.shape[1], seq_length):\n","        # The features\n","        x = arr[:, n:n+seq_length]\n","        # The targets, shifted by one\n","        y = np.zeros_like(x)\n","        try:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n","        except IndexError:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n","        yield x, y"],"metadata":{"id":"IPcNO5whE3o_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check if GPU is available\n","train_on_gpu = torch.cuda.is_available()\n","if(train_on_gpu):\n","    print('Training on GPU!')\n","else: \n","    print('No GPU available, training on CPU; consider making n_epochs very small.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CU6iwH-rE6KN","executionInfo":{"status":"ok","timestamp":1669395060333,"user_tz":300,"elapsed":582,"user":{"displayName":"Namita Shukla","userId":"15938957227034652948"}},"outputId":"373a869b-b5c4-43e9-ac18-768389c4954f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on GPU!\n"]}]},{"cell_type":"code","source":["class CharRNN(nn.Module):\n","    \n","    def __init__(self, tokens, n_hidden=256, n_layers=2,\n","                               drop_prob=0.5, lr=0.001):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.n_layers = n_layers\n","        self.n_hidden = n_hidden\n","        self.lr = lr\n","        \n","        # creating character dictionaries\n","        self.chars = tokens\n","        self.int2char = dict(enumerate(self.chars))\n","        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n","        \n","        #lstm layer - here number of characters is the embedding size as the input is one hot encoded\n","        self.lstm=nn.LSTM(len(self.chars),n_hidden,n_layers,\n","                          dropout=drop_prob,batch_first=True)\n","        \n","        #dropout layer\n","        self.dropout=nn.Dropout(drop_prob)\n","        \n","        #output layer\n","        self.fc=nn.Linear(n_hidden,len(self.chars))\n","    \n","    def forward(self, x, hidden):\n","        ''' Forward pass through the network. \n","            These inputs are x, and the hidden/cell state `hidden`. '''\n","        ## Get the outputs and the new hidden state from the lstm\n","        r_output, hidden = self.lstm(x, hidden)\n","        \n","        ## pass through a dropout layer\n","        out = self.dropout(r_output)\n","        \n","        # Stack up LSTM outputs using view\n","        # you may need to use contiguous to reshape the output\n","        out = out.contiguous().view(-1, self.n_hidden)\n","        \n","        ## put x through the fully-connected layer\n","        out = self.fc(out)\n","        return out, hidden\n","    \n","    \n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n","        \n","        return hidden"],"metadata":{"id":"aDzScwjPE9o3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n","    ''' Training a network \n","    \n","        Arguments\n","        ---------\n","        \n","        net: CharRNN network\n","        data: text data to train the network\n","        epochs: Number of epochs to train\n","        batch_size: Number of mini-sequences per mini-batch, aka batch size\n","        seq_length: Number of character steps per mini-batch\n","        lr: learning rate\n","        clip: gradient clipping\n","        val_frac: Fraction of data to hold out for validation\n","        print_every: Number of steps for printing training and validation loss\n","    \n","    '''\n","    net.train()\n","    \n","    opt = torch.optim.Adam(net.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","    \n","    # create training and validation data\n","    val_idx = int(len(data)*(1-val_frac))\n","    data, val_data = data[:val_idx], data[val_idx:]\n","    \n","    if(train_on_gpu):\n","        net.cuda()\n","    \n","    counter = 0\n","    n_chars = len(net.chars)\n","    for e in range(epochs):\n","        # initialize hidden state\n","        h = net.init_hidden(batch_size)\n","        \n","        for x, y in get_batches(data, batch_size, seq_length):\n","            counter += 1\n","            \n","            # One-hot encode our data and make them Torch tensors\n","            x = one_hot_encode(x, n_chars)\n","            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n","            \n","            if(train_on_gpu):\n","                inputs, targets = inputs.cuda(), targets.cuda()\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            h = tuple([each.data for each in h])\n","            # zero accumulated gradients\n","            net.zero_grad()\n","            \n","            # get the output from the model\n","            output, h = net(inputs, h)\n","            \n","            # calculate the loss and perform backprop\n","            loss = criterion(output, targets.view(batch_size*seq_length).long())\n","            loss.backward()\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(net.parameters(), clip)\n","            opt.step()\n","            \n","            # loss stats\n","            if counter % print_every == 0:\n","                # Get validation loss\n","                val_h = net.init_hidden(batch_size)\n","                val_losses = []\n","                net.eval()\n","                for x, y in get_batches(val_data, batch_size, seq_length):\n","                    # One-hot encode our data and make them Torch tensors\n","                    x = one_hot_encode(x, n_chars)\n","                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n","                    \n","                    # Creating new variables for the hidden state, otherwise\n","                    # we'd backprop through the entire training history\n","                    val_h = tuple([each.data for each in val_h])\n","                    \n","                    inputs, targets = x, y\n","                    if(train_on_gpu):\n","                        inputs, targets = inputs.cuda(), targets.cuda()\n","                    output, val_h = net(inputs, val_h)\n","                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n","                \n","                    val_losses.append(val_loss.item())\n","                \n","                net.train() # reset to train mode after iterationg through validation data\n","                \n","                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                      \"Step: {}...\".format(counter),\n","                      \"Loss: {:.4f}...\".format(loss.item()),\n","                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"],"metadata":{"id":"VnHUu8WBFBtC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define and print the net\n","n_hidden = 512\n","n_layers = 2\n","net = CharRNN(chars, n_hidden, n_layers)\n","print(net)\n","batch_size = 128\n","seq_length = 100\n","n_epochs =  30\n","# train the model\n","train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nCyURUXyFBzg","outputId":"e0797ad6-376d-4622-b11f-650ceb100f14"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CharRNN(\n","  (lstm): LSTM(117, 512, num_layers=2, batch_first=True, dropout=0.5)\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=512, out_features=117, bias=True)\n",")\n","Epoch: 1/30... Step: 10... Loss: 3.3365... Val Loss: 3.1073\n","Epoch: 1/30... Step: 20... Loss: 3.2583... Val Loss: 3.0633\n","Epoch: 1/30... Step: 30... Loss: 3.2214... Val Loss: 3.0335\n","Epoch: 1/30... Step: 40... Loss: 3.2389... Val Loss: 3.0469\n","Epoch: 1/30... Step: 50... Loss: 3.1966... Val Loss: 3.0409\n","Epoch: 1/30... Step: 60... Loss: 3.1918... Val Loss: 3.0351\n","Epoch: 1/30... Step: 70... Loss: 3.1912... Val Loss: 3.0389\n","Epoch: 1/30... Step: 80... Loss: 3.1650... Val Loss: 3.0346\n","Epoch: 1/30... Step: 90... Loss: 3.1890... Val Loss: 3.0324\n","Epoch: 1/30... Step: 100... Loss: 3.1854... Val Loss: 3.0297\n","Epoch: 1/30... Step: 110... Loss: 3.1476... Val Loss: 3.0094\n","Epoch: 1/30... Step: 120... Loss: 3.1327... Val Loss: 2.9843\n","Epoch: 1/30... Step: 130... Loss: 3.0531... Val Loss: 2.9450\n","Epoch: 1/30... Step: 140... Loss: 3.0071... Val Loss: 2.9075\n","Epoch: 1/30... Step: 150... Loss: 2.9284... Val Loss: 2.8399\n","Epoch: 1/30... Step: 160... Loss: 2.8283... Val Loss: 2.8141\n","Epoch: 1/30... Step: 170... Loss: 2.7425... Val Loss: 2.7762\n","Epoch: 1/30... Step: 180... Loss: 2.6866... Val Loss: 2.7054\n","Epoch: 1/30... Step: 190... Loss: 2.6643... Val Loss: 2.6779\n","Epoch: 1/30... Step: 200... Loss: 2.6159... Val Loss: 2.6700\n","Epoch: 1/30... Step: 210... Loss: 2.5742... Val Loss: 2.6423\n","Epoch: 1/30... Step: 220... Loss: 2.5256... Val Loss: 2.6293\n","Epoch: 1/30... Step: 230... Loss: 2.5451... Val Loss: 2.6361\n","Epoch: 1/30... Step: 240... Loss: 2.5106... Val Loss: 2.5887\n","Epoch: 1/30... Step: 250... Loss: 2.4885... Val Loss: 2.5625\n","Epoch: 1/30... Step: 260... Loss: 2.4598... Val Loss: 2.5562\n","Epoch: 1/30... Step: 270... Loss: 2.4509... Val Loss: 2.5337\n","Epoch: 1/30... Step: 280... Loss: 2.4320... Val Loss: 2.5188\n","Epoch: 1/30... Step: 290... Loss: 2.3964... Val Loss: 2.5183\n","Epoch: 1/30... Step: 300... Loss: 2.3957... Val Loss: 2.4989\n","Epoch: 1/30... Step: 310... Loss: 2.3632... Val Loss: 2.4683\n","Epoch: 1/30... Step: 320... Loss: 2.3280... Val Loss: 2.4492\n","Epoch: 1/30... Step: 330... Loss: 2.3294... Val Loss: 2.4469\n","Epoch: 1/30... Step: 340... Loss: 2.2924... Val Loss: 2.4266\n","Epoch: 1/30... Step: 350... Loss: 2.2956... Val Loss: 2.4181\n","Epoch: 1/30... Step: 360... Loss: 2.2979... Val Loss: 2.4122\n","Epoch: 1/30... Step: 370... Loss: 2.2510... Val Loss: 2.4048\n","Epoch: 1/30... Step: 380... Loss: 2.2469... Val Loss: 2.3941\n","Epoch: 1/30... Step: 390... Loss: 2.2432... Val Loss: 2.3751\n","Epoch: 1/30... Step: 400... Loss: 2.2209... Val Loss: 2.3660\n","Epoch: 1/30... Step: 410... Loss: 2.2249... Val Loss: 2.3554\n","Epoch: 1/30... Step: 420... Loss: 2.2130... Val Loss: 2.3495\n","Epoch: 1/30... Step: 430... Loss: 2.1849... Val Loss: 2.3397\n","Epoch: 1/30... Step: 440... Loss: 2.1834... Val Loss: 2.3346\n","Epoch: 1/30... Step: 450... Loss: 2.1708... Val Loss: 2.3287\n","Epoch: 1/30... Step: 460... Loss: 2.1735... Val Loss: 2.3148\n","Epoch: 1/30... Step: 470... Loss: 2.1554... Val Loss: 2.3064\n","Epoch: 1/30... Step: 480... Loss: 2.1249... Val Loss: 2.2887\n","Epoch: 1/30... Step: 490... Loss: 2.1027... Val Loss: 2.2868\n","Epoch: 1/30... Step: 500... Loss: 2.1119... Val Loss: 2.2716\n","Epoch: 1/30... Step: 510... Loss: 2.0516... Val Loss: 2.2817\n","Epoch: 1/30... Step: 520... Loss: 2.0867... Val Loss: 2.2706\n","Epoch: 1/30... Step: 530... Loss: 2.0956... Val Loss: 2.2571\n","Epoch: 1/30... Step: 540... Loss: 2.0732... Val Loss: 2.2609\n","Epoch: 1/30... Step: 550... Loss: 2.0692... Val Loss: 2.2530\n","Epoch: 1/30... Step: 560... Loss: 2.0479... Val Loss: 2.2518\n","Epoch: 1/30... Step: 570... Loss: 2.0472... Val Loss: 2.2312\n","Epoch: 1/30... Step: 580... Loss: 2.0575... Val Loss: 2.2372\n","Epoch: 1/30... Step: 590... Loss: 2.0202... Val Loss: 2.2139\n","Epoch: 1/30... Step: 600... Loss: 2.0391... Val Loss: 2.2195\n","Epoch: 1/30... Step: 610... Loss: 2.0290... Val Loss: 2.2134\n","Epoch: 1/30... Step: 620... Loss: 2.0182... Val Loss: 2.1914\n","Epoch: 1/30... Step: 630... Loss: 1.9860... Val Loss: 2.2071\n","Epoch: 1/30... Step: 640... Loss: 1.9947... Val Loss: 2.2027\n","Epoch: 1/30... Step: 650... Loss: 2.0005... Val Loss: 2.1916\n","Epoch: 1/30... Step: 660... Loss: 1.9738... Val Loss: 2.1665\n","Epoch: 1/30... Step: 670... Loss: 1.9873... Val Loss: 2.1673\n","Epoch: 1/30... Step: 680... Loss: 1.9601... Val Loss: 2.1656\n","Epoch: 1/30... Step: 690... Loss: 1.9576... Val Loss: 2.1605\n","Epoch: 1/30... Step: 700... Loss: 1.9442... Val Loss: 2.1442\n","Epoch: 1/30... Step: 710... Loss: 1.9406... Val Loss: 2.1343\n","Epoch: 1/30... Step: 720... Loss: 1.9477... Val Loss: 2.1257\n","Epoch: 1/30... Step: 730... Loss: 1.9284... Val Loss: 2.1241\n","Epoch: 1/30... Step: 740... Loss: 1.9184... Val Loss: 2.1221\n","Epoch: 2/30... Step: 750... Loss: 1.9396... Val Loss: 2.1668\n","Epoch: 2/30... Step: 760... Loss: 1.9003... Val Loss: 2.1117\n","Epoch: 2/30... Step: 770... Loss: 1.9375... Val Loss: 2.0987\n","Epoch: 2/30... Step: 780... Loss: 1.9347... Val Loss: 2.0986\n","Epoch: 2/30... Step: 790... Loss: 1.8988... Val Loss: 2.0974\n","Epoch: 2/30... Step: 800... Loss: 1.8954... Val Loss: 2.0956\n","Epoch: 2/30... Step: 810... Loss: 1.8890... Val Loss: 2.0996\n","Epoch: 2/30... Step: 820... Loss: 1.9026... Val Loss: 2.0882\n","Epoch: 2/30... Step: 830... Loss: 1.8617... Val Loss: 2.0840\n","Epoch: 2/30... Step: 840... Loss: 1.8809... Val Loss: 2.0773\n","Epoch: 2/30... Step: 850... Loss: 1.8686... Val Loss: 2.0603\n","Epoch: 2/30... Step: 860... Loss: 1.8230... Val Loss: 2.0630\n","Epoch: 2/30... Step: 870... Loss: 1.8722... Val Loss: 2.0498\n","Epoch: 2/30... Step: 880... Loss: 1.8327... Val Loss: 2.0380\n","Epoch: 2/30... Step: 890... Loss: 1.8569... Val Loss: 2.0564\n","Epoch: 2/30... Step: 900... Loss: 1.8007... Val Loss: 2.0515\n","Epoch: 2/30... Step: 910... Loss: 1.8401... Val Loss: 2.0537\n","Epoch: 2/30... Step: 920... Loss: 1.8350... Val Loss: 2.0380\n","Epoch: 2/30... Step: 930... Loss: 1.7941... Val Loss: 2.0239\n","Epoch: 2/30... Step: 940... Loss: 1.8150... Val Loss: 2.0391\n","Epoch: 2/30... Step: 950... Loss: 1.8347... Val Loss: 2.0310\n","Epoch: 2/30... Step: 960... Loss: 1.8182... Val Loss: 2.0382\n","Epoch: 2/30... Step: 970... Loss: 1.7901... Val Loss: 2.0371\n","Epoch: 2/30... Step: 980... Loss: 1.8012... Val Loss: 2.0023\n","Epoch: 2/30... Step: 990... Loss: 1.7687... Val Loss: 2.0138\n","Epoch: 2/30... Step: 1000... Loss: 1.8185... Val Loss: 2.0131\n","Epoch: 2/30... Step: 1010... Loss: 1.7833... Val Loss: 2.0094\n","Epoch: 2/30... Step: 1020... Loss: 1.7983... Val Loss: 2.0101\n","Epoch: 2/30... Step: 1030... Loss: 1.7931... Val Loss: 2.0003\n","Epoch: 2/30... Step: 1040... Loss: 1.8104... Val Loss: 1.9978\n","Epoch: 2/30... Step: 1050... Loss: 1.7675... Val Loss: 1.9952\n","Epoch: 2/30... Step: 1060... Loss: 1.8002... Val Loss: 1.9917\n","Epoch: 2/30... Step: 1070... Loss: 1.7744... Val Loss: 1.9946\n","Epoch: 2/30... Step: 1080... Loss: 1.7479... Val Loss: 1.9871\n","Epoch: 2/30... Step: 1090... Loss: 1.7553... Val Loss: 1.9692\n","Epoch: 2/30... Step: 1100... Loss: 1.7533... Val Loss: 1.9806\n","Epoch: 2/30... Step: 1110... Loss: 1.7805... Val Loss: 1.9794\n","Epoch: 2/30... Step: 1120... Loss: 1.7376... Val Loss: 1.9729\n","Epoch: 2/30... Step: 1130... Loss: 1.7599... Val Loss: 1.9683\n","Epoch: 2/30... Step: 1140... Loss: 1.7480... Val Loss: 1.9639\n","Epoch: 2/30... Step: 1150... Loss: 1.7388... Val Loss: 1.9575\n","Epoch: 2/30... Step: 1160... Loss: 1.7278... Val Loss: 1.9530\n","Epoch: 2/30... Step: 1170... Loss: 1.7328... Val Loss: 1.9625\n","Epoch: 2/30... Step: 1180... Loss: 1.7311... Val Loss: 1.9515\n","Epoch: 2/30... Step: 1190... Loss: 1.7227... Val Loss: 1.9555\n","Epoch: 2/30... Step: 1200... Loss: 1.7450... Val Loss: 1.9448\n","Epoch: 2/30... Step: 1210... Loss: 1.7352... Val Loss: 1.9444\n","Epoch: 2/30... Step: 1220... Loss: 1.6688... Val Loss: 1.9436\n","Epoch: 2/30... Step: 1230... Loss: 1.7412... Val Loss: 1.9507\n","Epoch: 2/30... Step: 1240... Loss: 1.7157... Val Loss: 1.9415\n","Epoch: 2/30... Step: 1250... Loss: 1.7393... Val Loss: 1.9297\n","Epoch: 2/30... Step: 1260... Loss: 1.7226... Val Loss: 1.9372\n","Epoch: 2/30... Step: 1270... Loss: 1.6847... Val Loss: 1.9389\n","Epoch: 2/30... Step: 1280... Loss: 1.6977... Val Loss: 1.9493\n","Epoch: 2/30... Step: 1290... Loss: 1.6948... Val Loss: 1.9427\n","Epoch: 2/30... Step: 1300... Loss: 1.6668... Val Loss: 1.9268\n","Epoch: 2/30... Step: 1310... Loss: 1.6591... Val Loss: 1.9283\n","Epoch: 2/30... Step: 1320... Loss: 1.7312... Val Loss: 1.9200\n","Epoch: 2/30... Step: 1330... Loss: 1.6594... Val Loss: 1.9266\n","Epoch: 2/30... Step: 1340... Loss: 1.7047... Val Loss: 1.9250\n","Epoch: 2/30... Step: 1350... Loss: 1.6790... Val Loss: 1.9332\n","Epoch: 2/30... Step: 1360... Loss: 1.6820... Val Loss: 1.9263\n","Epoch: 2/30... Step: 1370... Loss: 1.6775... Val Loss: 1.9329\n","Epoch: 2/30... Step: 1380... Loss: 1.6807... Val Loss: 1.9268\n","Epoch: 2/30... Step: 1390... Loss: 1.6854... Val Loss: 1.9167\n","Epoch: 2/30... Step: 1400... Loss: 1.6584... Val Loss: 1.9302\n","Epoch: 2/30... Step: 1410... Loss: 1.6504... Val Loss: 1.9116\n","Epoch: 2/30... Step: 1420... Loss: 1.6729... Val Loss: 1.9270\n","Epoch: 2/30... Step: 1430... Loss: 1.6548... Val Loss: 1.9306\n","Epoch: 2/30... Step: 1440... Loss: 1.6746... Val Loss: 1.9194\n","Epoch: 2/30... Step: 1450... Loss: 1.6148... Val Loss: 1.9130\n","Epoch: 2/30... Step: 1460... Loss: 1.6714... Val Loss: 1.9146\n","Epoch: 2/30... Step: 1470... Loss: 1.6578... Val Loss: 1.9089\n","Epoch: 2/30... Step: 1480... Loss: 1.6578... Val Loss: 1.9044\n","Epoch: 2/30... Step: 1490... Loss: 1.6508... Val Loss: 1.9122\n","Epoch: 3/30... Step: 1500... Loss: 1.7170... Val Loss: 1.9202\n","Epoch: 3/30... Step: 1510... Loss: 1.7150... Val Loss: 1.9043\n","Epoch: 3/30... Step: 1520... Loss: 1.6426... Val Loss: 1.9020\n","Epoch: 3/30... Step: 1530... Loss: 1.6773... Val Loss: 1.8842\n","Epoch: 3/30... Step: 1540... Loss: 1.6953... Val Loss: 1.8855\n","Epoch: 3/30... Step: 1550... Loss: 1.6532... Val Loss: 1.8780\n","Epoch: 3/30... Step: 1560... Loss: 1.6413... Val Loss: 1.8793\n","Epoch: 3/30... Step: 1570... Loss: 1.6586... Val Loss: 1.8759\n","Epoch: 3/30... Step: 1580... Loss: 1.6588... Val Loss: 1.8718\n","Epoch: 3/30... Step: 1590... Loss: 1.6404... Val Loss: 1.8714\n","Epoch: 3/30... Step: 1600... Loss: 1.6661... Val Loss: 1.8848\n","Epoch: 3/30... Step: 1610... Loss: 1.6320... Val Loss: 1.8762\n","Epoch: 3/30... Step: 1620... Loss: 1.6520... Val Loss: 1.8742\n","Epoch: 3/30... Step: 1630... Loss: 1.6531... Val Loss: 1.8700\n","Epoch: 3/30... Step: 1640... Loss: 1.6232... Val Loss: 1.8644\n","Epoch: 3/30... Step: 1650... Loss: 1.6182... Val Loss: 1.8692\n","Epoch: 3/30... Step: 1660... Loss: 1.6438... Val Loss: 1.8830\n","Epoch: 3/30... Step: 1670... Loss: 1.6333... Val Loss: 1.8633\n","Epoch: 3/30... Step: 1680... Loss: 1.6523... Val Loss: 1.8583\n","Epoch: 3/30... Step: 1690... Loss: 1.5972... Val Loss: 1.8805\n","Epoch: 3/30... Step: 1700... Loss: 1.5858... Val Loss: 1.8684\n","Epoch: 3/30... Step: 1710... Loss: 1.6608... Val Loss: 1.8671\n","Epoch: 3/30... Step: 1720... Loss: 1.5938... Val Loss: 1.8588\n","Epoch: 3/30... Step: 1730... Loss: 1.6189... Val Loss: 1.8493\n","Epoch: 3/30... Step: 1740... Loss: 1.6306... Val Loss: 1.8456\n","Epoch: 3/30... Step: 1750... Loss: 1.6241... Val Loss: 1.8632\n","Epoch: 3/30... Step: 1760... Loss: 1.6167... Val Loss: 1.8547\n","Epoch: 3/30... Step: 1770... Loss: 1.6345... Val Loss: 1.8527\n","Epoch: 3/30... Step: 1780... Loss: 1.6064... Val Loss: 1.8634\n","Epoch: 3/30... Step: 1790... Loss: 1.5601... Val Loss: 1.8337\n","Epoch: 3/30... Step: 1800... Loss: 1.6224... Val Loss: 1.8409\n","Epoch: 3/30... Step: 1810... Loss: 1.5839... Val Loss: 1.8567\n","Epoch: 3/30... Step: 1820... Loss: 1.5943... Val Loss: 1.8463\n","Epoch: 3/30... Step: 1830... Loss: 1.5839... Val Loss: 1.8385\n","Epoch: 3/30... Step: 1840... Loss: 1.6108... Val Loss: 1.8517\n","Epoch: 3/30... Step: 1850... Loss: 1.5994... Val Loss: 1.8450\n","Epoch: 3/30... Step: 1860... Loss: 1.5757... Val Loss: 1.8407\n","Epoch: 3/30... Step: 1870... Loss: 1.5841... Val Loss: 1.8409\n","Epoch: 3/30... Step: 1880... Loss: 1.5786... Val Loss: 1.8474\n","Epoch: 3/30... Step: 1890... Loss: 1.5918... Val Loss: 1.8476\n","Epoch: 3/30... Step: 1900... Loss: 1.5868... Val Loss: 1.8355\n","Epoch: 3/30... Step: 1910... Loss: 1.5992... Val Loss: 1.8309\n","Epoch: 3/30... Step: 1920... Loss: 1.5783... Val Loss: 1.8451\n","Epoch: 3/30... Step: 1930... Loss: 1.5546... Val Loss: 1.8386\n","Epoch: 3/30... Step: 1940... Loss: 1.5336... Val Loss: 1.8281\n","Epoch: 3/30... Step: 1950... Loss: 1.5508... Val Loss: 1.8333\n","Epoch: 3/30... Step: 1960... Loss: 1.5911... Val Loss: 1.8411\n","Epoch: 3/30... Step: 1970... Loss: 1.5617... Val Loss: 1.8348\n","Epoch: 3/30... Step: 1980... Loss: 1.5746... Val Loss: 1.8436\n","Epoch: 3/30... Step: 1990... Loss: 1.5787... Val Loss: 1.8318\n","Epoch: 3/30... Step: 2000... Loss: 1.5677... Val Loss: 1.8297\n","Epoch: 3/30... Step: 2010... Loss: 1.6226... Val Loss: 1.8564\n","Epoch: 3/30... Step: 2020... Loss: 1.5337... Val Loss: 1.8201\n","Epoch: 3/30... Step: 2030... Loss: 1.5794... Val Loss: 1.8407\n","Epoch: 3/30... Step: 2040... Loss: 1.5653... Val Loss: 1.8401\n","Epoch: 3/30... Step: 2050... Loss: 1.5686... Val Loss: 1.8324\n","Epoch: 3/30... Step: 2060... Loss: 1.5817... Val Loss: 1.8363\n","Epoch: 3/30... Step: 2070... Loss: 1.5454... Val Loss: 1.8283\n","Epoch: 3/30... Step: 2080... Loss: 1.5567... Val Loss: 1.8346\n","Epoch: 3/30... Step: 2090... Loss: 1.5666... Val Loss: 1.8360\n","Epoch: 3/30... Step: 2100... Loss: 1.5565... Val Loss: 1.8448\n","Epoch: 3/30... Step: 2110... Loss: 1.5555... Val Loss: 1.8437\n","Epoch: 3/30... Step: 2120... Loss: 1.5736... Val Loss: 1.8489\n","Epoch: 3/30... Step: 2130... Loss: 1.5625... Val Loss: 1.8488\n","Epoch: 3/30... Step: 2140... Loss: 1.5401... Val Loss: 1.8223\n","Epoch: 3/30... Step: 2150... Loss: 1.5578... Val Loss: 1.8411\n","Epoch: 3/30... Step: 2160... Loss: 1.5682... Val Loss: 1.8450\n","Epoch: 3/30... Step: 2170... Loss: 1.5435... Val Loss: 1.8322\n","Epoch: 3/30... Step: 2180... Loss: 1.5235... Val Loss: 1.8502\n","Epoch: 3/30... Step: 2190... Loss: 1.5542... Val Loss: 1.8354\n","Epoch: 3/30... Step: 2200... Loss: 1.5385... Val Loss: 1.8300\n","Epoch: 3/30... Step: 2210... Loss: 1.5386... Val Loss: 1.8199\n","Epoch: 3/30... Step: 2220... Loss: 1.5531... Val Loss: 1.8122\n","Epoch: 3/30... Step: 2230... Loss: 1.5166... Val Loss: 1.8170\n","Epoch: 3/30... Step: 2240... Loss: 1.5451... Val Loss: 1.8307\n","Epoch: 4/30... Step: 2250... Loss: 1.5779... Val Loss: 1.8343\n","Epoch: 4/30... Step: 2260... Loss: 1.5382... Val Loss: 1.8015\n","Epoch: 4/30... Step: 2270... Loss: 1.5865... Val Loss: 1.8135\n","Epoch: 4/30... Step: 2280... Loss: 1.5347... Val Loss: 1.7928\n","Epoch: 4/30... Step: 2290... Loss: 1.5786... Val Loss: 1.7940\n","Epoch: 4/30... Step: 2300... Loss: 1.5474... Val Loss: 1.8283\n","Epoch: 4/30... Step: 2310... Loss: 1.5261... Val Loss: 1.8281\n","Epoch: 4/30... Step: 2320... Loss: 1.5571... Val Loss: 1.8018\n","Epoch: 4/30... Step: 2330... Loss: 1.5485... Val Loss: 1.8000\n","Epoch: 4/30... Step: 2340... Loss: 1.5261... Val Loss: 1.8152\n","Epoch: 4/30... Step: 2350... Loss: 1.5324... Val Loss: 1.8084\n","Epoch: 4/30... Step: 2360... Loss: 1.5444... Val Loss: 1.7991\n","Epoch: 4/30... Step: 2370... Loss: 1.5829... Val Loss: 1.8493\n","Epoch: 4/30... Step: 2380... Loss: 1.5471... Val Loss: 1.8078\n","Epoch: 4/30... Step: 2390... Loss: 1.5334... Val Loss: 1.8011\n","Epoch: 4/30... Step: 2400... Loss: 1.5530... Val Loss: 1.7866\n","Epoch: 4/30... Step: 2410... Loss: 1.5502... Val Loss: 1.7860\n","Epoch: 4/30... Step: 2420... Loss: 1.5305... Val Loss: 1.7823\n","Epoch: 4/30... Step: 2430... Loss: 1.5441... Val Loss: 1.7913\n","Epoch: 4/30... Step: 2440... Loss: 1.5239... Val Loss: 1.7926\n","Epoch: 4/30... Step: 2450... Loss: 1.5422... Val Loss: 1.7848\n","Epoch: 4/30... Step: 2460... Loss: 1.5219... Val Loss: 1.7887\n","Epoch: 4/30... Step: 2470... Loss: 1.5105... Val Loss: 1.7737\n","Epoch: 4/30... Step: 2480... Loss: 1.5314... Val Loss: 1.7686\n","Epoch: 4/30... Step: 2490... Loss: 1.5389... Val Loss: 1.7776\n","Epoch: 4/30... Step: 2500... Loss: 1.5386... Val Loss: 1.7798\n","Epoch: 4/30... Step: 2510... Loss: 1.5411... Val Loss: 1.7680\n","Epoch: 4/30... Step: 2520... Loss: 1.5227... Val Loss: 1.7615\n","Epoch: 4/30... Step: 2530... Loss: 1.5317... Val Loss: 1.7824\n","Epoch: 4/30... Step: 2540... Loss: 1.5441... Val Loss: 1.7773\n","Epoch: 4/30... Step: 2550... Loss: 1.5169... Val Loss: 1.7727\n","Epoch: 4/30... Step: 2560... Loss: 1.5240... Val Loss: 1.7680\n","Epoch: 4/30... Step: 2570... Loss: 1.5164... Val Loss: 1.7714\n","Epoch: 4/30... Step: 2580... Loss: 1.4944... Val Loss: 1.7720\n","Epoch: 4/30... Step: 2590... Loss: 1.5015... Val Loss: 1.7642\n","Epoch: 4/30... Step: 2600... Loss: 1.5406... Val Loss: 1.7740\n","Epoch: 4/30... Step: 2610... Loss: 1.5019... Val Loss: 1.7724\n","Epoch: 4/30... Step: 2620... Loss: 1.5046... Val Loss: 1.7659\n","Epoch: 4/30... Step: 2630... Loss: 1.4955... Val Loss: 1.7541\n","Epoch: 4/30... Step: 2640... Loss: 1.4767... Val Loss: 1.7650\n","Epoch: 4/30... Step: 2650... Loss: 1.4792... Val Loss: 1.7654\n","Epoch: 4/30... Step: 2660... Loss: 1.5142... Val Loss: 1.7607\n","Epoch: 4/30... Step: 2670... Loss: 1.5060... Val Loss: 1.7643\n","Epoch: 4/30... Step: 2680... Loss: 1.5123... Val Loss: 1.7563\n","Epoch: 4/30... Step: 2690... Loss: 1.4898... Val Loss: 1.7637\n","Epoch: 4/30... Step: 2700... Loss: 1.4994... Val Loss: 1.7601\n","Epoch: 4/30... Step: 2710... Loss: 1.5114... Val Loss: 1.7486\n","Epoch: 4/30... Step: 2720... Loss: 1.4949... Val Loss: 1.7412\n","Epoch: 4/30... Step: 2730... Loss: 1.4839... Val Loss: 1.7448\n","Epoch: 4/30... Step: 2740... Loss: 1.5253... Val Loss: 1.7586\n","Epoch: 4/30... Step: 2750... Loss: 1.4538... Val Loss: 1.7522\n","Epoch: 4/30... Step: 2760... Loss: 1.4880... Val Loss: 1.7663\n","Epoch: 4/30... Step: 2770... Loss: 1.4946... Val Loss: 1.7551\n","Epoch: 4/30... Step: 2780... Loss: 1.4878... Val Loss: 1.7479\n","Epoch: 4/30... Step: 2790... Loss: 1.5074... Val Loss: 1.7557\n","Epoch: 4/30... Step: 2800... Loss: 1.5177... Val Loss: 1.7474\n","Epoch: 4/30... Step: 2810... Loss: 1.4659... Val Loss: 1.7551\n","Epoch: 4/30... Step: 2820... Loss: 1.4957... Val Loss: 1.7571\n","Epoch: 4/30... Step: 2830... Loss: 1.4607... Val Loss: 1.7528\n","Epoch: 4/30... Step: 2840... Loss: 1.5083... Val Loss: 1.7713\n","Epoch: 4/30... Step: 2850... Loss: 1.4815... Val Loss: 1.7768\n","Epoch: 4/30... Step: 2860... Loss: 1.4888... Val Loss: 1.7731\n","Epoch: 4/30... Step: 2870... Loss: 1.4867... Val Loss: 1.7597\n","Epoch: 4/30... Step: 2880... Loss: 1.4888... Val Loss: 1.7556\n","Epoch: 4/30... Step: 2890... Loss: 1.5039... Val Loss: 1.7565\n","Epoch: 4/30... Step: 2900... Loss: 1.4907... Val Loss: 1.7746\n","Epoch: 4/30... Step: 2910... Loss: 1.4974... Val Loss: 1.7544\n","Epoch: 4/30... Step: 2920... Loss: 1.4645... Val Loss: 1.7477\n","Epoch: 4/30... Step: 2930... Loss: 1.4637... Val Loss: 1.7601\n","Epoch: 4/30... Step: 2940... Loss: 1.4971... Val Loss: 1.7639\n","Epoch: 4/30... Step: 2950... Loss: 1.4762... Val Loss: 1.7552\n","Epoch: 4/30... Step: 2960... Loss: 1.4683... Val Loss: 1.7666\n","Epoch: 4/30... Step: 2970... Loss: 1.4348... Val Loss: 1.7598\n","Epoch: 4/30... Step: 2980... Loss: 1.4512... Val Loss: 1.7447\n","Epoch: 4/30... Step: 2990... Loss: 1.4707... Val Loss: 1.7479\n","Epoch: 5/30... Step: 3000... Loss: 1.4710... Val Loss: 1.7937\n","Epoch: 5/30... Step: 3010... Loss: 1.4830... Val Loss: 1.8190\n","Epoch: 5/30... Step: 3020... Loss: 1.4947... Val Loss: 1.7858\n","Epoch: 5/30... Step: 3030... Loss: 1.4920... Val Loss: 1.7523\n","Epoch: 5/30... Step: 3040... Loss: 1.4910... Val Loss: 1.7492\n","Epoch: 5/30... Step: 3050... Loss: 1.4949... Val Loss: 1.7441\n","Epoch: 5/30... Step: 3060... Loss: 1.4568... Val Loss: 1.7414\n","Epoch: 5/30... Step: 3070... Loss: 1.5198... Val Loss: 1.7330\n","Epoch: 5/30... Step: 3080... Loss: 1.4611... Val Loss: 1.7382\n","Epoch: 5/30... Step: 3090... Loss: 1.4650... Val Loss: 1.7330\n","Epoch: 5/30... Step: 3100... Loss: 1.4644... Val Loss: 1.7388\n","Epoch: 5/30... Step: 3110... Loss: 1.4621... Val Loss: 1.7315\n","Epoch: 5/30... Step: 3120... Loss: 1.4841... Val Loss: 1.7324\n","Epoch: 5/30... Step: 3130... Loss: 1.4762... Val Loss: 1.7338\n","Epoch: 5/30... Step: 3140... Loss: 1.4565... Val Loss: 1.7317\n","Epoch: 5/30... Step: 3150... Loss: 1.4680... Val Loss: 1.7386\n","Epoch: 5/30... Step: 3160... Loss: 1.4578... Val Loss: 1.7303\n","Epoch: 5/30... Step: 3170... Loss: 1.4727... Val Loss: 1.7199\n","Epoch: 5/30... Step: 3180... Loss: 1.5106... Val Loss: 1.7230\n","Epoch: 5/30... Step: 3190... Loss: 1.4618... Val Loss: 1.7324\n","Epoch: 5/30... Step: 3200... Loss: 1.4679... Val Loss: 1.7299\n","Epoch: 5/30... Step: 3210... Loss: 1.4683... Val Loss: 1.7236\n","Epoch: 5/30... Step: 3220... Loss: 1.4688... Val Loss: 1.7251\n","Epoch: 5/30... Step: 3230... Loss: 1.4852... Val Loss: 1.7156\n","Epoch: 5/30... Step: 3240... Loss: 1.4481... Val Loss: 1.7141\n","Epoch: 5/30... Step: 3250... Loss: 1.4544... Val Loss: 1.7227\n","Epoch: 5/30... Step: 3260... Loss: 1.4521... Val Loss: 1.7148\n","Epoch: 5/30... Step: 3270... Loss: 1.4552... Val Loss: 1.7125\n","Epoch: 5/30... Step: 3280... Loss: 1.4679... Val Loss: 1.7278\n","Epoch: 5/30... Step: 3290... Loss: 1.4681... Val Loss: 1.7225\n","Epoch: 5/30... Step: 3300... Loss: 1.4667... Val Loss: 1.7141\n","Epoch: 5/30... Step: 3310... Loss: 1.4820... Val Loss: 1.7167\n","Epoch: 5/30... Step: 3320... Loss: 1.4397... Val Loss: 1.7270\n","Epoch: 5/30... Step: 3330... Loss: 1.4866... Val Loss: 1.7278\n","Epoch: 5/30... Step: 3340... Loss: 1.4480... Val Loss: 1.7358\n","Epoch: 5/30... Step: 3350... Loss: 1.4643... Val Loss: 1.7401\n","Epoch: 5/30... Step: 3360... Loss: 1.4557... Val Loss: 1.7476\n","Epoch: 5/30... Step: 3370... Loss: 1.4440... Val Loss: 1.7417\n","Epoch: 5/30... Step: 3380... Loss: 1.4606... Val Loss: 1.7370\n","Epoch: 5/30... Step: 3390... Loss: 1.4235... Val Loss: 1.7432\n","Epoch: 5/30... Step: 3400... Loss: 1.4141... Val Loss: 1.7594\n","Epoch: 5/30... Step: 3410... Loss: 1.4128... Val Loss: 1.7405\n","Epoch: 5/30... Step: 3420... Loss: 1.4565... Val Loss: 1.7372\n","Epoch: 5/30... Step: 3430... Loss: 1.4502... Val Loss: 1.7303\n","Epoch: 5/30... Step: 3440... Loss: 1.4444... Val Loss: 1.7397\n","Epoch: 5/30... Step: 3450... Loss: 1.4675... Val Loss: 1.7268\n","Epoch: 5/30... Step: 3460... Loss: 1.4384... Val Loss: 1.7191\n","Epoch: 5/30... Step: 3470... Loss: 1.4462... Val Loss: 1.7241\n","Epoch: 5/30... Step: 3480... Loss: 1.4443... Val Loss: 1.7220\n","Epoch: 5/30... Step: 3490... Loss: 1.4396... Val Loss: 1.7300\n","Epoch: 5/30... Step: 3500... Loss: 1.4390... Val Loss: 1.7209\n","Epoch: 5/30... Step: 3510... Loss: 1.4410... Val Loss: 1.7316\n","Epoch: 5/30... Step: 3520... Loss: 1.4797... Val Loss: 1.7220\n","Epoch: 5/30... Step: 3530... Loss: 1.4553... Val Loss: 1.7266\n","Epoch: 5/30... Step: 3540... Loss: 1.4697... Val Loss: 1.7376\n","Epoch: 5/30... Step: 3550... Loss: 1.4320... Val Loss: 1.7373\n","Epoch: 5/30... Step: 3560... Loss: 1.4550... Val Loss: 1.7456\n","Epoch: 5/30... Step: 3570... Loss: 1.4520... Val Loss: 1.7365\n","Epoch: 5/30... Step: 3580... Loss: 1.4250... Val Loss: 1.7344\n","Epoch: 5/30... Step: 3590... Loss: 1.4150... Val Loss: 1.7513\n","Epoch: 5/30... Step: 3600... Loss: 1.4429... Val Loss: 1.7447\n","Epoch: 5/30... Step: 3610... Loss: 1.4497... Val Loss: 1.7453\n","Epoch: 5/30... Step: 3620... Loss: 1.4243... Val Loss: 1.7357\n","Epoch: 5/30... Step: 3630... Loss: 1.4390... Val Loss: 1.7440\n","Epoch: 5/30... Step: 3640... Loss: 1.4381... Val Loss: 1.7384\n","Epoch: 5/30... Step: 3650... Loss: 1.4466... Val Loss: 1.7363\n","Epoch: 5/30... Step: 3660... Loss: 1.4439... Val Loss: 1.7213\n","Epoch: 5/30... Step: 3670... Loss: 1.4279... Val Loss: 1.7048\n","Epoch: 5/30... Step: 3680... Loss: 1.4589... Val Loss: 1.7193\n","Epoch: 5/30... Step: 3690... Loss: 1.4531... Val Loss: 1.7257\n","Epoch: 5/30... Step: 3700... Loss: 1.4522... Val Loss: 1.7761\n","Epoch: 5/30... Step: 3710... Loss: 1.4221... Val Loss: 1.7817\n","Epoch: 5/30... Step: 3720... Loss: 1.4472... Val Loss: 1.7739\n","Epoch: 5/30... Step: 3730... Loss: 1.4554... Val Loss: 1.7761\n","Epoch: 5/30... Step: 3740... Loss: 1.4933... Val Loss: 1.7940\n","Epoch: 6/30... Step: 3750... Loss: 1.4439... Val Loss: 1.7473\n","Epoch: 6/30... Step: 3760... Loss: 1.4223... Val Loss: 1.7517\n","Epoch: 6/30... Step: 3770... Loss: 1.4441... Val Loss: 1.7967\n","Epoch: 6/30... Step: 3780... Loss: 1.4610... Val Loss: 1.7810\n","Epoch: 6/30... Step: 3790... Loss: 1.4349... Val Loss: 1.7579\n","Epoch: 6/30... Step: 3800... Loss: 1.4517... Val Loss: 1.7277\n","Epoch: 6/30... Step: 3810... Loss: 1.4608... Val Loss: 1.7191\n","Epoch: 6/30... Step: 3820... Loss: 1.4439... Val Loss: 1.7174\n","Epoch: 6/30... Step: 3830... Loss: 1.4309... Val Loss: 1.7361\n","Epoch: 6/30... Step: 3840... Loss: 1.4471... Val Loss: 1.7454\n","Epoch: 6/30... Step: 3850... Loss: 1.4453... Val Loss: 1.7157\n","Epoch: 6/30... Step: 3860... Loss: 1.4568... Val Loss: 1.7188\n","Epoch: 6/30... Step: 3870... Loss: 1.4389... Val Loss: 1.7183\n","Epoch: 6/30... Step: 3880... Loss: 1.4363... Val Loss: 1.7129\n","Epoch: 6/30... Step: 3890... Loss: 1.4652... Val Loss: 1.7095\n","Epoch: 6/30... Step: 3900... Loss: 1.4269... Val Loss: 1.7176\n","Epoch: 6/30... Step: 3910... Loss: 1.4294... Val Loss: 1.7040\n","Epoch: 6/30... Step: 3920... Loss: 1.4496... Val Loss: 1.7089\n","Epoch: 6/30... Step: 3930... Loss: 1.4000... Val Loss: 1.7216\n","Epoch: 6/30... Step: 3940... Loss: 1.4461... Val Loss: 1.7097\n","Epoch: 6/30... Step: 3950... Loss: 1.4363... Val Loss: 1.7069\n","Epoch: 6/30... Step: 3960... Loss: 1.3973... Val Loss: 1.7126\n","Epoch: 6/30... Step: 3970... Loss: 1.4392... Val Loss: 1.7039\n","Epoch: 6/30... Step: 3980... Loss: 1.4216... Val Loss: 1.7131\n","Epoch: 6/30... Step: 3990... Loss: 1.4313... Val Loss: 1.7124\n","Epoch: 6/30... Step: 4000... Loss: 1.4181... Val Loss: 1.7168\n","Epoch: 6/30... Step: 4010... Loss: 1.4521... Val Loss: 1.7141\n","Epoch: 6/30... Step: 4020... Loss: 1.4411... Val Loss: 1.7201\n","Epoch: 6/30... Step: 4030... Loss: 1.4400... Val Loss: 1.7189\n","Epoch: 6/30... Step: 4040... Loss: 1.4473... Val Loss: 1.7293\n","Epoch: 6/30... Step: 4050... Loss: 1.4399... Val Loss: 1.7092\n","Epoch: 6/30... Step: 4060... Loss: 1.4149... Val Loss: 1.7034\n","Epoch: 6/30... Step: 4070... Loss: 1.4172... Val Loss: 1.7106\n","Epoch: 6/30... Step: 4080... Loss: 1.4110... Val Loss: 1.7271\n","Epoch: 6/30... Step: 4090... Loss: 1.4188... Val Loss: 1.7411\n","Epoch: 6/30... Step: 4100... Loss: 1.4641... Val Loss: 1.7488\n","Epoch: 6/30... Step: 4110... Loss: 1.3986... Val Loss: 1.7513\n","Epoch: 6/30... Step: 4120... Loss: 1.4033... Val Loss: 1.7639\n","Epoch: 6/30... Step: 4130... Loss: 1.4200... Val Loss: 1.7476\n","Epoch: 6/30... Step: 4140... Loss: 1.3877... Val Loss: 1.7564\n","Epoch: 6/30... Step: 4150... Loss: 1.4282... Val Loss: 1.7590\n","Epoch: 6/30... Step: 4160... Loss: 1.4108... Val Loss: 1.7552\n","Epoch: 6/30... Step: 4170... Loss: 1.4134... Val Loss: 1.7419\n","Epoch: 6/30... Step: 4180... Loss: 1.4072... Val Loss: 1.7341\n","Epoch: 6/30... Step: 4190... Loss: 1.4209... Val Loss: 1.7261\n","Epoch: 6/30... Step: 4200... Loss: 1.4158... Val Loss: 1.7102\n","Epoch: 6/30... Step: 4210... Loss: 1.4251... Val Loss: 1.7286\n","Epoch: 6/30... Step: 4220... Loss: 1.3966... Val Loss: 1.7117\n","Epoch: 6/30... Step: 4230... Loss: 1.3864... Val Loss: 1.7414\n","Epoch: 6/30... Step: 4240... Loss: 1.3935... Val Loss: 1.7283\n","Epoch: 6/30... Step: 4250... Loss: 1.3789... Val Loss: 1.7231\n","Epoch: 6/30... Step: 4260... Loss: 1.3956... Val Loss: 1.7095\n","Epoch: 6/30... Step: 4270... Loss: 1.4033... Val Loss: 1.7097\n","Epoch: 6/30... Step: 4280... Loss: 1.4290... Val Loss: 1.7116\n","Epoch: 6/30... Step: 4290... Loss: 1.4049... Val Loss: 1.7203\n","Epoch: 6/30... Step: 4300... Loss: 1.4038... Val Loss: 1.7128\n","Epoch: 6/30... Step: 4310... Loss: 1.4064... Val Loss: 1.7138\n","Epoch: 6/30... Step: 4320... Loss: 1.4301... Val Loss: 1.7567\n","Epoch: 6/30... Step: 4330... Loss: 1.4021... Val Loss: 1.7217\n","Epoch: 6/30... Step: 4340... Loss: 1.4183... Val Loss: 1.7421\n","Epoch: 6/30... Step: 4350... Loss: 1.4237... Val Loss: 1.7408\n","Epoch: 6/30... Step: 4360... Loss: 1.4227... Val Loss: 1.7641\n","Epoch: 6/30... Step: 4370... Loss: 1.3913... Val Loss: 1.7796\n","Epoch: 6/30... Step: 4380... Loss: 1.4035... Val Loss: 1.7206\n","Epoch: 6/30... Step: 4390... Loss: 1.4081... Val Loss: 1.7254\n","Epoch: 6/30... Step: 4400... Loss: 1.4020... Val Loss: 1.7303\n","Epoch: 6/30... Step: 4410... Loss: 1.4103... Val Loss: 1.7194\n","Epoch: 6/30... Step: 4420... Loss: 1.3863... Val Loss: 1.7624\n","Epoch: 6/30... Step: 4430... Loss: 1.3994... Val Loss: 1.7542\n","Epoch: 6/30... Step: 4440... Loss: 1.4191... Val Loss: 1.7382\n","Epoch: 6/30... Step: 4450... Loss: 1.4064... Val Loss: 1.7483\n","Epoch: 6/30... Step: 4460... Loss: 1.3974... Val Loss: 1.7114\n","Epoch: 6/30... Step: 4470... Loss: 1.4028... Val Loss: 1.7207\n","Epoch: 6/30... Step: 4480... Loss: 1.4074... Val Loss: 1.7092\n","Epoch: 7/30... Step: 4490... Loss: 1.4054... Val Loss: 1.7888\n","Epoch: 7/30... Step: 4500... Loss: 1.3803... Val Loss: 1.7589\n","Epoch: 7/30... Step: 4510... Loss: 1.4287... Val Loss: 1.7861\n","Epoch: 7/30... Step: 4520... Loss: 1.4172... Val Loss: 1.7623\n","Epoch: 7/30... Step: 4530... Loss: 1.4089... Val Loss: 1.7520\n","Epoch: 7/30... Step: 4540... Loss: 1.4102... Val Loss: 1.7414\n","Epoch: 7/30... Step: 4550... Loss: 1.4036... Val Loss: 1.7376\n","Epoch: 7/30... Step: 4560... Loss: 1.4150... Val Loss: 1.7331\n","Epoch: 7/30... Step: 4570... Loss: 1.3955... Val Loss: 1.7294\n","Epoch: 7/30... Step: 4580... Loss: 1.4213... Val Loss: 1.7173\n","Epoch: 7/30... Step: 4590... Loss: 1.4021... Val Loss: 1.7507\n","Epoch: 7/30... Step: 4600... Loss: 1.3855... Val Loss: 1.7325\n","Epoch: 7/30... Step: 4610... Loss: 1.4270... Val Loss: 1.7252\n","Epoch: 7/30... Step: 4620... Loss: 1.3755... Val Loss: 1.7146\n","Epoch: 7/30... Step: 4630... Loss: 1.4081... Val Loss: 1.7176\n","Epoch: 7/30... Step: 4640... Loss: 1.3747... Val Loss: 1.7152\n","Epoch: 7/30... Step: 4650... Loss: 1.4076... Val Loss: 1.7188\n","Epoch: 7/30... Step: 4660... Loss: 1.4098... Val Loss: 1.7287\n","Epoch: 7/30... Step: 4670... Loss: 1.3723... Val Loss: 1.7007\n","Epoch: 7/30... Step: 4680... Loss: 1.4011... Val Loss: 1.7521\n","Epoch: 7/30... Step: 4690... Loss: 1.4171... Val Loss: 1.7201\n","Epoch: 7/30... Step: 4700... Loss: 1.4118... Val Loss: 1.7482\n","Epoch: 7/30... Step: 4710... Loss: 1.3719... Val Loss: 1.7378\n","Epoch: 7/30... Step: 4720... Loss: 1.3960... Val Loss: 1.7361\n","Epoch: 7/30... Step: 4730... Loss: 1.3842... Val Loss: 1.7196\n","Epoch: 7/30... Step: 4740... Loss: 1.4086... Val Loss: 1.7212\n","Epoch: 7/30... Step: 4750... Loss: 1.3839... Val Loss: 1.7767\n","Epoch: 7/30... Step: 4760... Loss: 1.4186... Val Loss: 1.7161\n","Epoch: 7/30... Step: 4770... Loss: 1.4080... Val Loss: 1.7563\n","Epoch: 7/30... Step: 4780... Loss: 1.4259... Val Loss: 1.7380\n","Epoch: 7/30... Step: 4790... Loss: 1.3932... Val Loss: 1.7364\n","Epoch: 7/30... Step: 4800... Loss: 1.4057... Val Loss: 1.7576\n","Epoch: 7/30... Step: 4810... Loss: 1.4042... Val Loss: 1.7278\n","Epoch: 7/30... Step: 4820... Loss: 1.3617... Val Loss: 1.7382\n","Epoch: 7/30... Step: 4830... Loss: 1.3956... Val Loss: 1.7411\n","Epoch: 7/30... Step: 4840... Loss: 1.3878... Val Loss: 1.7443\n","Epoch: 7/30... Step: 4850... Loss: 1.4061... Val Loss: 1.7273\n","Epoch: 7/30... Step: 4860... Loss: 1.3914... Val Loss: 1.7350\n","Epoch: 7/30... Step: 4870... Loss: 1.4133... Val Loss: 1.7520\n","Epoch: 7/30... Step: 4880... Loss: 1.4012... Val Loss: 1.7318\n","Epoch: 7/30... Step: 4890... Loss: 1.3968... Val Loss: 1.7415\n","Epoch: 7/30... Step: 4900... Loss: 1.3969... Val Loss: 1.7403\n","Epoch: 7/30... Step: 4910... Loss: 1.3914... Val Loss: 1.7333\n","Epoch: 7/30... Step: 4920... Loss: 1.3880... Val Loss: 1.7370\n","Epoch: 7/30... Step: 4930... Loss: 1.3884... Val Loss: 1.7554\n","Epoch: 7/30... Step: 4940... Loss: 1.4093... Val Loss: 1.7356\n","Epoch: 7/30... Step: 4950... Loss: 1.3971... Val Loss: 1.7105\n","Epoch: 7/30... Step: 4960... Loss: 1.3630... Val Loss: 1.7341\n","Epoch: 7/30... Step: 4970... Loss: 1.4181... Val Loss: 1.7327\n","Epoch: 7/30... Step: 4980... Loss: 1.3818... Val Loss: 1.7522\n","Epoch: 7/30... Step: 4990... Loss: 1.4038... Val Loss: 1.7436\n","Epoch: 7/30... Step: 5000... Loss: 1.4200... Val Loss: 1.7476\n","Epoch: 7/30... Step: 5010... Loss: 1.3615... Val Loss: 1.7460\n","Epoch: 7/30... Step: 5020... Loss: 1.3982... Val Loss: 1.7347\n","Epoch: 7/30... Step: 5030... Loss: 1.3889... Val Loss: 1.7035\n","Epoch: 7/30... Step: 5040... Loss: 1.3488... Val Loss: 1.7321\n","Epoch: 7/30... Step: 5050... Loss: 1.3632... Val Loss: 1.7330\n","Epoch: 7/30... Step: 5060... Loss: 1.4195... Val Loss: 1.7146\n","Epoch: 7/30... Step: 5070... Loss: 1.3686... Val Loss: 1.7511\n","Epoch: 7/30... Step: 5080... Loss: 1.4005... Val Loss: 1.7376\n","Epoch: 7/30... Step: 5090... Loss: 1.3911... Val Loss: 1.7274\n","Epoch: 7/30... Step: 5100... Loss: 1.3865... Val Loss: 1.7514\n","Epoch: 7/30... Step: 5110... Loss: 1.3795... Val Loss: 1.7133\n","Epoch: 7/30... Step: 5120... Loss: 1.3824... Val Loss: 1.7498\n","Epoch: 7/30... Step: 5130... Loss: 1.3770... Val Loss: 1.6986\n","Epoch: 7/30... Step: 5140... Loss: 1.3787... Val Loss: 1.7092\n","Epoch: 7/30... Step: 5150... Loss: 1.3733... Val Loss: 1.7232\n","Epoch: 7/30... Step: 5160... Loss: 1.3837... Val Loss: 1.6930\n","Epoch: 7/30... Step: 5170... Loss: 1.3652... Val Loss: 1.7103\n","Epoch: 7/30... Step: 5180... Loss: 1.3882... Val Loss: 1.7241\n","Epoch: 7/30... Step: 5190... Loss: 1.3466... Val Loss: 1.7128\n","Epoch: 7/30... Step: 5200... Loss: 1.3969... Val Loss: 1.7621\n","Epoch: 7/30... Step: 5210... Loss: 1.3837... Val Loss: 1.7380\n","Epoch: 7/30... Step: 5220... Loss: 1.3800... Val Loss: 1.7515\n","Epoch: 7/30... Step: 5230... Loss: 1.3733... Val Loss: 1.7091\n","Epoch: 8/30... Step: 5240... Loss: 1.3981... Val Loss: 1.7999\n","Epoch: 8/30... Step: 5250... Loss: 1.4074... Val Loss: 1.7771\n","Epoch: 8/30... Step: 5260... Loss: 1.3652... Val Loss: 1.7983\n","Epoch: 8/30... Step: 5270... Loss: 1.3975... Val Loss: 1.7742\n","Epoch: 8/30... Step: 5280... Loss: 1.4144... Val Loss: 1.7865\n","Epoch: 8/30... Step: 5290... Loss: 1.3800... Val Loss: 1.7653\n","Epoch: 8/30... Step: 5300... Loss: 1.3692... Val Loss: 1.7730\n","Epoch: 8/30... Step: 5310... Loss: 1.3972... Val Loss: 1.7451\n","Epoch: 8/30... Step: 5320... Loss: 1.3940... Val Loss: 1.7622\n","Epoch: 8/30... Step: 5330... Loss: 1.3697... Val Loss: 1.7578\n","Epoch: 8/30... Step: 5340... Loss: 1.4037... Val Loss: 1.7691\n","Epoch: 8/30... Step: 5350... Loss: 1.3696... Val Loss: 1.7702\n","Epoch: 8/30... Step: 5360... Loss: 1.3899... Val Loss: 1.7611\n","Epoch: 8/30... Step: 5370... Loss: 1.4053... Val Loss: 1.7671\n","Epoch: 8/30... Step: 5380... Loss: 1.3785... Val Loss: 1.7642\n","Epoch: 8/30... Step: 5390... Loss: 1.3902... Val Loss: 1.7743\n","Epoch: 8/30... Step: 5400... Loss: 1.3962... Val Loss: 1.7651\n","Epoch: 8/30... Step: 5410... Loss: 1.3932... Val Loss: 1.7579\n","Epoch: 8/30... Step: 5420... Loss: 1.4134... Val Loss: 1.7839\n","Epoch: 8/30... Step: 5430... Loss: 1.3514... Val Loss: 1.7641\n","Epoch: 8/30... Step: 5440... Loss: 1.3548... Val Loss: 1.7793\n","Epoch: 8/30... Step: 5450... Loss: 1.4238... Val Loss: 1.7918\n","Epoch: 8/30... Step: 5460... Loss: 1.3512... Val Loss: 1.7522\n","Epoch: 8/30... Step: 5470... Loss: 1.3737... Val Loss: 1.7662\n","Epoch: 8/30... Step: 5480... Loss: 1.4071... Val Loss: 1.7688\n","Epoch: 8/30... Step: 5490... Loss: 1.3909... Val Loss: 1.7422\n","Epoch: 8/30... Step: 5500... Loss: 1.3880... Val Loss: 1.7847\n","Epoch: 8/30... Step: 5510... Loss: 1.4036... Val Loss: 1.7397\n","Epoch: 8/30... Step: 5520... Loss: 1.3669... Val Loss: 1.7852\n","Epoch: 8/30... Step: 5530... Loss: 1.3343... Val Loss: 1.7776\n","Epoch: 8/30... Step: 5540... Loss: 1.3889... Val Loss: 1.7635\n","Epoch: 8/30... Step: 5550... Loss: 1.3698... Val Loss: 1.7868\n","Epoch: 8/30... Step: 5560... Loss: 1.3778... Val Loss: 1.7550\n","Epoch: 8/30... Step: 5570... Loss: 1.3683... Val Loss: 1.7759\n","Epoch: 8/30... Step: 5580... Loss: 1.3893... Val Loss: 1.7478\n","Epoch: 8/30... Step: 5590... Loss: 1.3880... Val Loss: 1.7604\n","Epoch: 8/30... Step: 5600... Loss: 1.3638... Val Loss: 1.7637\n","Epoch: 8/30... Step: 5610... Loss: 1.3703... Val Loss: 1.7486\n","Epoch: 8/30... Step: 5620... Loss: 1.3725... Val Loss: 1.7611\n","Epoch: 8/30... Step: 5630... Loss: 1.3842... Val Loss: 1.7562\n","Epoch: 8/30... Step: 5640... Loss: 1.3858... Val Loss: 1.7440\n","Epoch: 8/30... Step: 5650... Loss: 1.4066... Val Loss: 1.7600\n","Epoch: 8/30... Step: 5660... Loss: 1.3767... Val Loss: 1.7309\n","Epoch: 8/30... Step: 5670... Loss: 1.3590... Val Loss: 1.7633\n","Epoch: 8/30... Step: 5680... Loss: 1.3415... Val Loss: 1.7645\n","Epoch: 8/30... Step: 5690... Loss: 1.3483... Val Loss: 1.7500\n","Epoch: 8/30... Step: 5700... Loss: 1.3847... Val Loss: 1.7569\n","Epoch: 8/30... Step: 5710... Loss: 1.3706... Val Loss: 1.7479\n","Epoch: 8/30... Step: 5720... Loss: 1.3844... Val Loss: 1.7582\n","Epoch: 8/30... Step: 5730... Loss: 1.3895... Val Loss: 1.7486\n","Epoch: 8/30... Step: 5740... Loss: 1.3730... Val Loss: 1.7533\n","Epoch: 8/30... Step: 5750... Loss: 1.4206... Val Loss: 1.7411\n","Epoch: 8/30... Step: 5760... Loss: 1.3533... Val Loss: 1.7436\n","Epoch: 8/30... Step: 5770... Loss: 1.3811... Val Loss: 1.7478\n","Epoch: 8/30... Step: 5780... Loss: 1.3691... Val Loss: 1.7412\n","Epoch: 8/30... Step: 5790... Loss: 1.3587... Val Loss: 1.7689\n","Epoch: 8/30... Step: 5800... Loss: 1.3760... Val Loss: 1.7597\n","Epoch: 8/30... Step: 5810... Loss: 1.3644... Val Loss: 1.7434\n","Epoch: 8/30... Step: 5820... Loss: 1.3729... Val Loss: 1.7548\n","Epoch: 8/30... Step: 5830... Loss: 1.3835... Val Loss: 1.7351\n","Epoch: 8/30... Step: 5840... Loss: 1.3774... Val Loss: 1.7282\n","Epoch: 8/30... Step: 5850... Loss: 1.3704... Val Loss: 1.7860\n","Epoch: 8/30... Step: 5860... Loss: 1.3816... Val Loss: 1.7691\n","Epoch: 8/30... Step: 5870... Loss: 1.3762... Val Loss: 1.7596\n","Epoch: 8/30... Step: 5880... Loss: 1.3552... Val Loss: 1.7544\n","Epoch: 8/30... Step: 5890... Loss: 1.3779... Val Loss: 1.7331\n","Epoch: 8/30... Step: 5900... Loss: 1.3774... Val Loss: 1.7307\n","Epoch: 8/30... Step: 5910... Loss: 1.3645... Val Loss: 1.7116\n","Epoch: 8/30... Step: 5920... Loss: 1.3373... Val Loss: 1.7416\n","Epoch: 8/30... Step: 5930... Loss: 1.3780... Val Loss: 1.7351\n","Epoch: 8/30... Step: 5940... Loss: 1.3536... Val Loss: 1.7371\n","Epoch: 8/30... Step: 5950... Loss: 1.3526... Val Loss: 1.7772\n","Epoch: 8/30... Step: 5960... Loss: 1.3747... Val Loss: 1.7374\n","Epoch: 8/30... Step: 5970... Loss: 1.3501... Val Loss: 1.7164\n","Epoch: 8/30... Step: 5980... Loss: 1.3561... Val Loss: 1.7385\n","Epoch: 9/30... Step: 5990... Loss: 1.3688... Val Loss: 1.7615\n","Epoch: 9/30... Step: 6000... Loss: 1.3508... Val Loss: 1.7689\n","Epoch: 9/30... Step: 6010... Loss: 1.3852... Val Loss: 1.7596\n","Epoch: 9/30... Step: 6020... Loss: 1.3372... Val Loss: 1.7467\n","Epoch: 9/30... Step: 6030... Loss: 1.3901... Val Loss: 1.7553\n","Epoch: 9/30... Step: 6040... Loss: 1.3663... Val Loss: 1.7499\n","Epoch: 9/30... Step: 6050... Loss: 1.3548... Val Loss: 1.7529\n","Epoch: 9/30... Step: 6060... Loss: 1.3829... Val Loss: 1.7431\n","Epoch: 9/30... Step: 6070... Loss: 1.3858... Val Loss: 1.7505\n","Epoch: 9/30... Step: 6080... Loss: 1.3550... Val Loss: 1.7466\n","Epoch: 9/30... Step: 6090... Loss: 1.3634... Val Loss: 1.7552\n","Epoch: 9/30... Step: 6100... Loss: 1.3705... Val Loss: 1.7567\n","Epoch: 9/30... Step: 6110... Loss: 1.3588... Val Loss: 1.7392\n","Epoch: 9/30... Step: 6120... Loss: 1.3427... Val Loss: 1.7729\n","Epoch: 9/30... Step: 6130... Loss: 1.3534... Val Loss: 1.7578\n","Epoch: 9/30... Step: 6140... Loss: 1.3792... Val Loss: 1.7715\n","Epoch: 9/30... Step: 6150... Loss: 1.3678... Val Loss: 1.7514\n","Epoch: 9/30... Step: 6160... Loss: 1.3592... Val Loss: 1.7579\n","Epoch: 9/30... Step: 6170... Loss: 1.3744... Val Loss: 1.7867\n","Epoch: 9/30... Step: 6180... Loss: 1.3557... Val Loss: 1.7564\n","Epoch: 9/30... Step: 6190... Loss: 1.3743... Val Loss: 1.7642\n","Epoch: 9/30... Step: 6200... Loss: 1.3530... Val Loss: 1.7756\n","Epoch: 9/30... Step: 6210... Loss: 1.3525... Val Loss: 1.7630\n","Epoch: 9/30... Step: 6220... Loss: 1.3846... Val Loss: 1.7725\n","Epoch: 9/30... Step: 6230... Loss: 1.3742... Val Loss: 1.7804\n","Epoch: 9/30... Step: 6240... Loss: 1.3827... Val Loss: 1.7597\n","Epoch: 9/30... Step: 6250... Loss: 1.3690... Val Loss: 1.7659\n","Epoch: 9/30... Step: 6260... Loss: 1.3684... Val Loss: 1.7617\n","Epoch: 9/30... Step: 6270... Loss: 1.3661... Val Loss: 1.7565\n","Epoch: 9/30... Step: 6280... Loss: 1.3843... Val Loss: 1.7564\n","Epoch: 9/30... Step: 6290... Loss: 1.3716... Val Loss: 1.7529\n","Epoch: 9/30... Step: 6300... Loss: 1.3661... Val Loss: 1.7665\n","Epoch: 9/30... Step: 6310... Loss: 1.3674... Val Loss: 1.7758\n","Epoch: 9/30... Step: 6320... Loss: 1.3466... Val Loss: 1.7536\n","Epoch: 9/30... Step: 6330... Loss: 1.3386... Val Loss: 1.7581\n","Epoch: 9/30... Step: 6340... Loss: 1.3886... Val Loss: 1.7697\n","Epoch: 9/30... Step: 6350... Loss: 1.3569... Val Loss: 1.7834\n","Epoch: 9/30... Step: 6360... Loss: 1.3570... Val Loss: 1.7609\n","Epoch: 9/30... Step: 6370... Loss: 1.3475... Val Loss: 1.7802\n","Epoch: 9/30... Step: 6380... Loss: 1.3300... Val Loss: 1.7697\n","Epoch: 9/30... Step: 6390... Loss: 1.3301... Val Loss: 1.7650\n","Epoch: 9/30... Step: 6400... Loss: 1.3572... Val Loss: 1.7700\n","Epoch: 9/30... Step: 6410... Loss: 1.3529... Val Loss: 1.7394\n","Epoch: 9/30... Step: 6420... Loss: 1.3589... Val Loss: 1.7657\n","Epoch: 9/30... Step: 6430... Loss: 1.3370... Val Loss: 1.7774\n","Epoch: 9/30... Step: 6440... Loss: 1.3556... Val Loss: 1.7584\n","Epoch: 9/30... Step: 6450... Loss: 1.3635... Val Loss: 1.7586\n","Epoch: 9/30... Step: 6460... Loss: 1.3573... Val Loss: 1.7680\n","Epoch: 9/30... Step: 6470... Loss: 1.3370... Val Loss: 1.7837\n","Epoch: 9/30... Step: 6480... Loss: 1.3811... Val Loss: 1.7649\n","Epoch: 9/30... Step: 6490... Loss: 1.3252... Val Loss: 1.7640\n","Epoch: 9/30... Step: 6500... Loss: 1.3697... Val Loss: 1.7818\n","Epoch: 9/30... Step: 6510... Loss: 1.3659... Val Loss: 1.7873\n","Epoch: 9/30... Step: 6520... Loss: 1.3557... Val Loss: 1.8054\n","Epoch: 9/30... Step: 6530... Loss: 1.3705... Val Loss: 1.7745\n","Epoch: 9/30... Step: 6540... Loss: 1.3809... Val Loss: 1.7885\n","Epoch: 9/30... Step: 6550... Loss: 1.3405... Val Loss: 1.7895\n","Epoch: 9/30... Step: 6560... Loss: 1.3553... Val Loss: 1.7776\n","Epoch: 9/30... Step: 6570... Loss: 1.3257... Val Loss: 1.7792\n","Epoch: 9/30... Step: 6580... Loss: 1.3629... Val Loss: 1.7790\n","Epoch: 9/30... Step: 6590... Loss: 1.3459... Val Loss: 1.7725\n","Epoch: 9/30... Step: 6600... Loss: 1.3621... Val Loss: 1.7938\n","Epoch: 9/30... Step: 6610... Loss: 1.3575... Val Loss: 1.8062\n","Epoch: 9/30... Step: 6620... Loss: 1.3591... Val Loss: 1.7652\n","Epoch: 9/30... Step: 6630... Loss: 1.3767... Val Loss: 1.7960\n","Epoch: 9/30... Step: 6640... Loss: 1.3653... Val Loss: 1.7848\n","Epoch: 9/30... Step: 6650... Loss: 1.3682... Val Loss: 1.7671\n","Epoch: 9/30... Step: 6660... Loss: 1.3235... Val Loss: 1.7734\n","Epoch: 9/30... Step: 6670... Loss: 1.3312... Val Loss: 1.7831\n","Epoch: 9/30... Step: 6680... Loss: 1.3564... Val Loss: 1.7757\n","Epoch: 9/30... Step: 6690... Loss: 1.3533... Val Loss: 1.7754\n","Epoch: 9/30... Step: 6700... Loss: 1.3488... Val Loss: 1.7920\n","Epoch: 9/30... Step: 6710... Loss: 1.3077... Val Loss: 1.7721\n","Epoch: 9/30... Step: 6720... Loss: 1.3320... Val Loss: 1.7781\n","Epoch: 9/30... Step: 6730... Loss: 1.3430... Val Loss: 1.7836\n","Epoch: 10/30... Step: 6740... Loss: 1.3454... Val Loss: 1.7006\n","Epoch: 10/30... Step: 6750... Loss: 1.3552... Val Loss: 1.7014\n","Epoch: 10/30... Step: 6760... Loss: 1.3628... Val Loss: 1.6736\n","Epoch: 10/30... Step: 6770... Loss: 1.3661... Val Loss: 1.6681\n","Epoch: 10/30... Step: 6780... Loss: 1.3678... Val Loss: 1.6692\n","Epoch: 10/30... Step: 6790... Loss: 1.3790... Val Loss: 1.6741\n","Epoch: 10/30... Step: 6800... Loss: 1.3359... Val Loss: 1.6647\n","Epoch: 10/30... Step: 6810... Loss: 1.3966... Val Loss: 1.6772\n","Epoch: 10/30... Step: 6820... Loss: 1.3384... Val Loss: 1.6791\n","Epoch: 10/30... Step: 6830... Loss: 1.3414... Val Loss: 1.6754\n","Epoch: 10/30... Step: 6840... Loss: 1.3595... Val Loss: 1.6837\n","Epoch: 10/30... Step: 6850... Loss: 1.3398... Val Loss: 1.6761\n","Epoch: 10/30... Step: 6860... Loss: 1.3607... Val Loss: 1.6717\n","Epoch: 10/30... Step: 6870... Loss: 1.3516... Val Loss: 1.6891\n","Epoch: 10/30... Step: 6880... Loss: 1.3471... Val Loss: 1.6776\n","Epoch: 10/30... Step: 6890... Loss: 1.3451... Val Loss: 1.6773\n","Epoch: 10/30... Step: 6900... Loss: 1.3455... Val Loss: 1.6760\n","Epoch: 10/30... Step: 6910... Loss: 1.3685... Val Loss: 1.6865\n","Epoch: 10/30... Step: 6920... Loss: 1.3931... Val Loss: 1.6741\n","Epoch: 10/30... Step: 6930... Loss: 1.3521... Val Loss: 1.6748\n","Epoch: 10/30... Step: 6940... Loss: 1.3421... Val Loss: 1.6897\n","Epoch: 10/30... Step: 6950... Loss: 1.3450... Val Loss: 1.6858\n","Epoch: 10/30... Step: 6960... Loss: 1.3483... Val Loss: 1.6804\n","Epoch: 10/30... Step: 6970... Loss: 1.3641... Val Loss: 1.6799\n","Epoch: 10/30... Step: 6980... Loss: 1.3383... Val Loss: 1.6665\n","Epoch: 10/30... Step: 6990... Loss: 1.3424... Val Loss: 1.6947\n","Epoch: 10/30... Step: 7000... Loss: 1.3380... Val Loss: 1.6700\n","Epoch: 10/30... Step: 7010... Loss: 1.3474... Val Loss: 1.6673\n","Epoch: 10/30... Step: 7020... Loss: 1.3561... Val Loss: 1.6758\n","Epoch: 10/30... Step: 7030... Loss: 1.3568... Val Loss: 1.6750\n","Epoch: 10/30... Step: 7040... Loss: 1.3515... Val Loss: 1.6624\n","Epoch: 10/30... Step: 7050... Loss: 1.3657... Val Loss: 1.6820\n","Epoch: 10/30... Step: 7060... Loss: 1.3355... Val Loss: 1.7064\n","Epoch: 10/30... Step: 7070... Loss: 1.3831... Val Loss: 1.6808\n","Epoch: 10/30... Step: 7080... Loss: 1.3464... Val Loss: 1.7617\n","Epoch: 10/30... Step: 7090... Loss: 1.3563... Val Loss: 1.7270\n","Epoch: 10/30... Step: 7100... Loss: 1.3427... Val Loss: 1.7640\n","Epoch: 10/30... Step: 7110... Loss: 1.3262... Val Loss: 1.7339\n","Epoch: 10/30... Step: 7120... Loss: 1.3432... Val Loss: 1.7346\n","Epoch: 10/30... Step: 7130... Loss: 1.3213... Val Loss: 1.7303\n","Epoch: 10/30... Step: 7140... Loss: 1.3075... Val Loss: 1.7263\n","Epoch: 10/30... Step: 7150... Loss: 1.3060... Val Loss: 1.7430\n","Epoch: 10/30... Step: 7160... Loss: 1.3521... Val Loss: 1.6891\n","Epoch: 10/30... Step: 7170... Loss: 1.3437... Val Loss: 1.6971\n","Epoch: 10/30... Step: 7180... Loss: 1.3346... Val Loss: 1.7495\n","Epoch: 10/30... Step: 7190... Loss: 1.3563... Val Loss: 1.7175\n","Epoch: 10/30... Step: 7200... Loss: 1.3466... Val Loss: 1.7295\n","Epoch: 10/30... Step: 7210... Loss: 1.3480... Val Loss: 1.7490\n","Epoch: 10/30... Step: 7220... Loss: 1.3405... Val Loss: 1.7687\n","Epoch: 10/30... Step: 7230... Loss: 1.3328... Val Loss: 1.7250\n","Epoch: 10/30... Step: 7240... Loss: 1.3340... Val Loss: 1.7368\n","Epoch: 10/30... Step: 7250... Loss: 1.3358... Val Loss: 1.7009\n","Epoch: 10/30... Step: 7260... Loss: 1.3693... Val Loss: 1.6992\n","Epoch: 10/30... Step: 7270... Loss: 1.3611... Val Loss: 1.7327\n","Epoch: 10/30... Step: 7280... Loss: 1.3593... Val Loss: 1.7196\n","Epoch: 10/30... Step: 7290... Loss: 1.3282... Val Loss: 1.7188\n","Epoch: 10/30... Step: 7300... Loss: 1.3581... Val Loss: 1.7249\n","Epoch: 10/30... Step: 7310... Loss: 1.3486... Val Loss: 1.7350\n","Epoch: 10/30... Step: 7320... Loss: 1.3218... Val Loss: 1.7583\n","Epoch: 10/30... Step: 7330... Loss: 1.3250... Val Loss: 1.7827\n","Epoch: 10/30... Step: 7340... Loss: 1.3445... Val Loss: 1.7505\n","Epoch: 10/30... Step: 7350... Loss: 1.3406... Val Loss: 1.7622\n","Epoch: 10/30... Step: 7360... Loss: 1.3272... Val Loss: 1.7468\n","Epoch: 10/30... Step: 7370... Loss: 1.3486... Val Loss: 1.7401\n","Epoch: 10/30... Step: 7380... Loss: 1.3405... Val Loss: 1.7380\n","Epoch: 10/30... Step: 7390... Loss: 1.3472... Val Loss: 1.7380\n","Epoch: 10/30... Step: 7400... Loss: 1.3412... Val Loss: 1.6991\n","Epoch: 10/30... Step: 7410... Loss: 1.3386... Val Loss: 1.7765\n","Epoch: 10/30... Step: 7420... Loss: 1.3633... Val Loss: 1.7692\n","Epoch: 10/30... Step: 7430... Loss: 1.3467... Val Loss: 1.7659\n","Epoch: 10/30... Step: 7440... Loss: 1.3468... Val Loss: 1.7434\n","Epoch: 10/30... Step: 7450... Loss: 1.3214... Val Loss: 1.7453\n","Epoch: 10/30... Step: 7460... Loss: 1.3482... Val Loss: 1.7417\n","Epoch: 10/30... Step: 7470... Loss: 1.3525... Val Loss: 1.7310\n","Epoch: 10/30... Step: 7480... Loss: 1.3916... Val Loss: 1.7312\n","Epoch: 11/30... Step: 7490... Loss: 1.3552... Val Loss: 1.6916\n","Epoch: 11/30... Step: 7500... Loss: 1.3333... Val Loss: 1.7712\n","Epoch: 11/30... Step: 7510... Loss: 1.3258... Val Loss: 1.6747\n","Epoch: 11/30... Step: 7520... Loss: 1.3656... Val Loss: 1.6566\n","Epoch: 11/30... Step: 7530... Loss: 1.3413... Val Loss: 1.6497\n","Epoch: 11/30... Step: 7540... Loss: 1.3584... Val Loss: 1.6493\n","Epoch: 11/30... Step: 7550... Loss: 1.3662... Val Loss: 1.6468\n","Epoch: 11/30... Step: 7560... Loss: 1.3580... Val Loss: 1.6549\n","Epoch: 11/30... Step: 7570... Loss: 1.3357... Val Loss: 1.6547\n","Epoch: 11/30... Step: 7580... Loss: 1.3385... Val Loss: 1.6550\n","Epoch: 11/30... Step: 7590... Loss: 1.3455... Val Loss: 1.6754\n","Epoch: 11/30... Step: 7600... Loss: 1.3554... Val Loss: 1.6523\n","Epoch: 11/30... Step: 7610... Loss: 1.3431... Val Loss: 1.6537\n","Epoch: 11/30... Step: 7620... Loss: 1.3517... Val Loss: 1.6548\n","Epoch: 11/30... Step: 7630... Loss: 1.3681... Val Loss: 1.6655\n","Epoch: 11/30... Step: 7640... Loss: 1.3466... Val Loss: 1.6826\n","Epoch: 11/30... Step: 7650... Loss: 1.3487... Val Loss: 1.6601\n","Epoch: 11/30... Step: 7660... Loss: 1.3582... Val Loss: 1.6971\n","Epoch: 11/30... Step: 7670... Loss: 1.3068... Val Loss: 1.6717\n","Epoch: 11/30... Step: 7680... Loss: 1.3530... Val Loss: 1.6758\n","Epoch: 11/30... Step: 7690... Loss: 1.3427... Val Loss: 1.6838\n","Epoch: 11/30... Step: 7700... Loss: 1.3101... Val Loss: 1.6794\n","Epoch: 11/30... Step: 7710... Loss: 1.3421... Val Loss: 1.6515\n","Epoch: 11/30... Step: 7720... Loss: 1.3356... Val Loss: 1.6755\n","Epoch: 11/30... Step: 7730... Loss: 1.3452... Val Loss: 1.6536\n","Epoch: 11/30... Step: 7740... Loss: 1.3315... Val Loss: 1.7257\n","Epoch: 11/30... Step: 7750... Loss: 1.3536... Val Loss: 1.6657\n","Epoch: 11/30... Step: 7760... Loss: 1.3499... Val Loss: 1.6639\n","Epoch: 11/30... Step: 7770... Loss: 1.3577... Val Loss: 1.6637\n","Epoch: 11/30... Step: 7780... Loss: 1.3645... Val Loss: 1.6519\n","Epoch: 11/30... Step: 7790... Loss: 1.3555... Val Loss: 1.6344\n","Epoch: 11/30... Step: 7800... Loss: 1.3217... Val Loss: 1.6597\n","Epoch: 11/30... Step: 7810... Loss: 1.3362... Val Loss: 1.6742\n","Epoch: 11/30... Step: 7820... Loss: 1.3326... Val Loss: 1.7235\n","Epoch: 11/30... Step: 7830... Loss: 1.3416... Val Loss: 1.6947\n","Epoch: 11/30... Step: 7840... Loss: 1.3703... Val Loss: 1.7051\n","Epoch: 11/30... Step: 7850... Loss: 1.3211... Val Loss: 1.7063\n","Epoch: 11/30... Step: 7860... Loss: 1.3251... Val Loss: 1.7268\n","Epoch: 11/30... Step: 7870... Loss: 1.3388... Val Loss: 1.7055\n","Epoch: 11/30... Step: 7880... Loss: 1.2978... Val Loss: 1.6990\n","Epoch: 11/30... Step: 7890... Loss: 1.3495... Val Loss: 1.7280\n","Epoch: 11/30... Step: 7900... Loss: 1.3274... Val Loss: 1.7162\n","Epoch: 11/30... Step: 7910... Loss: 1.3367... Val Loss: 1.6972\n","Epoch: 11/30... Step: 7920... Loss: 1.3261... Val Loss: 1.7106\n","Epoch: 11/30... Step: 7930... Loss: 1.3325... Val Loss: 1.7173\n","Epoch: 11/30... Step: 7940... Loss: 1.3300... Val Loss: 1.7159\n","Epoch: 11/30... Step: 7950... Loss: 1.3394... Val Loss: 1.6951\n","Epoch: 11/30... Step: 7960... Loss: 1.3228... Val Loss: 1.7045\n","Epoch: 11/30... Step: 7970... Loss: 1.2981... Val Loss: 1.7047\n","Epoch: 11/30... Step: 7980... Loss: 1.3149... Val Loss: 1.7110\n","Epoch: 11/30... Step: 7990... Loss: 1.3063... Val Loss: 1.7131\n","Epoch: 11/30... Step: 8000... Loss: 1.3175... Val Loss: 1.6767\n","Epoch: 11/30... Step: 8010... Loss: 1.3289... Val Loss: 1.6736\n","Epoch: 11/30... Step: 8020... Loss: 1.3408... Val Loss: 1.7204\n","Epoch: 11/30... Step: 8030... Loss: 1.3238... Val Loss: 1.6901\n","Epoch: 11/30... Step: 8040... Loss: 1.3281... Val Loss: 1.6991\n","Epoch: 11/30... Step: 8050... Loss: 1.3209... Val Loss: 1.6846\n","Epoch: 11/30... Step: 8060... Loss: 1.3458... Val Loss: 1.7060\n","Epoch: 11/30... Step: 8070... Loss: 1.3171... Val Loss: 1.7311\n","Epoch: 11/30... Step: 8080... Loss: 1.3358... Val Loss: 1.7416\n","Epoch: 11/30... Step: 8090... Loss: 1.3382... Val Loss: 1.6899\n","Epoch: 11/30... Step: 8100... Loss: 1.3484... Val Loss: 1.7201\n","Epoch: 11/30... Step: 8110... Loss: 1.3157... Val Loss: 1.7065\n","Epoch: 11/30... Step: 8120... Loss: 1.3152... Val Loss: 1.6958\n","Epoch: 11/30... Step: 8130... Loss: 1.3226... Val Loss: 1.7202\n","Epoch: 11/30... Step: 8140... Loss: 1.3209... Val Loss: 1.6956\n","Epoch: 11/30... Step: 8150... Loss: 1.3377... Val Loss: 1.6796\n","Epoch: 11/30... Step: 8160... Loss: 1.3166... Val Loss: 1.7067\n","Epoch: 11/30... Step: 8170... Loss: 1.3217... Val Loss: 1.6924\n","Epoch: 11/30... Step: 8180... Loss: 1.3530... Val Loss: 1.6846\n","Epoch: 11/30... Step: 8190... Loss: 1.3342... Val Loss: 1.6880\n","Epoch: 11/30... Step: 8200... Loss: 1.3249... Val Loss: 1.6867\n","Epoch: 11/30... Step: 8210... Loss: 1.3240... Val Loss: 1.6616\n","Epoch: 11/30... Step: 8220... Loss: 1.3356... Val Loss: 1.6759\n","Epoch: 12/30... Step: 8230... Loss: 1.3237... Val Loss: 1.7856\n","Epoch: 12/30... Step: 8240... Loss: 1.2997... Val Loss: 1.7627\n","Epoch: 12/30... Step: 8250... Loss: 1.3452... Val Loss: 1.7814\n","Epoch: 12/30... Step: 8260... Loss: 1.3495... Val Loss: 1.7681\n","Epoch: 12/30... Step: 8270... Loss: 1.3370... Val Loss: 1.7764\n","Epoch: 12/30... Step: 8280... Loss: 1.3450... Val Loss: 1.7350\n","Epoch: 12/30... Step: 8290... Loss: 1.3252... Val Loss: 1.7429\n","Epoch: 12/30... Step: 8300... Loss: 1.3427... Val Loss: 1.7448\n","Epoch: 12/30... Step: 8310... Loss: 1.3133... Val Loss: 1.7504\n","Epoch: 12/30... Step: 8320... Loss: 1.3443... Val Loss: 1.7656\n","Epoch: 12/30... Step: 8330... Loss: 1.3449... Val Loss: 1.7435\n","Epoch: 12/30... Step: 8340... Loss: 1.3228... Val Loss: 1.7722\n","Epoch: 12/30... Step: 8350... Loss: 1.3633... Val Loss: 1.7574\n","Epoch: 12/30... Step: 8360... Loss: 1.3115... Val Loss: 1.7610\n","Epoch: 12/30... Step: 8370... Loss: 1.3279... Val Loss: 1.7730\n","Epoch: 12/30... Step: 8380... Loss: 1.3039... Val Loss: 1.7634\n","Epoch: 12/30... Step: 8390... Loss: 1.3325... Val Loss: 1.7661\n","Epoch: 12/30... Step: 8400... Loss: 1.3338... Val Loss: 1.7778\n","Epoch: 12/30... Step: 8410... Loss: 1.2968... Val Loss: 1.7730\n","Epoch: 12/30... Step: 8420... Loss: 1.3383... Val Loss: 1.7460\n","Epoch: 12/30... Step: 8430... Loss: 1.3411... Val Loss: 1.7857\n","Epoch: 12/30... Step: 8440... Loss: 1.3346... Val Loss: 1.7719\n","Epoch: 12/30... Step: 8450... Loss: 1.3052... Val Loss: 1.7680\n","Epoch: 12/30... Step: 8460... Loss: 1.3194... Val Loss: 1.7520\n","Epoch: 12/30... Step: 8470... Loss: 1.3157... Val Loss: 1.7466\n","Epoch: 12/30... Step: 8480... Loss: 1.3448... Val Loss: 1.7485\n","Epoch: 12/30... Step: 8490... Loss: 1.3073... Val Loss: 1.7818\n","Epoch: 12/30... Step: 8500... Loss: 1.3335... Val Loss: 1.7570\n","Epoch: 12/30... Step: 8510... Loss: 1.3384... Val Loss: 1.7475\n","Epoch: 12/30... Step: 8520... Loss: 1.3488... Val Loss: 1.7609\n","Epoch: 12/30... Step: 8530... Loss: 1.3263... Val Loss: 1.7664\n","Epoch: 12/30... Step: 8540... Loss: 1.3225... Val Loss: 1.7599\n","Epoch: 12/30... Step: 8550... Loss: 1.3373... Val Loss: 1.7698\n","Epoch: 12/30... Step: 8560... Loss: 1.2949... Val Loss: 1.7731\n","Epoch: 12/30... Step: 8570... Loss: 1.3283... Val Loss: 1.7524\n","Epoch: 12/30... Step: 8580... Loss: 1.3173... Val Loss: 1.7862\n","Epoch: 12/30... Step: 8590... Loss: 1.3398... Val Loss: 1.7729\n","Epoch: 12/30... Step: 8600... Loss: 1.3263... Val Loss: 1.7697\n","Epoch: 12/30... Step: 8610... Loss: 1.3450... Val Loss: 1.7774\n","Epoch: 12/30... Step: 8620... Loss: 1.3317... Val Loss: 1.7753\n","Epoch: 12/30... Step: 8630... Loss: 1.3339... Val Loss: 1.7575\n","Epoch: 12/30... Step: 8640... Loss: 1.3252... Val Loss: 1.7565\n","Epoch: 12/30... Step: 8650... Loss: 1.3270... Val Loss: 1.7753\n","Epoch: 12/30... Step: 8660... Loss: 1.3279... Val Loss: 1.7752\n","Epoch: 12/30... Step: 8670... Loss: 1.3270... Val Loss: 1.7518\n","Epoch: 12/30... Step: 8680... Loss: 1.3357... Val Loss: 1.7645\n","Epoch: 12/30... Step: 8690... Loss: 1.3404... Val Loss: 1.7718\n","Epoch: 12/30... Step: 8700... Loss: 1.2912... Val Loss: 1.7648\n","Epoch: 12/30... Step: 8710... Loss: 1.3467... Val Loss: 1.7662\n","Epoch: 12/30... Step: 8720... Loss: 1.3095... Val Loss: 1.7752\n","Epoch: 12/30... Step: 8730... Loss: 1.3281... Val Loss: 1.7737\n","Epoch: 12/30... Step: 8740... Loss: 1.3442... Val Loss: 1.7665\n","Epoch: 12/30... Step: 8750... Loss: 1.2978... Val Loss: 1.7484\n","Epoch: 12/30... Step: 8760... Loss: 1.3349... Val Loss: 1.7519\n","Epoch: 12/30... Step: 8770... Loss: 1.3205... Val Loss: 1.7573\n","Epoch: 12/30... Step: 8780... Loss: 1.2843... Val Loss: 1.7847\n","Epoch: 12/30... Step: 8790... Loss: 1.3021... Val Loss: 1.7753\n","Epoch: 12/30... Step: 8800... Loss: 1.3564... Val Loss: 1.7727\n","Epoch: 12/30... Step: 8810... Loss: 1.3077... Val Loss: 1.7572\n","Epoch: 12/30... Step: 8820... Loss: 1.3281... Val Loss: 1.7789\n","Epoch: 12/30... Step: 8830... Loss: 1.3139... Val Loss: 1.7891\n","Epoch: 12/30... Step: 8840... Loss: 1.3246... Val Loss: 1.7713\n","Epoch: 12/30... Step: 8850... Loss: 1.3175... Val Loss: 1.7772\n","Epoch: 12/30... Step: 8860... Loss: 1.3296... Val Loss: 1.7877\n","Epoch: 12/30... Step: 8870... Loss: 1.3067... Val Loss: 1.7869\n","Epoch: 12/30... Step: 8880... Loss: 1.3098... Val Loss: 1.7722\n","Epoch: 12/30... Step: 8890... Loss: 1.3005... Val Loss: 1.7845\n","Epoch: 12/30... Step: 8900... Loss: 1.3122... Val Loss: 1.7585\n","Epoch: 12/30... Step: 8910... Loss: 1.2987... Val Loss: 1.7792\n","Epoch: 12/30... Step: 8920... Loss: 1.3178... Val Loss: 1.7843\n","Epoch: 12/30... Step: 8930... Loss: 1.2829... Val Loss: 1.7865\n","Epoch: 12/30... Step: 8940... Loss: 1.3212... Val Loss: 1.7817\n","Epoch: 12/30... Step: 8950... Loss: 1.3196... Val Loss: 1.7743\n","Epoch: 12/30... Step: 8960... Loss: 1.3180... Val Loss: 1.7657\n","Epoch: 12/30... Step: 8970... Loss: 1.3026... Val Loss: 1.7613\n","Epoch: 13/30... Step: 8980... Loss: 1.3216... Val Loss: 1.7310\n","Epoch: 13/30... Step: 8990... Loss: 1.3313... Val Loss: 1.6950\n","Epoch: 13/30... Step: 9000... Loss: 1.3047... Val Loss: 1.6887\n","Epoch: 13/30... Step: 9010... Loss: 1.3316... Val Loss: 1.6896\n","Epoch: 13/30... Step: 9020... Loss: 1.3414... Val Loss: 1.6924\n","Epoch: 13/30... Step: 9030... Loss: 1.3132... Val Loss: 1.6609\n","Epoch: 13/30... Step: 9040... Loss: 1.3114... Val Loss: 1.6704\n","Epoch: 13/30... Step: 9050... Loss: 1.3199... Val Loss: 1.6477\n","Epoch: 13/30... Step: 9060... Loss: 1.3301... Val Loss: 1.6723\n","Epoch: 13/30... Step: 9070... Loss: 1.3068... Val Loss: 1.6921\n","Epoch: 13/30... Step: 9080... Loss: 1.3439... Val Loss: 1.6881\n","Epoch: 13/30... Step: 9090... Loss: 1.3096... Val Loss: 1.6834\n","Epoch: 13/30... Step: 9100... Loss: 1.3296... Val Loss: 1.6677\n","Epoch: 13/30... Step: 9110... Loss: 1.3304... Val Loss: 1.6808\n","Epoch: 13/30... Step: 9120... Loss: 1.3130... Val Loss: 1.6796\n","Epoch: 13/30... Step: 9130... Loss: 1.3202... Val Loss: 1.6926\n","Epoch: 13/30... Step: 9140... Loss: 1.3327... Val Loss: 1.6668\n","Epoch: 13/30... Step: 9150... Loss: 1.3270... Val Loss: 1.6854\n","Epoch: 13/30... Step: 9160... Loss: 1.3435... Val Loss: 1.6946\n","Epoch: 13/30... Step: 9170... Loss: 1.2869... Val Loss: 1.7007\n","Epoch: 13/30... Step: 9180... Loss: 1.2965... Val Loss: 1.7106\n","Epoch: 13/30... Step: 9190... Loss: 1.3596... Val Loss: 1.7103\n","Epoch: 13/30... Step: 9200... Loss: 1.2990... Val Loss: 1.6932\n","Epoch: 13/30... Step: 9210... Loss: 1.3183... Val Loss: 1.6940\n","Epoch: 13/30... Step: 9220... Loss: 1.3518... Val Loss: 1.7019\n","Epoch: 13/30... Step: 9230... Loss: 1.3302... Val Loss: 1.6806\n","Epoch: 13/30... Step: 9240... Loss: 1.3317... Val Loss: 1.7228\n","Epoch: 13/30... Step: 9250... Loss: 1.3312... Val Loss: 1.6905\n","Epoch: 13/30... Step: 9260... Loss: 1.3144... Val Loss: 1.6744\n","Epoch: 13/30... Step: 9270... Loss: 1.2916... Val Loss: 1.7061\n","Epoch: 13/30... Step: 9280... Loss: 1.3259... Val Loss: 1.6957\n","Epoch: 13/30... Step: 9290... Loss: 1.3106... Val Loss: 1.6935\n","Epoch: 13/30... Step: 9300... Loss: 1.3255... Val Loss: 1.7098\n","Epoch: 13/30... Step: 9310... Loss: 1.3155... Val Loss: 1.7291\n","Epoch: 13/30... Step: 9320... Loss: 1.3369... Val Loss: 1.6889\n","Epoch: 13/30... Step: 9330... Loss: 1.3358... Val Loss: 1.7290\n","Epoch: 13/30... Step: 9340... Loss: 1.3152... Val Loss: 1.7698\n","Epoch: 13/30... Step: 9350... Loss: 1.3093... Val Loss: 1.7456\n","Epoch: 13/30... Step: 9360... Loss: 1.3213... Val Loss: 1.7785\n","Epoch: 13/30... Step: 9370... Loss: 1.3242... Val Loss: 1.6965\n","Epoch: 13/30... Step: 9380... Loss: 1.3294... Val Loss: 1.6938\n","Epoch: 13/30... Step: 9390... Loss: 1.3464... Val Loss: 1.7011\n","Epoch: 13/30... Step: 9400... Loss: 1.3216... Val Loss: 1.7026\n","Epoch: 13/30... Step: 9410... Loss: 1.3007... Val Loss: 1.7113\n","Epoch: 13/30... Step: 9420... Loss: 1.2802... Val Loss: 1.6859\n","Epoch: 13/30... Step: 9430... Loss: 1.2942... Val Loss: 1.6792\n","Epoch: 13/30... Step: 9440... Loss: 1.3353... Val Loss: 1.6941\n","Epoch: 13/30... Step: 9450... Loss: 1.3125... Val Loss: 1.6885\n","Epoch: 13/30... Step: 9460... Loss: 1.3242... Val Loss: 1.7093\n","Epoch: 13/30... Step: 9470... Loss: 1.3238... Val Loss: 1.7037\n","Epoch: 13/30... Step: 9480... Loss: 1.3089... Val Loss: 1.7187\n","Epoch: 13/30... Step: 9490... Loss: 1.3652... Val Loss: 1.7646\n","Epoch: 13/30... Step: 9500... Loss: 1.3008... Val Loss: 1.7201\n","Epoch: 13/30... Step: 9510... Loss: 1.3368... Val Loss: 1.7222\n","Epoch: 13/30... Step: 9520... Loss: 1.3146... Val Loss: 1.7383\n","Epoch: 13/30... Step: 9530... Loss: 1.3052... Val Loss: 1.7544\n","Epoch: 13/30... Step: 9540... Loss: 1.3221... Val Loss: 1.7228\n","Epoch: 13/30... Step: 9550... Loss: 1.3094... Val Loss: 1.7241\n","Epoch: 13/30... Step: 9560... Loss: 1.3127... Val Loss: 1.7455\n","Epoch: 13/30... Step: 9570... Loss: 1.3210... Val Loss: 1.7420\n","Epoch: 13/30... Step: 9580... Loss: 1.3251... Val Loss: 1.7589\n","Epoch: 13/30... Step: 9590... Loss: 1.3180... Val Loss: 1.7104\n","Epoch: 13/30... Step: 9600... Loss: 1.3243... Val Loss: 1.7184\n","Epoch: 13/30... Step: 9610... Loss: 1.3288... Val Loss: 1.7061\n","Epoch: 13/30... Step: 9620... Loss: 1.2985... Val Loss: 1.7040\n","Epoch: 13/30... Step: 9630... Loss: 1.3205... Val Loss: 1.7032\n","Epoch: 13/30... Step: 9640... Loss: 1.3271... Val Loss: 1.7042\n","Epoch: 13/30... Step: 9650... Loss: 1.3077... Val Loss: 1.6808\n","Epoch: 13/30... Step: 9660... Loss: 1.2869... Val Loss: 1.6987\n","Epoch: 13/30... Step: 9670... Loss: 1.3272... Val Loss: 1.7064\n","Epoch: 13/30... Step: 9680... Loss: 1.2953... Val Loss: 1.7313\n","Epoch: 13/30... Step: 9690... Loss: 1.3097... Val Loss: 1.7497\n","Epoch: 13/30... Step: 9700... Loss: 1.3153... Val Loss: 1.7084\n","Epoch: 13/30... Step: 9710... Loss: 1.2956... Val Loss: 1.7021\n","Epoch: 13/30... Step: 9720... Loss: 1.3065... Val Loss: 1.7273\n","Epoch: 14/30... Step: 9730... Loss: 1.3233... Val Loss: 1.7394\n","Epoch: 14/30... Step: 9740... Loss: 1.3077... Val Loss: 1.7325\n","Epoch: 14/30... Step: 9750... Loss: 1.3311... Val Loss: 1.7404\n","Epoch: 14/30... Step: 9760... Loss: 1.2930... Val Loss: 1.6981\n","Epoch: 14/30... Step: 9770... Loss: 1.3401... Val Loss: 1.7032\n","Epoch: 14/30... Step: 9780... Loss: 1.3084... Val Loss: 1.7144\n","Epoch: 14/30... Step: 9790... Loss: 1.3059... Val Loss: 1.7558\n","Epoch: 14/30... Step: 9800... Loss: 1.3296... Val Loss: 1.7282\n","Epoch: 14/30... Step: 9810... Loss: 1.3219... Val Loss: 1.7392\n","Epoch: 14/30... Step: 9820... Loss: 1.3026... Val Loss: 1.7460\n","Epoch: 14/30... Step: 9830... Loss: 1.3036... Val Loss: 1.7326\n","Epoch: 14/30... Step: 9840... Loss: 1.3204... Val Loss: 1.7575\n","Epoch: 14/30... Step: 9850... Loss: 1.2983... Val Loss: 1.7480\n","Epoch: 14/30... Step: 9860... Loss: 1.2888... Val Loss: 1.7756\n","Epoch: 14/30... Step: 9870... Loss: 1.3048... Val Loss: 1.7454\n","Epoch: 14/30... Step: 9880... Loss: 1.3283... Val Loss: 1.7608\n","Epoch: 14/30... Step: 9890... Loss: 1.3196... Val Loss: 1.7391\n","Epoch: 14/30... Step: 9900... Loss: 1.3108... Val Loss: 1.7492\n","Epoch: 14/30... Step: 9910... Loss: 1.3286... Val Loss: 1.7390\n","Epoch: 14/30... Step: 9920... Loss: 1.3113... Val Loss: 1.7623\n","Epoch: 14/30... Step: 9930... Loss: 1.3323... Val Loss: 1.7514\n","Epoch: 14/30... Step: 9940... Loss: 1.2997... Val Loss: 1.7526\n","Epoch: 14/30... Step: 9950... Loss: 1.2987... Val Loss: 1.7449\n","Epoch: 14/30... Step: 9960... Loss: 1.3347... Val Loss: 1.7464\n","Epoch: 14/30... Step: 9970... Loss: 1.3282... Val Loss: 1.7452\n","Epoch: 14/30... Step: 9980... Loss: 1.3301... Val Loss: 1.7521\n","Epoch: 14/30... Step: 9990... Loss: 1.3252... Val Loss: 1.7749\n","Epoch: 14/30... Step: 10000... Loss: 1.3154... Val Loss: 1.7591\n","Epoch: 14/30... Step: 10010... Loss: 1.3216... Val Loss: 1.7525\n","Epoch: 14/30... Step: 10020... Loss: 1.3329... Val Loss: 1.7635\n","Epoch: 14/30... Step: 10030... Loss: 1.3181... Val Loss: 1.7685\n","Epoch: 14/30... Step: 10040... Loss: 1.3223... Val Loss: 1.7707\n","Epoch: 14/30... Step: 10050... Loss: 1.3120... Val Loss: 1.7710\n","Epoch: 14/30... Step: 10060... Loss: 1.2999... Val Loss: 1.7580\n","Epoch: 14/30... Step: 10070... Loss: 1.2893... Val Loss: 1.7779\n","Epoch: 14/30... Step: 10080... Loss: 1.3364... Val Loss: 1.7718\n","Epoch: 14/30... Step: 10090... Loss: 1.3144... Val Loss: 1.7882\n","Epoch: 14/30... Step: 10100... Loss: 1.3108... Val Loss: 1.7876\n","Epoch: 14/30... Step: 10110... Loss: 1.2925... Val Loss: 1.7859\n","Epoch: 14/30... Step: 10120... Loss: 1.2878... Val Loss: 1.7845\n","Epoch: 14/30... Step: 10130... Loss: 1.2819... Val Loss: 1.7688\n","Epoch: 14/30... Step: 10140... Loss: 1.3016... Val Loss: 1.7558\n","Epoch: 14/30... Step: 10150... Loss: 1.3119... Val Loss: 1.7686\n","Epoch: 14/30... Step: 10160... Loss: 1.3085... Val Loss: 1.7678\n","Epoch: 14/30... Step: 10170... Loss: 1.3035... Val Loss: 1.7632\n","Epoch: 14/30... Step: 10180... Loss: 1.3103... Val Loss: 1.7418\n","Epoch: 14/30... Step: 10190... Loss: 1.3117... Val Loss: 1.7551\n","Epoch: 14/30... Step: 10200... Loss: 1.3063... Val Loss: 1.7659\n","Epoch: 14/30... Step: 10210... Loss: 1.2970... Val Loss: 1.7727\n","Epoch: 14/30... Step: 10220... Loss: 1.3269... Val Loss: 1.7799\n","Epoch: 14/30... Step: 10230... Loss: 1.2702... Val Loss: 1.7744\n","Epoch: 14/30... Step: 10240... Loss: 1.3007... Val Loss: 1.7688\n","Epoch: 14/30... Step: 10250... Loss: 1.3102... Val Loss: 1.7598\n","Epoch: 14/30... Step: 10260... Loss: 1.2968... Val Loss: 1.7625\n","Epoch: 14/30... Step: 10270... Loss: 1.3167... Val Loss: 1.7568\n","Epoch: 14/30... Step: 10280... Loss: 1.3263... Val Loss: 1.7652\n","Epoch: 14/30... Step: 10290... Loss: 1.2892... Val Loss: 1.7791\n","Epoch: 14/30... Step: 10300... Loss: 1.3118... Val Loss: 1.7766\n","Epoch: 14/30... Step: 10310... Loss: 1.2857... Val Loss: 1.7700\n","Epoch: 14/30... Step: 10320... Loss: 1.3135... Val Loss: 1.7730\n","Epoch: 14/30... Step: 10330... Loss: 1.2955... Val Loss: 1.7969\n","Epoch: 14/30... Step: 10340... Loss: 1.3135... Val Loss: 1.7738\n","Epoch: 14/30... Step: 10350... Loss: 1.3161... Val Loss: 1.7747\n","Epoch: 14/30... Step: 10360... Loss: 1.3110... Val Loss: 1.7709\n","Epoch: 14/30... Step: 10370... Loss: 1.3278... Val Loss: 1.7669\n","Epoch: 14/30... Step: 10380... Loss: 1.3062... Val Loss: 1.7546\n","Epoch: 14/30... Step: 10390... Loss: 1.3136... Val Loss: 1.7792\n","Epoch: 14/30... Step: 10400... Loss: 1.2773... Val Loss: 1.7294\n","Epoch: 14/30... Step: 10410... Loss: 1.2936... Val Loss: 1.7676\n","Epoch: 14/30... Step: 10420... Loss: 1.3071... Val Loss: 1.7696\n","Epoch: 14/30... Step: 10430... Loss: 1.3051... Val Loss: 1.7623\n","Epoch: 14/30... Step: 10440... Loss: 1.2946... Val Loss: 1.7711\n","Epoch: 14/30... Step: 10450... Loss: 1.2672... Val Loss: 1.7501\n","Epoch: 14/30... Step: 10460... Loss: 1.2882... Val Loss: 1.7590\n","Epoch: 14/30... Step: 10470... Loss: 1.3027... Val Loss: 1.7798\n","Epoch: 15/30... Step: 10480... Loss: 1.2952... Val Loss: 1.7790\n","Epoch: 15/30... Step: 10490... Loss: 1.3171... Val Loss: 1.7560\n","Epoch: 15/30... Step: 10500... Loss: 1.3213... Val Loss: 1.7607\n","Epoch: 15/30... Step: 10510... Loss: 1.3255... Val Loss: 1.7796\n","Epoch: 15/30... Step: 10520... Loss: 1.3253... Val Loss: 1.7849\n","Epoch: 15/30... Step: 10530... Loss: 1.3310... Val Loss: 1.7921\n","Epoch: 15/30... Step: 10540... Loss: 1.2974... Val Loss: 1.7818\n","Epoch: 15/30... Step: 10550... Loss: 1.3458... Val Loss: 1.7660\n","Epoch: 15/30... Step: 10560... Loss: 1.3041... Val Loss: 1.7844\n","Epoch: 15/30... Step: 10570... Loss: 1.2968... Val Loss: 1.7739\n","Epoch: 15/30... Step: 10580... Loss: 1.3003... Val Loss: 1.7806\n","Epoch: 15/30... Step: 10590... Loss: 1.2946... Val Loss: 1.7850\n","Epoch: 15/30... Step: 10600... Loss: 1.3162... Val Loss: 1.7399\n","Epoch: 15/30... Step: 10610... Loss: 1.3156... Val Loss: 1.7692\n","Epoch: 15/30... Step: 10620... Loss: 1.2903... Val Loss: 1.7586\n","Epoch: 15/30... Step: 10630... Loss: 1.2997... Val Loss: 1.7931\n","Epoch: 15/30... Step: 10640... Loss: 1.3089... Val Loss: 1.7670\n","Epoch: 15/30... Step: 10650... Loss: 1.3158... Val Loss: 1.7713\n","Epoch: 15/30... Step: 10660... Loss: 1.3378... Val Loss: 1.7691\n","Epoch: 15/30... Step: 10670... Loss: 1.3219... Val Loss: 1.7633\n","Epoch: 15/30... Step: 10680... Loss: 1.2993... Val Loss: 1.7708\n","Epoch: 15/30... Step: 10690... Loss: 1.2907... Val Loss: 1.7367\n","Epoch: 15/30... Step: 10700... Loss: 1.3011... Val Loss: 1.7241\n","Epoch: 15/30... Step: 10710... Loss: 1.3233... Val Loss: 1.7553\n","Epoch: 15/30... Step: 10720... Loss: 1.2896... Val Loss: 1.7423\n","Epoch: 15/30... Step: 10730... Loss: 1.3030... Val Loss: 1.7534\n","Epoch: 15/30... Step: 10740... Loss: 1.2948... Val Loss: 1.7491\n","Epoch: 15/30... Step: 10750... Loss: 1.3060... Val Loss: 1.7562\n","Epoch: 15/30... Step: 10760... Loss: 1.3143... Val Loss: 1.7590\n","Epoch: 15/30... Step: 10770... Loss: 1.3118... Val Loss: 1.7618\n","Epoch: 15/30... Step: 10780... Loss: 1.3058... Val Loss: 1.7649\n","Epoch: 15/30... Step: 10790... Loss: 1.3230... Val Loss: 1.7766\n","Epoch: 15/30... Step: 10800... Loss: 1.2837... Val Loss: 1.7868\n","Epoch: 15/30... Step: 10810... Loss: 1.3408... Val Loss: 1.7869\n","Epoch: 15/30... Step: 10820... Loss: 1.2981... Val Loss: 1.7720\n","Epoch: 15/30... Step: 10830... Loss: 1.3159... Val Loss: 1.7465\n","Epoch: 15/30... Step: 10840... Loss: 1.3030... Val Loss: 1.7582\n","Epoch: 15/30... Step: 10850... Loss: 1.2943... Val Loss: 1.7714\n","Epoch: 15/30... Step: 10860... Loss: 1.3069... Val Loss: 1.7738\n","Epoch: 15/30... Step: 10870... Loss: 1.2788... Val Loss: 1.7621\n","Epoch: 15/30... Step: 10880... Loss: 1.2719... Val Loss: 1.7859\n","Epoch: 15/30... Step: 10890... Loss: 1.2585... Val Loss: 1.7926\n","Epoch: 15/30... Step: 10900... Loss: 1.3043... Val Loss: 1.7754\n","Epoch: 15/30... Step: 10910... Loss: 1.2934... Val Loss: 1.7769\n","Epoch: 15/30... Step: 10920... Loss: 1.3013... Val Loss: 1.7898\n","Epoch: 15/30... Step: 10930... Loss: 1.3114... Val Loss: 1.7822\n","Epoch: 15/30... Step: 10940... Loss: 1.3104... Val Loss: 1.7757\n","Epoch: 15/30... Step: 10950... Loss: 1.3101... Val Loss: 1.7584\n","Epoch: 15/30... Step: 10960... Loss: 1.2976... Val Loss: 1.7951\n","Epoch: 15/30... Step: 10970... Loss: 1.2913... Val Loss: 1.7903\n","Epoch: 15/30... Step: 10980... Loss: 1.3048... Val Loss: 1.7749\n","Epoch: 15/30... Step: 10990... Loss: 1.2966... Val Loss: 1.7747\n","Epoch: 15/30... Step: 11000... Loss: 1.3317... Val Loss: 1.7715\n","Epoch: 15/30... Step: 11010... Loss: 1.3232... Val Loss: 1.7787\n","Epoch: 15/30... Step: 11020... Loss: 1.3185... Val Loss: 1.7881\n","Epoch: 15/30... Step: 11030... Loss: 1.2874... Val Loss: 1.7827\n","Epoch: 15/30... Step: 11040... Loss: 1.3083... Val Loss: 1.7747\n","Epoch: 15/30... Step: 11050... Loss: 1.3055... Val Loss: 1.7762\n","Epoch: 15/30... Step: 11060... Loss: 1.2808... Val Loss: 1.7912\n","Epoch: 15/30... Step: 11070... Loss: 1.2827... Val Loss: 1.7964\n","Epoch: 15/30... Step: 11080... Loss: 1.2973... Val Loss: 1.7938\n","Epoch: 15/30... Step: 11090... Loss: 1.2981... Val Loss: 1.7932\n","Epoch: 15/30... Step: 11100... Loss: 1.2807... Val Loss: 1.7940\n","Epoch: 15/30... Step: 11110... Loss: 1.3029... Val Loss: 1.7832\n","Epoch: 15/30... Step: 11120... Loss: 1.2920... Val Loss: 1.7780\n","Epoch: 15/30... Step: 11130... Loss: 1.3082... Val Loss: 1.7697\n","Epoch: 15/30... Step: 11140... Loss: 1.3020... Val Loss: 1.7774\n","Epoch: 15/30... Step: 11150... Loss: 1.2890... Val Loss: 1.7848\n","Epoch: 15/30... Step: 11160... Loss: 1.3279... Val Loss: 1.7714\n","Epoch: 15/30... Step: 11170... Loss: 1.3067... Val Loss: 1.7901\n","Epoch: 15/30... Step: 11180... Loss: 1.3065... Val Loss: 1.7727\n","Epoch: 15/30... Step: 11190... Loss: 1.2951... Val Loss: 1.7835\n","Epoch: 15/30... Step: 11200... Loss: 1.3133... Val Loss: 1.7914\n","Epoch: 15/30... Step: 11210... Loss: 1.3192... Val Loss: 1.7873\n","Epoch: 15/30... Step: 11220... Loss: 1.3508... Val Loss: 1.8007\n","Epoch: 16/30... Step: 11230... Loss: 1.3000... Val Loss: 1.6861\n","Epoch: 16/30... Step: 11240... Loss: 1.2920... Val Loss: 1.6646\n","Epoch: 16/30... Step: 11250... Loss: 1.2953... Val Loss: 1.6538\n","Epoch: 16/30... Step: 11260... Loss: 1.3262... Val Loss: 1.6546\n","Epoch: 16/30... Step: 11270... Loss: 1.3013... Val Loss: 1.6466\n","Epoch: 16/30... Step: 11280... Loss: 1.3138... Val Loss: 1.6468\n","Epoch: 16/30... Step: 11290... Loss: 1.3258... Val Loss: 1.6291\n","Epoch: 16/30... Step: 11300... Loss: 1.3144... Val Loss: 1.6608\n","Epoch: 16/30... Step: 11310... Loss: 1.2945... Val Loss: 1.6646\n","Epoch: 16/30... Step: 11320... Loss: 1.3040... Val Loss: 1.6802\n","Epoch: 16/30... Step: 11330... Loss: 1.3162... Val Loss: 1.6606\n","Epoch: 16/30... Step: 11340... Loss: 1.3226... Val Loss: 1.6733\n","Epoch: 16/30... Step: 11350... Loss: 1.3004... Val Loss: 1.6484\n","Epoch: 16/30... Step: 11360... Loss: 1.3100... Val Loss: 1.6712\n","Epoch: 16/30... Step: 11370... Loss: 1.3292... Val Loss: 1.6890\n","Epoch: 16/30... Step: 11380... Loss: 1.2996... Val Loss: 1.6932\n","Epoch: 16/30... Step: 11390... Loss: 1.3065... Val Loss: 1.6607\n","Epoch: 16/30... Step: 11400... Loss: 1.3251... Val Loss: 1.7067\n","Epoch: 16/30... Step: 11410... Loss: 1.2723... Val Loss: 1.6833\n","Epoch: 16/30... Step: 11420... Loss: 1.3072... Val Loss: 1.6918\n","Epoch: 16/30... Step: 11430... Loss: 1.3009... Val Loss: 1.6883\n","Epoch: 16/30... Step: 11440... Loss: 1.2711... Val Loss: 1.6867\n","Epoch: 16/30... Step: 11450... Loss: 1.3028... Val Loss: 1.6673\n","Epoch: 16/30... Step: 11460... Loss: 1.2949... Val Loss: 1.6868\n","Epoch: 16/30... Step: 11470... Loss: 1.3117... Val Loss: 1.6828\n","Epoch: 16/30... Step: 11480... Loss: 1.2873... Val Loss: 1.7441\n","Epoch: 16/30... Step: 11490... Loss: 1.3230... Val Loss: 1.6852\n","Epoch: 16/30... Step: 11500... Loss: 1.3069... Val Loss: 1.7118\n","Epoch: 16/30... Step: 11510... Loss: 1.3137... Val Loss: 1.6827\n","Epoch: 16/30... Step: 11520... Loss: 1.3222... Val Loss: 1.7088\n","Epoch: 16/30... Step: 11530... Loss: 1.3010... Val Loss: 1.7107\n","Epoch: 16/30... Step: 11540... Loss: 1.2789... Val Loss: 1.7147\n","Epoch: 16/30... Step: 11550... Loss: 1.2972... Val Loss: 1.7611\n","Epoch: 16/30... Step: 11560... Loss: 1.2857... Val Loss: 1.7230\n","Epoch: 16/30... Step: 11570... Loss: 1.2906... Val Loss: 1.7871\n","Epoch: 16/30... Step: 11580... Loss: 1.3259... Val Loss: 1.7238\n","Epoch: 16/30... Step: 11590... Loss: 1.2784... Val Loss: 1.7050\n","Epoch: 16/30... Step: 11600... Loss: 1.2987... Val Loss: 1.7614\n","Epoch: 16/30... Step: 11610... Loss: 1.3040... Val Loss: 1.7371\n","Epoch: 16/30... Step: 11620... Loss: 1.2646... Val Loss: 1.7670\n","Epoch: 16/30... Step: 11630... Loss: 1.3112... Val Loss: 1.7479\n","Epoch: 16/30... Step: 11640... Loss: 1.2941... Val Loss: 1.7631\n","Epoch: 16/30... Step: 11650... Loss: 1.3013... Val Loss: 1.7124\n","Epoch: 16/30... Step: 11660... Loss: 1.2899... Val Loss: 1.6764\n","Epoch: 16/30... Step: 11670... Loss: 1.3008... Val Loss: 1.7234\n","Epoch: 16/30... Step: 11680... Loss: 1.2935... Val Loss: 1.6942\n","Epoch: 16/30... Step: 11690... Loss: 1.3063... Val Loss: 1.7224\n","Epoch: 16/30... Step: 11700... Loss: 1.2754... Val Loss: 1.7001\n","Epoch: 16/30... Step: 11710... Loss: 1.2652... Val Loss: 1.6958\n","Epoch: 16/30... Step: 11720... Loss: 1.2743... Val Loss: 1.6957\n","Epoch: 16/30... Step: 11730... Loss: 1.2686... Val Loss: 1.7350\n","Epoch: 16/30... Step: 11740... Loss: 1.2704... Val Loss: 1.6816\n","Epoch: 16/30... Step: 11750... Loss: 1.2829... Val Loss: 1.7102\n","Epoch: 16/30... Step: 11760... Loss: 1.3062... Val Loss: 1.7173\n","Epoch: 16/30... Step: 11770... Loss: 1.2888... Val Loss: 1.7145\n","Epoch: 16/30... Step: 11780... Loss: 1.2838... Val Loss: 1.7200\n","Epoch: 16/30... Step: 11790... Loss: 1.2922... Val Loss: 1.6982\n","Epoch: 16/30... Step: 11800... Loss: 1.3034... Val Loss: 1.7264\n","Epoch: 16/30... Step: 11810... Loss: 1.2933... Val Loss: 1.7519\n","Epoch: 16/30... Step: 11820... Loss: 1.2977... Val Loss: 1.7900\n","Epoch: 16/30... Step: 11830... Loss: 1.2882... Val Loss: 1.7381\n","Epoch: 16/30... Step: 11840... Loss: 1.3115... Val Loss: 1.7162\n","Epoch: 16/30... Step: 11850... Loss: 1.2732... Val Loss: 1.7216\n","Epoch: 16/30... Step: 11860... Loss: 1.2754... Val Loss: 1.7171\n","Epoch: 16/30... Step: 11870... Loss: 1.2968... Val Loss: 1.7356\n","Epoch: 16/30... Step: 11880... Loss: 1.2854... Val Loss: 1.7190\n","Epoch: 16/30... Step: 11890... Loss: 1.2952... Val Loss: 1.6998\n","Epoch: 16/30... Step: 11900... Loss: 1.2812... Val Loss: 1.7223\n","Epoch: 16/30... Step: 11910... Loss: 1.2812... Val Loss: 1.6933\n","Epoch: 16/30... Step: 11920... Loss: 1.3106... Val Loss: 1.7759\n","Epoch: 16/30... Step: 11930... Loss: 1.2866... Val Loss: 1.7472\n","Epoch: 16/30... Step: 11940... Loss: 1.2868... Val Loss: 1.7410\n","Epoch: 16/30... Step: 11950... Loss: 1.2883... Val Loss: 1.7009\n","Epoch: 16/30... Step: 11960... Loss: 1.3005... Val Loss: 1.7207\n","Epoch: 17/30... Step: 11970... Loss: 1.2921... Val Loss: 1.8161\n","Epoch: 17/30... Step: 11980... Loss: 1.2698... Val Loss: 1.7998\n","Epoch: 17/30... Step: 11990... Loss: 1.3074... Val Loss: 1.7726\n","Epoch: 17/30... Step: 12000... Loss: 1.3075... Val Loss: 1.7891\n","Epoch: 17/30... Step: 12010... Loss: 1.2976... Val Loss: 1.7717\n","Epoch: 17/30... Step: 12020... Loss: 1.2949... Val Loss: 1.7623\n","Epoch: 17/30... Step: 12030... Loss: 1.2922... Val Loss: 1.7716\n","Epoch: 17/30... Step: 12040... Loss: 1.3127... Val Loss: 1.7639\n","Epoch: 17/30... Step: 12050... Loss: 1.2907... Val Loss: 1.7616\n","Epoch: 17/30... Step: 12060... Loss: 1.3131... Val Loss: 1.7880\n","Epoch: 17/30... Step: 12070... Loss: 1.3105... Val Loss: 1.7879\n","Epoch: 17/30... Step: 12080... Loss: 1.2834... Val Loss: 1.7800\n","Epoch: 17/30... Step: 12090... Loss: 1.3197... Val Loss: 1.7778\n","Epoch: 17/30... Step: 12100... Loss: 1.2788... Val Loss: 1.7713\n","Epoch: 17/30... Step: 12110... Loss: 1.2925... Val Loss: 1.7650\n","Epoch: 17/30... Step: 12120... Loss: 1.2669... Val Loss: 1.7604\n","Epoch: 17/30... Step: 12130... Loss: 1.3040... Val Loss: 1.7727\n","Epoch: 17/30... Step: 12140... Loss: 1.3028... Val Loss: 1.7658\n","Epoch: 17/30... Step: 12150... Loss: 1.2645... Val Loss: 1.7936\n","Epoch: 17/30... Step: 12160... Loss: 1.2968... Val Loss: 1.7967\n","Epoch: 17/30... Step: 12170... Loss: 1.3008... Val Loss: 1.7985\n","Epoch: 17/30... Step: 12180... Loss: 1.2994... Val Loss: 1.7837\n","Epoch: 17/30... Step: 12190... Loss: 1.2631... Val Loss: 1.7809\n","Epoch: 17/30... Step: 12200... Loss: 1.2884... Val Loss: 1.7670\n","Epoch: 17/30... Step: 12210... Loss: 1.2859... Val Loss: 1.7756\n","Epoch: 17/30... Step: 12220... Loss: 1.2982... Val Loss: 1.7700\n","Epoch: 17/30... Step: 12230... Loss: 1.2746... Val Loss: 1.8030\n","Epoch: 17/30... Step: 12240... Loss: 1.3065... Val Loss: 1.7887\n","Epoch: 17/30... Step: 12250... Loss: 1.3113... Val Loss: 1.7783\n","Epoch: 17/30... Step: 12260... Loss: 1.3099... Val Loss: 1.7893\n","Epoch: 17/30... Step: 12270... Loss: 1.2840... Val Loss: 1.7636\n","Epoch: 17/30... Step: 12280... Loss: 1.2893... Val Loss: 1.7645\n","Epoch: 17/30... Step: 12290... Loss: 1.3026... Val Loss: 1.7814\n","Epoch: 17/30... Step: 12300... Loss: 1.2654... Val Loss: 1.8046\n","Epoch: 17/30... Step: 12310... Loss: 1.2938... Val Loss: 1.7783\n","Epoch: 17/30... Step: 12320... Loss: 1.2823... Val Loss: 1.7960\n","Epoch: 17/30... Step: 12330... Loss: 1.2987... Val Loss: 1.7881\n","Epoch: 17/30... Step: 12340... Loss: 1.2952... Val Loss: 1.7849\n","Epoch: 17/30... Step: 12350... Loss: 1.3111... Val Loss: 1.7974\n","Epoch: 17/30... Step: 12360... Loss: 1.3017... Val Loss: 1.7701\n","Epoch: 17/30... Step: 12370... Loss: 1.2952... Val Loss: 1.7840\n","Epoch: 17/30... Step: 12380... Loss: 1.2978... Val Loss: 1.7840\n","Epoch: 17/30... Step: 12390... Loss: 1.2932... Val Loss: 1.7938\n","Epoch: 17/30... Step: 12400... Loss: 1.2839... Val Loss: 1.7868\n","Epoch: 17/30... Step: 12410... Loss: 1.2979... Val Loss: 1.7769\n","Epoch: 17/30... Step: 12420... Loss: 1.3025... Val Loss: 1.7860\n","Epoch: 17/30... Step: 12430... Loss: 1.3054... Val Loss: 1.7729\n","Epoch: 17/30... Step: 12440... Loss: 1.2507... Val Loss: 1.7760\n","Epoch: 17/30... Step: 12450... Loss: 1.3175... Val Loss: 1.7747\n","Epoch: 17/30... Step: 12460... Loss: 1.2763... Val Loss: 1.7661\n","Epoch: 17/30... Step: 12470... Loss: 1.2995... Val Loss: 1.7973\n","Epoch: 17/30... Step: 12480... Loss: 1.3102... Val Loss: 1.7727\n","Epoch: 17/30... Step: 12490... Loss: 1.2658... Val Loss: 1.7712\n","Epoch: 17/30... Step: 12500... Loss: 1.2873... Val Loss: 1.7885\n","Epoch: 17/30... Step: 12510... Loss: 1.2836... Val Loss: 1.7781\n","Epoch: 17/30... Step: 12520... Loss: 1.2497... Val Loss: 1.7838\n","Epoch: 17/30... Step: 12530... Loss: 1.2656... Val Loss: 1.7872\n","Epoch: 17/30... Step: 12540... Loss: 1.3200... Val Loss: 1.7879\n","Epoch: 17/30... Step: 12550... Loss: 1.2718... Val Loss: 1.8027\n","Epoch: 17/30... Step: 12560... Loss: 1.2984... Val Loss: 1.7939\n","Epoch: 17/30... Step: 12570... Loss: 1.2814... Val Loss: 1.8079\n","Epoch: 17/30... Step: 12580... Loss: 1.2912... Val Loss: 1.8006\n","Epoch: 17/30... Step: 12590... Loss: 1.2870... Val Loss: 1.7942\n","Epoch: 17/30... Step: 12600... Loss: 1.2929... Val Loss: 1.8014\n","Epoch: 17/30... Step: 12610... Loss: 1.2751... Val Loss: 1.8031\n","Epoch: 17/30... Step: 12620... Loss: 1.2795... Val Loss: 1.7896\n","Epoch: 17/30... Step: 12630... Loss: 1.2653... Val Loss: 1.7873\n","Epoch: 17/30... Step: 12640... Loss: 1.2829... Val Loss: 1.7838\n","Epoch: 17/30... Step: 12650... Loss: 1.2665... Val Loss: 1.7938\n","Epoch: 17/30... Step: 12660... Loss: 1.2898... Val Loss: 1.7941\n","Epoch: 17/30... Step: 12670... Loss: 1.2555... Val Loss: 1.7963\n","Epoch: 17/30... Step: 12680... Loss: 1.2981... Val Loss: 1.7971\n","Epoch: 17/30... Step: 12690... Loss: 1.2782... Val Loss: 1.7995\n","Epoch: 17/30... Step: 12700... Loss: 1.2905... Val Loss: 1.8019\n","Epoch: 17/30... Step: 12710... Loss: 1.2736... Val Loss: 1.7882\n","Epoch: 18/30... Step: 12720... Loss: 1.2958... Val Loss: 1.7808\n","Epoch: 18/30... Step: 12730... Loss: 1.3048... Val Loss: 1.7387\n","Epoch: 18/30... Step: 12740... Loss: 1.2716... Val Loss: 1.6834\n","Epoch: 18/30... Step: 12750... Loss: 1.3079... Val Loss: 1.6951\n","Epoch: 18/30... Step: 12760... Loss: 1.3134... Val Loss: 1.6955\n","Epoch: 18/30... Step: 12770... Loss: 1.2881... Val Loss: 1.6900\n","Epoch: 18/30... Step: 12780... Loss: 1.2686... Val Loss: 1.6921\n","Epoch: 18/30... Step: 12790... Loss: 1.2893... Val Loss: 1.6680\n","Epoch: 18/30... Step: 12800... Loss: 1.3042... Val Loss: 1.7024\n","Epoch: 18/30... Step: 12810... Loss: 1.2737... Val Loss: 1.7020\n","Epoch: 18/30... Step: 12820... Loss: 1.3186... Val Loss: 1.7214\n","Epoch: 18/30... Step: 12830... Loss: 1.2774... Val Loss: 1.7106\n","Epoch: 18/30... Step: 12840... Loss: 1.2953... Val Loss: 1.7007\n","Epoch: 18/30... Step: 12850... Loss: 1.3025... Val Loss: 1.6972\n","Epoch: 18/30... Step: 12860... Loss: 1.2790... Val Loss: 1.7212\n","Epoch: 18/30... Step: 12870... Loss: 1.2880... Val Loss: 1.7649\n","Epoch: 18/30... Step: 12880... Loss: 1.3009... Val Loss: 1.7218\n","Epoch: 18/30... Step: 12890... Loss: 1.2938... Val Loss: 1.7099\n","Epoch: 18/30... Step: 12900... Loss: 1.3165... Val Loss: 1.7631\n","Epoch: 18/30... Step: 12910... Loss: 1.2638... Val Loss: 1.7541\n","Epoch: 18/30... Step: 12920... Loss: 1.2570... Val Loss: 1.7675\n","Epoch: 18/30... Step: 12930... Loss: 1.3254... Val Loss: 1.7462\n","Epoch: 18/30... Step: 12940... Loss: 1.2651... Val Loss: 1.7697\n","Epoch: 18/30... Step: 12950... Loss: 1.2854... Val Loss: 1.7312\n","Epoch: 18/30... Step: 12960... Loss: 1.3200... Val Loss: 1.7023\n","Epoch: 18/30... Step: 12970... Loss: 1.2966... Val Loss: 1.7124\n","Epoch: 18/30... Step: 12980... Loss: 1.3026... Val Loss: 1.7977\n","Epoch: 18/30... Step: 12990... Loss: 1.3128... Val Loss: 1.7633\n","Epoch: 18/30... Step: 13000... Loss: 1.2762... Val Loss: 1.7083\n","Epoch: 18/30... Step: 13010... Loss: 1.2525... Val Loss: 1.7552\n","Epoch: 18/30... Step: 13020... Loss: 1.2931... Val Loss: 1.7110\n","Epoch: 18/30... Step: 13030... Loss: 1.2770... Val Loss: 1.7543\n","Epoch: 18/30... Step: 13040... Loss: 1.2979... Val Loss: 1.7465\n","Epoch: 18/30... Step: 13050... Loss: 1.2972... Val Loss: 1.7844\n","Epoch: 18/30... Step: 13060... Loss: 1.3007... Val Loss: 1.7515\n","Epoch: 18/30... Step: 13070... Loss: 1.2972... Val Loss: 1.7918\n","Epoch: 18/30... Step: 13080... Loss: 1.2770... Val Loss: 1.7898\n","Epoch: 18/30... Step: 13090... Loss: 1.2802... Val Loss: 1.7650\n","Epoch: 18/30... Step: 13100... Loss: 1.2849... Val Loss: 1.8000\n","Epoch: 18/30... Step: 13110... Loss: 1.2920... Val Loss: 1.7051\n","Epoch: 18/30... Step: 13120... Loss: 1.2848... Val Loss: 1.7473\n","Epoch: 18/30... Step: 13130... Loss: 1.3141... Val Loss: 1.7178\n","Epoch: 18/30... Step: 13140... Loss: 1.2896... Val Loss: 1.7848\n","Epoch: 18/30... Step: 13150... Loss: 1.2731... Val Loss: 1.7623\n","Epoch: 18/30... Step: 13160... Loss: 1.2578... Val Loss: 1.7468\n","Epoch: 18/30... Step: 13170... Loss: 1.2706... Val Loss: 1.7660\n","Epoch: 18/30... Step: 13180... Loss: 1.2977... Val Loss: 1.7493\n","Epoch: 18/30... Step: 13190... Loss: 1.2850... Val Loss: 1.7798\n","Epoch: 18/30... Step: 13200... Loss: 1.2948... Val Loss: 1.7448\n","Epoch: 18/30... Step: 13210... Loss: 1.3015... Val Loss: 1.7687\n","Epoch: 18/30... Step: 13220... Loss: 1.2825... Val Loss: 1.7741\n","Epoch: 18/30... Step: 13230... Loss: 1.3253... Val Loss: 1.7946\n","Epoch: 18/30... Step: 13240... Loss: 1.2744... Val Loss: 1.7490\n","Epoch: 18/30... Step: 13250... Loss: 1.3052... Val Loss: 1.7607\n","Epoch: 18/30... Step: 13260... Loss: 1.2761... Val Loss: 1.7715\n","Epoch: 18/30... Step: 13270... Loss: 1.2705... Val Loss: 1.8149\n","Epoch: 18/30... Step: 13280... Loss: 1.2919... Val Loss: 1.7733\n","Epoch: 18/30... Step: 13290... Loss: 1.2785... Val Loss: 1.7749\n","Epoch: 18/30... Step: 13300... Loss: 1.2767... Val Loss: 1.7733\n","Epoch: 18/30... Step: 13310... Loss: 1.2961... Val Loss: 1.7848\n","Epoch: 18/30... Step: 13320... Loss: 1.2887... Val Loss: 1.7994\n","Epoch: 18/30... Step: 13330... Loss: 1.2923... Val Loss: 1.8017\n","Epoch: 18/30... Step: 13340... Loss: 1.2976... Val Loss: 1.7771\n","Epoch: 18/30... Step: 13350... Loss: 1.2867... Val Loss: 1.7684\n","Epoch: 18/30... Step: 13360... Loss: 1.2769... Val Loss: 1.7202\n","Epoch: 18/30... Step: 13370... Loss: 1.2978... Val Loss: 1.7479\n","Epoch: 18/30... Step: 13380... Loss: 1.2979... Val Loss: 1.7823\n","Epoch: 18/30... Step: 13390... Loss: 1.2716... Val Loss: 1.7227\n","Epoch: 18/30... Step: 13400... Loss: 1.2562... Val Loss: 1.7800\n","Epoch: 18/30... Step: 13410... Loss: 1.2919... Val Loss: 1.7493\n","Epoch: 18/30... Step: 13420... Loss: 1.2658... Val Loss: 1.7983\n","Epoch: 18/30... Step: 13430... Loss: 1.2702... Val Loss: 1.8000\n","Epoch: 18/30... Step: 13440... Loss: 1.2929... Val Loss: 1.7741\n","Epoch: 18/30... Step: 13450... Loss: 1.2713... Val Loss: 1.7403\n","Epoch: 18/30... Step: 13460... Loss: 1.2796... Val Loss: 1.7952\n","Epoch: 19/30... Step: 13470... Loss: 1.2936... Val Loss: 1.7994\n","Epoch: 19/30... Step: 13480... Loss: 1.2715... Val Loss: 1.7274\n","Epoch: 19/30... Step: 13490... Loss: 1.2935... Val Loss: 1.7360\n","Epoch: 19/30... Step: 13500... Loss: 1.2620... Val Loss: 1.7277\n","Epoch: 19/30... Step: 13510... Loss: 1.3045... Val Loss: 1.7204\n","Epoch: 19/30... Step: 13520... Loss: 1.2870... Val Loss: 1.7362\n","Epoch: 19/30... Step: 13530... Loss: 1.2727... Val Loss: 1.7345\n","Epoch: 19/30... Step: 13540... Loss: 1.3004... Val Loss: 1.6939\n","Epoch: 19/30... Step: 13550... Loss: 1.2953... Val Loss: 1.7324\n","Epoch: 19/30... Step: 13560... Loss: 1.2764... Val Loss: 1.7456\n","Epoch: 19/30... Step: 13570... Loss: 1.2834... Val Loss: 1.7181\n","Epoch: 19/30... Step: 13580... Loss: 1.2863... Val Loss: 1.7535\n","Epoch: 19/30... Step: 13590... Loss: 1.2771... Val Loss: 1.7398\n","Epoch: 19/30... Step: 13600... Loss: 1.2632... Val Loss: 1.7473\n","Epoch: 19/30... Step: 13610... Loss: 1.2678... Val Loss: 1.7654\n","Epoch: 19/30... Step: 13620... Loss: 1.2956... Val Loss: 1.7492\n","Epoch: 19/30... Step: 13630... Loss: 1.2865... Val Loss: 1.7259\n","Epoch: 19/30... Step: 13640... Loss: 1.2825... Val Loss: 1.7316\n","Epoch: 19/30... Step: 13650... Loss: 1.2978... Val Loss: 1.7174\n","Epoch: 19/30... Step: 13660... Loss: 1.2760... Val Loss: 1.7229\n","Epoch: 19/30... Step: 13670... Loss: 1.2941... Val Loss: 1.7570\n","Epoch: 19/30... Step: 13680... Loss: 1.2769... Val Loss: 1.7663\n","Epoch: 19/30... Step: 13690... Loss: 1.2790... Val Loss: 1.7208\n","Epoch: 19/30... Step: 13700... Loss: 1.3121... Val Loss: 1.7519\n","Epoch: 19/30... Step: 13710... Loss: 1.2911... Val Loss: 1.7207\n","Epoch: 19/30... Step: 13720... Loss: 1.3017... Val Loss: 1.7539\n","Epoch: 19/30... Step: 13730... Loss: 1.2929... Val Loss: 1.7632\n","Epoch: 19/30... Step: 13740... Loss: 1.2821... Val Loss: 1.7481\n","Epoch: 19/30... Step: 13750... Loss: 1.2913... Val Loss: 1.7308\n","Epoch: 19/30... Step: 13760... Loss: 1.2999... Val Loss: 1.7626\n","Epoch: 19/30... Step: 13770... Loss: 1.2943... Val Loss: 1.7357\n","Epoch: 19/30... Step: 13780... Loss: 1.2924... Val Loss: 1.7654\n","Epoch: 19/30... Step: 13790... Loss: 1.2847... Val Loss: 1.7750\n","Epoch: 19/30... Step: 13800... Loss: 1.2655... Val Loss: 1.7894\n","Epoch: 19/30... Step: 13810... Loss: 1.2598... Val Loss: 1.7419\n","Epoch: 19/30... Step: 13820... Loss: 1.3137... Val Loss: 1.7594\n","Epoch: 19/30... Step: 13830... Loss: 1.2812... Val Loss: 1.7979\n","Epoch: 19/30... Step: 13840... Loss: 1.2841... Val Loss: 1.7710\n","Epoch: 19/30... Step: 13850... Loss: 1.2650... Val Loss: 1.7801\n","Epoch: 19/30... Step: 13860... Loss: 1.2626... Val Loss: 1.7788\n","Epoch: 19/30... Step: 13870... Loss: 1.2562... Val Loss: 1.7752\n","Epoch: 19/30... Step: 13880... Loss: 1.2749... Val Loss: 1.7472\n","Epoch: 19/30... Step: 13890... Loss: 1.2861... Val Loss: 1.7777\n","Epoch: 19/30... Step: 13900... Loss: 1.2819... Val Loss: 1.7842\n","Epoch: 19/30... Step: 13910... Loss: 1.2679... Val Loss: 1.7784\n","Epoch: 19/30... Step: 13920... Loss: 1.2818... Val Loss: 1.7405\n","Epoch: 19/30... Step: 13930... Loss: 1.2845... Val Loss: 1.7520\n","Epoch: 19/30... Step: 13940... Loss: 1.2838... Val Loss: 1.7645\n","Epoch: 19/30... Step: 13950... Loss: 1.2637... Val Loss: 1.7466\n","Epoch: 19/30... Step: 13960... Loss: 1.3082... Val Loss: 1.7542\n","Epoch: 19/30... Step: 13970... Loss: 1.2484... Val Loss: 1.7389\n","Epoch: 19/30... Step: 13980... Loss: 1.2778... Val Loss: 1.7461\n","Epoch: 19/30... Step: 13990... Loss: 1.2917... Val Loss: 1.7667\n","Epoch: 19/30... Step: 14000... Loss: 1.2820... Val Loss: 1.7698\n","Epoch: 19/30... Step: 14010... Loss: 1.2935... Val Loss: 1.7744\n","Epoch: 19/30... Step: 14020... Loss: 1.2979... Val Loss: 1.7776\n","Epoch: 19/30... Step: 14030... Loss: 1.2621... Val Loss: 1.7303\n","Epoch: 19/30... Step: 14040... Loss: 1.2795... Val Loss: 1.7423\n","Epoch: 19/30... Step: 14050... Loss: 1.2545... Val Loss: 1.7500\n","Epoch: 19/30... Step: 14060... Loss: 1.2816... Val Loss: 1.7472\n","Epoch: 19/30... Step: 14070... Loss: 1.2752... Val Loss: 1.8014\n","Epoch: 19/30... Step: 14080... Loss: 1.2854... Val Loss: 1.7886\n","Epoch: 19/30... Step: 14090... Loss: 1.2798... Val Loss: 1.7865\n","Epoch: 19/30... Step: 14100... Loss: 1.2929... Val Loss: 1.7633\n","Epoch: 19/30... Step: 14110... Loss: 1.2998... Val Loss: 1.7675\n","Epoch: 19/30... Step: 14120... Loss: 1.2839... Val Loss: 1.7448\n","Epoch: 19/30... Step: 14130... Loss: 1.2937... Val Loss: 1.7505\n","Epoch: 19/30... Step: 14140... Loss: 1.2512... Val Loss: 1.7483\n","Epoch: 19/30... Step: 14150... Loss: 1.2614... Val Loss: 1.7778\n","Epoch: 19/30... Step: 14160... Loss: 1.2779... Val Loss: 1.7377\n","Epoch: 19/30... Step: 14170... Loss: 1.2857... Val Loss: 1.7962\n","Epoch: 19/30... Step: 14180... Loss: 1.2753... Val Loss: 1.7553\n","Epoch: 19/30... Step: 14190... Loss: 1.2407... Val Loss: 1.7515\n","Epoch: 19/30... Step: 14200... Loss: 1.2636... Val Loss: 1.7371\n","Epoch: 19/30... Step: 14210... Loss: 1.2781... Val Loss: 1.7431\n","Epoch: 20/30... Step: 14220... Loss: 1.2705... Val Loss: 1.7222\n","Epoch: 20/30... Step: 14230... Loss: 1.2857... Val Loss: 1.7146\n","Epoch: 20/30... Step: 14240... Loss: 1.2919... Val Loss: 1.7124\n","Epoch: 20/30... Step: 14250... Loss: 1.2959... Val Loss: 1.7140\n","Epoch: 20/30... Step: 14260... Loss: 1.2922... Val Loss: 1.6966\n","Epoch: 20/30... Step: 14270... Loss: 1.3095... Val Loss: 1.7082\n","Epoch: 20/30... Step: 14280... Loss: 1.2639... Val Loss: 1.6911\n","Epoch: 20/30... Step: 14290... Loss: 1.3243... Val Loss: 1.6931\n","Epoch: 20/30... Step: 14300... Loss: 1.2707... Val Loss: 1.6975\n","Epoch: 20/30... Step: 14310... Loss: 1.2715... Val Loss: 1.7040\n","Epoch: 20/30... Step: 14320... Loss: 1.2756... Val Loss: 1.6872\n","Epoch: 20/30... Step: 14330... Loss: 1.2670... Val Loss: 1.7262\n","Epoch: 20/30... Step: 14340... Loss: 1.2896... Val Loss: 1.7090\n","Epoch: 20/30... Step: 14350... Loss: 1.2768... Val Loss: 1.7254\n","Epoch: 20/30... Step: 14360... Loss: 1.2681... Val Loss: 1.7353\n","Epoch: 20/30... Step: 14370... Loss: 1.2743... Val Loss: 1.7296\n","Epoch: 20/30... Step: 14380... Loss: 1.2804... Val Loss: 1.7164\n","Epoch: 20/30... Step: 14390... Loss: 1.3047... Val Loss: 1.7290\n","Epoch: 20/30... Step: 14400... Loss: 1.3196... Val Loss: 1.7279\n","Epoch: 20/30... Step: 14410... Loss: 1.2852... Val Loss: 1.7296\n","Epoch: 20/30... Step: 14420... Loss: 1.2798... Val Loss: 1.7403\n","Epoch: 20/30... Step: 14430... Loss: 1.2789... Val Loss: 1.7309\n","Epoch: 20/30... Step: 14440... Loss: 1.2841... Val Loss: 1.7182\n","Epoch: 20/30... Step: 14450... Loss: 1.2941... Val Loss: 1.7547\n","Epoch: 20/30... Step: 14460... Loss: 1.2659... Val Loss: 1.7250\n","Epoch: 20/30... Step: 14470... Loss: 1.2712... Val Loss: 1.7319\n","Epoch: 20/30... Step: 14480... Loss: 1.2713... Val Loss: 1.7635\n","Epoch: 20/30... Step: 14490... Loss: 1.2703... Val Loss: 1.7530\n","Epoch: 20/30... Step: 14500... Loss: 1.2949... Val Loss: 1.7388\n","Epoch: 20/30... Step: 14510... Loss: 1.2863... Val Loss: 1.7472\n","Epoch: 20/30... Step: 14520... Loss: 1.2786... Val Loss: 1.7347\n","Epoch: 20/30... Step: 14530... Loss: 1.2981... Val Loss: 1.7576\n","Epoch: 20/30... Step: 14540... Loss: 1.2617... Val Loss: 1.7720\n","Epoch: 20/30... Step: 14550... Loss: 1.3211... Val Loss: 1.7705\n","Epoch: 20/30... Step: 14560... Loss: 1.2757... Val Loss: 1.7834\n","Epoch: 20/30... Step: 14570... Loss: 1.2921... Val Loss: 1.7660\n","Epoch: 20/30... Step: 14580... Loss: 1.2787... Val Loss: 1.7918\n","Epoch: 20/30... Step: 14590... Loss: 1.2605... Val Loss: 1.7734\n","Epoch: 20/30... Step: 14600... Loss: 1.2811... Val Loss: 1.7620\n","Epoch: 20/30... Step: 14610... Loss: 1.2587... Val Loss: 1.7492\n","Epoch: 20/30... Step: 14620... Loss: 1.2440... Val Loss: 1.7752\n","Epoch: 20/30... Step: 14630... Loss: 1.2408... Val Loss: 1.7709\n","Epoch: 20/30... Step: 14640... Loss: 1.2828... Val Loss: 1.7833\n","Epoch: 20/30... Step: 14650... Loss: 1.2689... Val Loss: 1.8039\n","Epoch: 20/30... Step: 14660... Loss: 1.2753... Val Loss: 1.7991\n","Epoch: 20/30... Step: 14670... Loss: 1.2836... Val Loss: 1.7833\n","Epoch: 20/30... Step: 14680... Loss: 1.2836... Val Loss: 1.7963\n","Epoch: 20/30... Step: 14690... Loss: 1.2876... Val Loss: 1.7900\n","Epoch: 20/30... Step: 14700... Loss: 1.2720... Val Loss: 1.7872\n","Epoch: 20/30... Step: 14710... Loss: 1.2665... Val Loss: 1.7770\n","Epoch: 20/30... Step: 14720... Loss: 1.2729... Val Loss: 1.7786\n","Epoch: 20/30... Step: 14730... Loss: 1.2700... Val Loss: 1.8032\n","Epoch: 20/30... Step: 14740... Loss: 1.3020... Val Loss: 1.8050\n","Epoch: 20/30... Step: 14750... Loss: 1.2909... Val Loss: 1.8001\n","Epoch: 20/30... Step: 14760... Loss: 1.2875... Val Loss: 1.7858\n","Epoch: 20/30... Step: 14770... Loss: 1.2630... Val Loss: 1.7929\n","Epoch: 20/30... Step: 14780... Loss: 1.2883... Val Loss: 1.8060\n","Epoch: 20/30... Step: 14790... Loss: 1.2871... Val Loss: 1.7864\n","Epoch: 20/30... Step: 14800... Loss: 1.2609... Val Loss: 1.7887\n","Epoch: 20/30... Step: 14810... Loss: 1.2548... Val Loss: 1.7927\n","Epoch: 20/30... Step: 14820... Loss: 1.2741... Val Loss: 1.8060\n","Epoch: 20/30... Step: 14830... Loss: 1.2697... Val Loss: 1.7999\n","Epoch: 20/30... Step: 14840... Loss: 1.2549... Val Loss: 1.8120\n","Epoch: 20/30... Step: 14850... Loss: 1.2801... Val Loss: 1.7957\n","Epoch: 20/30... Step: 14860... Loss: 1.2651... Val Loss: 1.7792\n","Epoch: 20/30... Step: 14870... Loss: 1.2782... Val Loss: 1.7795\n","Epoch: 20/30... Step: 14880... Loss: 1.2784... Val Loss: 1.7625\n","Epoch: 20/30... Step: 14890... Loss: 1.2641... Val Loss: 1.7644\n","Epoch: 20/30... Step: 14900... Loss: 1.2997... Val Loss: 1.7629\n","Epoch: 20/30... Step: 14910... Loss: 1.2857... Val Loss: 1.7631\n","Epoch: 20/30... Step: 14920... Loss: 1.2875... Val Loss: 1.7629\n","Epoch: 20/30... Step: 14930... Loss: 1.2551... Val Loss: 1.7714\n","Epoch: 20/30... Step: 14940... Loss: 1.3000... Val Loss: 1.7565\n","Epoch: 20/30... Step: 14950... Loss: 1.2853... Val Loss: 1.7430\n","Epoch: 20/30... Step: 14960... Loss: 1.3223... Val Loss: 1.7657\n","Epoch: 21/30... Step: 14970... Loss: 1.2661... Val Loss: 1.7703\n","Epoch: 21/30... Step: 14980... Loss: 1.2663... Val Loss: 1.7720\n","Epoch: 21/30... Step: 14990... Loss: 1.2654... Val Loss: 1.7712\n","Epoch: 21/30... Step: 15000... Loss: 1.3006... Val Loss: 1.7604\n","Epoch: 21/30... Step: 15010... Loss: 1.2743... Val Loss: 1.7799\n","Epoch: 21/30... Step: 15020... Loss: 1.2886... Val Loss: 1.7947\n","Epoch: 21/30... Step: 15030... Loss: 1.3013... Val Loss: 1.8031\n","Epoch: 21/30... Step: 15040... Loss: 1.3002... Val Loss: 1.7837\n","Epoch: 21/30... Step: 15050... Loss: 1.2735... Val Loss: 1.7908\n","Epoch: 21/30... Step: 15060... Loss: 1.2759... Val Loss: 1.8088\n","Epoch: 21/30... Step: 15070... Loss: 1.2869... Val Loss: 1.7875\n","Epoch: 21/30... Step: 15080... Loss: 1.2890... Val Loss: 1.8059\n","Epoch: 21/30... Step: 15090... Loss: 1.2777... Val Loss: 1.7786\n","Epoch: 21/30... Step: 15100... Loss: 1.2789... Val Loss: 1.7978\n","Epoch: 21/30... Step: 15110... Loss: 1.3048... Val Loss: 1.8014\n","Epoch: 21/30... Step: 15120... Loss: 1.2735... Val Loss: 1.8044\n","Epoch: 21/30... Step: 15130... Loss: 1.2893... Val Loss: 1.7629\n","Epoch: 21/30... Step: 15140... Loss: 1.3021... Val Loss: 1.8019\n","Epoch: 21/30... Step: 15150... Loss: 1.2494... Val Loss: 1.7916\n","Epoch: 21/30... Step: 15160... Loss: 1.2772... Val Loss: 1.7888\n","Epoch: 21/30... Step: 15170... Loss: 1.2794... Val Loss: 1.7869\n","Epoch: 21/30... Step: 15180... Loss: 1.2467... Val Loss: 1.8040\n","Epoch: 21/30... Step: 15190... Loss: 1.2842... Val Loss: 1.7655\n","Epoch: 21/30... Step: 15200... Loss: 1.2735... Val Loss: 1.8078\n","Epoch: 21/30... Step: 15210... Loss: 1.2922... Val Loss: 1.8090\n","Epoch: 21/30... Step: 15220... Loss: 1.2694... Val Loss: 1.8035\n","Epoch: 21/30... Step: 15230... Loss: 1.2932... Val Loss: 1.7975\n","Epoch: 21/30... Step: 15240... Loss: 1.2839... Val Loss: 1.8050\n","Epoch: 21/30... Step: 15250... Loss: 1.2977... Val Loss: 1.7827\n","Epoch: 21/30... Step: 15260... Loss: 1.2963... Val Loss: 1.7936\n","Epoch: 21/30... Step: 15270... Loss: 1.2859... Val Loss: 1.7835\n","Epoch: 21/30... Step: 15280... Loss: 1.2609... Val Loss: 1.7984\n","Epoch: 21/30... Step: 15290... Loss: 1.2742... Val Loss: 1.8092\n","Epoch: 21/30... Step: 15300... Loss: 1.2601... Val Loss: 1.7936\n","Epoch: 21/30... Step: 15310... Loss: 1.2594... Val Loss: 1.7961\n","Epoch: 21/30... Step: 15320... Loss: 1.2988... Val Loss: 1.7766\n","Epoch: 21/30... Step: 15330... Loss: 1.2586... Val Loss: 1.7760\n","Epoch: 21/30... Step: 15340... Loss: 1.2638... Val Loss: 1.7920\n","Epoch: 21/30... Step: 15350... Loss: 1.2698... Val Loss: 1.7612\n","Epoch: 21/30... Step: 15360... Loss: 1.2402... Val Loss: 1.7583\n","Epoch: 21/30... Step: 15370... Loss: 1.2889... Val Loss: 1.7929\n","Epoch: 21/30... Step: 15380... Loss: 1.2586... Val Loss: 1.7808\n","Epoch: 21/30... Step: 15390... Loss: 1.2800... Val Loss: 1.7615\n","Epoch: 21/30... Step: 15400... Loss: 1.2608... Val Loss: 1.7664\n","Epoch: 21/30... Step: 15410... Loss: 1.2715... Val Loss: 1.7892\n","Epoch: 21/30... Step: 15420... Loss: 1.2691... Val Loss: 1.7871\n","Epoch: 21/30... Step: 15430... Loss: 1.2724... Val Loss: 1.7834\n","Epoch: 21/30... Step: 15440... Loss: 1.2516... Val Loss: 1.8014\n","Epoch: 21/30... Step: 15450... Loss: 1.2432... Val Loss: 1.7749\n","Epoch: 21/30... Step: 15460... Loss: 1.2587... Val Loss: 1.7828\n","Epoch: 21/30... Step: 15470... Loss: 1.2495... Val Loss: 1.7863\n","Epoch: 21/30... Step: 15480... Loss: 1.2565... Val Loss: 1.7763\n","Epoch: 21/30... Step: 15490... Loss: 1.2683... Val Loss: 1.7848\n","Epoch: 21/30... Step: 15500... Loss: 1.2914... Val Loss: 1.7945\n","Epoch: 21/30... Step: 15510... Loss: 1.2587... Val Loss: 1.7944\n","Epoch: 21/30... Step: 15520... Loss: 1.2648... Val Loss: 1.7989\n","Epoch: 21/30... Step: 15530... Loss: 1.2630... Val Loss: 1.7955\n","Epoch: 21/30... Step: 15540... Loss: 1.2890... Val Loss: 1.8139\n","Epoch: 21/30... Step: 15550... Loss: 1.2682... Val Loss: 1.8111\n","Epoch: 21/30... Step: 15560... Loss: 1.2746... Val Loss: 1.8134\n","Epoch: 21/30... Step: 15570... Loss: 1.2651... Val Loss: 1.7825\n","Epoch: 21/30... Step: 15580... Loss: 1.2887... Val Loss: 1.8110\n","Epoch: 21/30... Step: 15590... Loss: 1.2515... Val Loss: 1.8087\n","Epoch: 21/30... Step: 15600... Loss: 1.2565... Val Loss: 1.7986\n","Epoch: 21/30... Step: 15610... Loss: 1.2672... Val Loss: 1.8002\n","Epoch: 21/30... Step: 15620... Loss: 1.2656... Val Loss: 1.7881\n","Epoch: 21/30... Step: 15630... Loss: 1.2753... Val Loss: 1.7654\n","Epoch: 21/30... Step: 15640... Loss: 1.2511... Val Loss: 1.8012\n","Epoch: 21/30... Step: 15650... Loss: 1.2593... Val Loss: 1.7919\n","Epoch: 21/30... Step: 15660... Loss: 1.2837... Val Loss: 1.8116\n","Epoch: 21/30... Step: 15670... Loss: 1.2682... Val Loss: 1.8113\n","Epoch: 21/30... Step: 15680... Loss: 1.2618... Val Loss: 1.8016\n","Epoch: 21/30... Step: 15690... Loss: 1.2604... Val Loss: 1.7835\n","Epoch: 21/30... Step: 15700... Loss: 1.2761... Val Loss: 1.8011\n","Epoch: 22/30... Step: 15710... Loss: 1.2689... Val Loss: 1.8278\n","Epoch: 22/30... Step: 15720... Loss: 1.2465... Val Loss: 1.7216\n","Epoch: 22/30... Step: 15730... Loss: 1.2830... Val Loss: 1.6928\n","Epoch: 22/30... Step: 15740... Loss: 1.2769... Val Loss: 1.7811\n","Epoch: 22/30... Step: 15750... Loss: 1.2755... Val Loss: 1.7751\n","Epoch: 22/30... Step: 15760... Loss: 1.2754... Val Loss: 1.7804\n","Epoch: 22/30... Step: 15770... Loss: 1.2682... Val Loss: 1.7867\n","Epoch: 22/30... Step: 15780... Loss: 1.2854... Val Loss: 1.7884\n","Epoch: 22/30... Step: 15790... Loss: 1.2674... Val Loss: 1.7760\n","Epoch: 22/30... Step: 15800... Loss: 1.2899... Val Loss: 1.7953\n","Epoch: 22/30... Step: 15810... Loss: 1.2785... Val Loss: 1.7773\n","Epoch: 22/30... Step: 15820... Loss: 1.2633... Val Loss: 1.7844\n","Epoch: 22/30... Step: 15830... Loss: 1.2996... Val Loss: 1.7957\n","Epoch: 22/30... Step: 15840... Loss: 1.2569... Val Loss: 1.7610\n","Epoch: 22/30... Step: 15850... Loss: 1.2757... Val Loss: 1.7804\n","Epoch: 22/30... Step: 15860... Loss: 1.2413... Val Loss: 1.7994\n","Epoch: 22/30... Step: 15870... Loss: 1.2785... Val Loss: 1.7943\n","Epoch: 22/30... Step: 15880... Loss: 1.2772... Val Loss: 1.7749\n","Epoch: 22/30... Step: 15890... Loss: 1.2419... Val Loss: 1.7840\n","Epoch: 22/30... Step: 15900... Loss: 1.2819... Val Loss: 1.7919\n","Epoch: 22/30... Step: 15910... Loss: 1.2755... Val Loss: 1.7871\n","Epoch: 22/30... Step: 15920... Loss: 1.2804... Val Loss: 1.8007\n","Epoch: 22/30... Step: 15930... Loss: 1.2405... Val Loss: 1.8033\n","Epoch: 22/30... Step: 15940... Loss: 1.2604... Val Loss: 1.7780\n","Epoch: 22/30... Step: 15950... Loss: 1.2626... Val Loss: 1.7806\n","Epoch: 22/30... Step: 15960... Loss: 1.2747... Val Loss: 1.7864\n","Epoch: 22/30... Step: 15970... Loss: 1.2583... Val Loss: 1.8080\n","Epoch: 22/30... Step: 15980... Loss: 1.2807... Val Loss: 1.7814\n","Epoch: 22/30... Step: 15990... Loss: 1.2891... Val Loss: 1.7882\n","Epoch: 22/30... Step: 16000... Loss: 1.2956... Val Loss: 1.8016\n","Epoch: 22/30... Step: 16010... Loss: 1.2752... Val Loss: 1.7773\n","Epoch: 22/30... Step: 16020... Loss: 1.2734... Val Loss: 1.7814\n","Epoch: 22/30... Step: 16030... Loss: 1.2809... Val Loss: 1.7928\n","Epoch: 22/30... Step: 16040... Loss: 1.2439... Val Loss: 1.8069\n","Epoch: 22/30... Step: 16050... Loss: 1.2778... Val Loss: 1.8013\n","Epoch: 22/30... Step: 16060... Loss: 1.2671... Val Loss: 1.8137\n","Epoch: 22/30... Step: 16070... Loss: 1.2695... Val Loss: 1.7860\n","Epoch: 22/30... Step: 16080... Loss: 1.2672... Val Loss: 1.7858\n","Epoch: 22/30... Step: 16090... Loss: 1.2922... Val Loss: 1.8209\n","Epoch: 22/30... Step: 16100... Loss: 1.2714... Val Loss: 1.7893\n","Epoch: 22/30... Step: 16110... Loss: 1.2798... Val Loss: 1.7925\n","Epoch: 22/30... Step: 16120... Loss: 1.2821... Val Loss: 1.8052\n","Epoch: 22/30... Step: 16130... Loss: 1.2732... Val Loss: 1.8096\n","Epoch: 22/30... Step: 16140... Loss: 1.2679... Val Loss: 1.7968\n","Epoch: 22/30... Step: 16150... Loss: 1.2679... Val Loss: 1.7984\n","Epoch: 22/30... Step: 16160... Loss: 1.2806... Val Loss: 1.8146\n","Epoch: 22/30... Step: 16170... Loss: 1.2912... Val Loss: 1.7779\n","Epoch: 22/30... Step: 16180... Loss: 1.2337... Val Loss: 1.7982\n","Epoch: 22/30... Step: 16190... Loss: 1.2951... Val Loss: 1.8043\n","Epoch: 22/30... Step: 16200... Loss: 1.2588... Val Loss: 1.7811\n","Epoch: 22/30... Step: 16210... Loss: 1.2718... Val Loss: 1.7995\n","Epoch: 22/30... Step: 16220... Loss: 1.2821... Val Loss: 1.7838\n","Epoch: 22/30... Step: 16230... Loss: 1.2439... Val Loss: 1.7666\n","Epoch: 22/30... Step: 16240... Loss: 1.2766... Val Loss: 1.7985\n","Epoch: 22/30... Step: 16250... Loss: 1.2669... Val Loss: 1.7906\n","Epoch: 22/30... Step: 16260... Loss: 1.2346... Val Loss: 1.8099\n","Epoch: 22/30... Step: 16270... Loss: 1.2563... Val Loss: 1.8073\n","Epoch: 22/30... Step: 16280... Loss: 1.2964... Val Loss: 1.7767\n","Epoch: 22/30... Step: 16290... Loss: 1.2515... Val Loss: 1.7796\n","Epoch: 22/30... Step: 16300... Loss: 1.2731... Val Loss: 1.8015\n","Epoch: 22/30... Step: 16310... Loss: 1.2611... Val Loss: 1.7941\n","Epoch: 22/30... Step: 16320... Loss: 1.2710... Val Loss: 1.7999\n","Epoch: 22/30... Step: 16330... Loss: 1.2600... Val Loss: 1.7699\n","Epoch: 22/30... Step: 16340... Loss: 1.2711... Val Loss: 1.7661\n","Epoch: 22/30... Step: 16350... Loss: 1.2495... Val Loss: 1.7895\n","Epoch: 22/30... Step: 16360... Loss: 1.2610... Val Loss: 1.7896\n","Epoch: 22/30... Step: 16370... Loss: 1.2439... Val Loss: 1.7891\n","Epoch: 22/30... Step: 16380... Loss: 1.2510... Val Loss: 1.7532\n","Epoch: 22/30... Step: 16390... Loss: 1.2451... Val Loss: 1.7818\n","Epoch: 22/30... Step: 16400... Loss: 1.2659... Val Loss: 1.7498\n","Epoch: 22/30... Step: 16410... Loss: 1.2313... Val Loss: 1.7868\n","Epoch: 22/30... Step: 16420... Loss: 1.2737... Val Loss: 1.7899\n","Epoch: 22/30... Step: 16430... Loss: 1.2631... Val Loss: 1.7531\n","Epoch: 22/30... Step: 16440... Loss: 1.2565... Val Loss: 1.7457\n","Epoch: 22/30... Step: 16450... Loss: 1.2534... Val Loss: 1.7699\n","Epoch: 23/30... Step: 16460... Loss: 1.2709... Val Loss: 1.8165\n","Epoch: 23/30... Step: 16470... Loss: 1.2843... Val Loss: 1.8016\n","Epoch: 23/30... Step: 16480... Loss: 1.2484... Val Loss: 1.7863\n","Epoch: 23/30... Step: 16490... Loss: 1.2808... Val Loss: 1.8099\n","Epoch: 23/30... Step: 16500... Loss: 1.2921... Val Loss: 1.7837\n","Epoch: 23/30... Step: 16510... Loss: 1.2643... Val Loss: 1.7924\n","Epoch: 23/30... Step: 16520... Loss: 1.2504... Val Loss: 1.8007\n","Epoch: 23/30... Step: 16530... Loss: 1.2768... Val Loss: 1.7793\n","Epoch: 23/30... Step: 16540... Loss: 1.2843... Val Loss: 1.7933\n","Epoch: 23/30... Step: 16550... Loss: 1.2543... Val Loss: 1.8026\n","Epoch: 23/30... Step: 16560... Loss: 1.2890... Val Loss: 1.7937\n","Epoch: 23/30... Step: 16570... Loss: 1.2523... Val Loss: 1.7964\n","Epoch: 23/30... Step: 16580... Loss: 1.2740... Val Loss: 1.8019\n","Epoch: 23/30... Step: 16590... Loss: 1.2891... Val Loss: 1.7879\n","Epoch: 23/30... Step: 16600... Loss: 1.2536... Val Loss: 1.7881\n","Epoch: 23/30... Step: 16610... Loss: 1.2609... Val Loss: 1.7974\n","Epoch: 23/30... Step: 16620... Loss: 1.2798... Val Loss: 1.7941\n","Epoch: 23/30... Step: 16630... Loss: 1.2735... Val Loss: 1.7936\n","Epoch: 23/30... Step: 16640... Loss: 1.3024... Val Loss: 1.8095\n","Epoch: 23/30... Step: 16650... Loss: 1.2396... Val Loss: 1.8085\n","Epoch: 23/30... Step: 16660... Loss: 1.2428... Val Loss: 1.8022\n","Epoch: 23/30... Step: 16670... Loss: 1.3029... Val Loss: 1.8027\n","Epoch: 23/30... Step: 16680... Loss: 1.2370... Val Loss: 1.8201\n","Epoch: 23/30... Step: 16690... Loss: 1.2650... Val Loss: 1.8086\n","Epoch: 23/30... Step: 16700... Loss: 1.3013... Val Loss: 1.8041\n","Epoch: 23/30... Step: 16710... Loss: 1.2814... Val Loss: 1.7889\n","Epoch: 23/30... Step: 16720... Loss: 1.2733... Val Loss: 1.8096\n","Epoch: 23/30... Step: 16730... Loss: 1.2861... Val Loss: 1.7789\n","Epoch: 23/30... Step: 16740... Loss: 1.2644... Val Loss: 1.7788\n","Epoch: 23/30... Step: 16750... Loss: 1.2362... Val Loss: 1.7956\n","Epoch: 23/30... Step: 16760... Loss: 1.2684... Val Loss: 1.7962\n","Epoch: 23/30... Step: 16770... Loss: 1.2640... Val Loss: 1.7836\n","Epoch: 23/30... Step: 16780... Loss: 1.2822... Val Loss: 1.8005\n","Epoch: 23/30... Step: 16790... Loss: 1.2771... Val Loss: 1.7837\n","Epoch: 23/30... Step: 16800... Loss: 1.2823... Val Loss: 1.7821\n","Epoch: 23/30... Step: 16810... Loss: 1.2744... Val Loss: 1.7980\n","Epoch: 23/30... Step: 16820... Loss: 1.2557... Val Loss: 1.7865\n","Epoch: 23/30... Step: 16830... Loss: 1.2660... Val Loss: 1.7874\n","Epoch: 23/30... Step: 16840... Loss: 1.2734... Val Loss: 1.7975\n","Epoch: 23/30... Step: 16850... Loss: 1.2708... Val Loss: 1.7723\n","Epoch: 23/30... Step: 16860... Loss: 1.2785... Val Loss: 1.7680\n","Epoch: 23/30... Step: 16870... Loss: 1.3058... Val Loss: 1.7550\n","Epoch: 23/30... Step: 16880... Loss: 1.2694... Val Loss: 1.7774\n","Epoch: 23/30... Step: 16890... Loss: 1.2529... Val Loss: 1.7854\n","Epoch: 23/30... Step: 16900... Loss: 1.2376... Val Loss: 1.7646\n","Epoch: 23/30... Step: 16910... Loss: 1.2469... Val Loss: 1.7996\n","Epoch: 23/30... Step: 16920... Loss: 1.2776... Val Loss: 1.7868\n","Epoch: 23/30... Step: 16930... Loss: 1.2694... Val Loss: 1.7990\n","Epoch: 23/30... Step: 16940... Loss: 1.2807... Val Loss: 1.8113\n","Epoch: 23/30... Step: 16950... Loss: 1.2732... Val Loss: 1.7909\n","Epoch: 23/30... Step: 16960... Loss: 1.2551... Val Loss: 1.8044\n","Epoch: 23/30... Step: 16970... Loss: 1.3078... Val Loss: 1.7846\n","Epoch: 23/30... Step: 16980... Loss: 1.2521... Val Loss: 1.7810\n","Epoch: 23/30... Step: 16990... Loss: 1.2801... Val Loss: 1.7762\n","Epoch: 23/30... Step: 17000... Loss: 1.2679... Val Loss: 1.7795\n","Epoch: 23/30... Step: 17010... Loss: 1.2572... Val Loss: 1.7972\n","Epoch: 23/30... Step: 17020... Loss: 1.2690... Val Loss: 1.7988\n","Epoch: 23/30... Step: 17030... Loss: 1.2564... Val Loss: 1.7832\n","Epoch: 23/30... Step: 17040... Loss: 1.2653... Val Loss: 1.7466\n","Epoch: 23/30... Step: 17050... Loss: 1.2863... Val Loss: 1.8020\n","Epoch: 23/30... Step: 17060... Loss: 1.2818... Val Loss: 1.8049\n","Epoch: 23/30... Step: 17070... Loss: 1.2698... Val Loss: 1.7681\n","Epoch: 23/30... Step: 17080... Loss: 1.2725... Val Loss: 1.7654\n","Epoch: 23/30... Step: 17090... Loss: 1.2695... Val Loss: 1.7924\n","Epoch: 23/30... Step: 17100... Loss: 1.2471... Val Loss: 1.8021\n","Epoch: 23/30... Step: 17110... Loss: 1.2812... Val Loss: 1.7896\n","Epoch: 23/30... Step: 17120... Loss: 1.2784... Val Loss: 1.7974\n","Epoch: 23/30... Step: 17130... Loss: 1.2512... Val Loss: 1.7826\n","Epoch: 23/30... Step: 17140... Loss: 1.2431... Val Loss: 1.8022\n","Epoch: 23/30... Step: 17150... Loss: 1.2707... Val Loss: 1.7935\n","Epoch: 23/30... Step: 17160... Loss: 1.2443... Val Loss: 1.8025\n","Epoch: 23/30... Step: 17170... Loss: 1.2526... Val Loss: 1.8068\n","Epoch: 23/30... Step: 17180... Loss: 1.2680... Val Loss: 1.8199\n","Epoch: 23/30... Step: 17190... Loss: 1.2488... Val Loss: 1.7849\n","Epoch: 23/30... Step: 17200... Loss: 1.2524... Val Loss: 1.8243\n","Epoch: 24/30... Step: 17210... Loss: 1.2596... Val Loss: 1.7619\n","Epoch: 24/30... Step: 17220... Loss: 1.2420... Val Loss: 1.6736\n","Epoch: 24/30... Step: 17230... Loss: 1.2783... Val Loss: 1.6575\n","Epoch: 24/30... Step: 17240... Loss: 1.2411... Val Loss: 1.6390\n","Epoch: 24/30... Step: 17250... Loss: 1.2887... Val Loss: 1.6457\n","Epoch: 24/30... Step: 17260... Loss: 1.2601... Val Loss: 1.6546\n","Epoch: 24/30... Step: 17270... Loss: 1.2515... Val Loss: 1.6642\n","Epoch: 24/30... Step: 17280... Loss: 1.2800... Val Loss: 1.6321\n","Epoch: 24/30... Step: 17290... Loss: 1.2808... Val Loss: 1.6399\n","Epoch: 24/30... Step: 17300... Loss: 1.2525... Val Loss: 1.6466\n","Epoch: 24/30... Step: 17310... Loss: 1.2630... Val Loss: 1.6494\n","Epoch: 24/30... Step: 17320... Loss: 1.2642... Val Loss: 1.6475\n","Epoch: 24/30... Step: 17330... Loss: 1.2607... Val Loss: 1.6758\n","Epoch: 24/30... Step: 17340... Loss: 1.2421... Val Loss: 1.6851\n","Epoch: 24/30... Step: 17350... Loss: 1.2534... Val Loss: 1.6656\n","Epoch: 24/30... Step: 17360... Loss: 1.2770... Val Loss: 1.6816\n","Epoch: 24/30... Step: 17370... Loss: 1.2620... Val Loss: 1.6823\n","Epoch: 24/30... Step: 17380... Loss: 1.2683... Val Loss: 1.6785\n","Epoch: 24/30... Step: 17390... Loss: 1.2776... Val Loss: 1.6821\n","Epoch: 24/30... Step: 17400... Loss: 1.2580... Val Loss: 1.6573\n","Epoch: 24/30... Step: 17410... Loss: 1.2889... Val Loss: 1.6850\n","Epoch: 24/30... Step: 17420... Loss: 1.2573... Val Loss: 1.6959\n","Epoch: 24/30... Step: 17430... Loss: 1.2566... Val Loss: 1.6583\n","Epoch: 24/30... Step: 17440... Loss: 1.2901... Val Loss: 1.6827\n","Epoch: 24/30... Step: 17450... Loss: 1.2814... Val Loss: 1.6464\n","Epoch: 24/30... Step: 17460... Loss: 1.2764... Val Loss: 1.6665\n","Epoch: 24/30... Step: 17470... Loss: 1.2690... Val Loss: 1.7722\n","Epoch: 24/30... Step: 17480... Loss: 1.2658... Val Loss: 1.6627\n","Epoch: 24/30... Step: 17490... Loss: 1.2747... Val Loss: 1.6557\n","Epoch: 24/30... Step: 17500... Loss: 1.2844... Val Loss: 1.6874\n","Epoch: 24/30... Step: 17510... Loss: 1.2690... Val Loss: 1.6725\n","Epoch: 24/30... Step: 17520... Loss: 1.2709... Val Loss: 1.7422\n","Epoch: 24/30... Step: 17530... Loss: 1.2654... Val Loss: 1.7572\n","Epoch: 24/30... Step: 17540... Loss: 1.2552... Val Loss: 1.7704\n","Epoch: 24/30... Step: 17550... Loss: 1.2466... Val Loss: 1.7362\n","Epoch: 24/30... Step: 17560... Loss: 1.2906... Val Loss: 1.7711\n","Epoch: 24/30... Step: 17570... Loss: 1.2620... Val Loss: 1.7826\n","Epoch: 24/30... Step: 17580... Loss: 1.2542... Val Loss: 1.7330\n","Epoch: 24/30... Step: 17590... Loss: 1.2469... Val Loss: 1.7337\n","Epoch: 24/30... Step: 17600... Loss: 1.2372... Val Loss: 1.7387\n","Epoch: 24/30... Step: 17610... Loss: 1.2311... Val Loss: 1.7632\n","Epoch: 24/30... Step: 17620... Loss: 1.2577... Val Loss: 1.7345\n","Epoch: 24/30... Step: 17630... Loss: 1.2678... Val Loss: 1.7520\n","Epoch: 24/30... Step: 17640... Loss: 1.2717... Val Loss: 1.7944\n","Epoch: 24/30... Step: 17650... Loss: 1.2513... Val Loss: 1.6808\n","Epoch: 24/30... Step: 17660... Loss: 1.2605... Val Loss: 1.7809\n","Epoch: 24/30... Step: 17670... Loss: 1.2699... Val Loss: 1.7735\n","Epoch: 24/30... Step: 17680... Loss: 1.2634... Val Loss: 1.7760\n","Epoch: 24/30... Step: 17690... Loss: 1.2457... Val Loss: 1.7846\n","Epoch: 24/30... Step: 17700... Loss: 1.2826... Val Loss: 1.7680\n","Epoch: 24/30... Step: 17710... Loss: 1.2299... Val Loss: 1.7513\n","Epoch: 24/30... Step: 17720... Loss: 1.2564... Val Loss: 1.7457\n","Epoch: 24/30... Step: 17730... Loss: 1.2701... Val Loss: 1.7138\n","Epoch: 24/30... Step: 17740... Loss: 1.2549... Val Loss: 1.6781\n","Epoch: 24/30... Step: 17750... Loss: 1.2718... Val Loss: 1.7349\n","Epoch: 24/30... Step: 17760... Loss: 1.2759... Val Loss: 1.7384\n","Epoch: 24/30... Step: 17770... Loss: 1.2404... Val Loss: 1.7599\n","Epoch: 24/30... Step: 17780... Loss: 1.2677... Val Loss: 1.7363\n","Epoch: 24/30... Step: 17790... Loss: 1.2308... Val Loss: 1.7611\n","Epoch: 24/30... Step: 17800... Loss: 1.2642... Val Loss: 1.6882\n","Epoch: 24/30... Step: 17810... Loss: 1.2549... Val Loss: 1.7851\n","Epoch: 24/30... Step: 17820... Loss: 1.2710... Val Loss: 1.7284\n","Epoch: 24/30... Step: 17830... Loss: 1.2728... Val Loss: 1.7571\n","Epoch: 24/30... Step: 17840... Loss: 1.2720... Val Loss: 1.7937\n","Epoch: 24/30... Step: 17850... Loss: 1.2846... Val Loss: 1.7358\n","Epoch: 24/30... Step: 17860... Loss: 1.2731... Val Loss: 1.7333\n","Epoch: 24/30... Step: 17870... Loss: 1.2782... Val Loss: 1.7387\n","Epoch: 24/30... Step: 17880... Loss: 1.2315... Val Loss: 1.7416\n","Epoch: 24/30... Step: 17890... Loss: 1.2439... Val Loss: 1.7824\n","Epoch: 24/30... Step: 17900... Loss: 1.2590... Val Loss: 1.7146\n","Epoch: 24/30... Step: 17910... Loss: 1.2592... Val Loss: 1.7697\n","Epoch: 24/30... Step: 17920... Loss: 1.2657... Val Loss: 1.6905\n","Epoch: 24/30... Step: 17930... Loss: 1.2181... Val Loss: 1.7170\n","Epoch: 24/30... Step: 17940... Loss: 1.2491... Val Loss: 1.6656\n","Epoch: 24/30... Step: 17950... Loss: 1.2603... Val Loss: 1.7346\n","Epoch: 25/30... Step: 17960... Loss: 1.2682... Val Loss: 1.7930\n","Epoch: 25/30... Step: 17970... Loss: 1.2696... Val Loss: 1.7268\n","Epoch: 25/30... Step: 17980... Loss: 1.2715... Val Loss: 1.7533\n","Epoch: 25/30... Step: 17990... Loss: 1.2845... Val Loss: 1.7864\n","Epoch: 25/30... Step: 18000... Loss: 1.2829... Val Loss: 1.7414\n","Epoch: 25/30... Step: 18010... Loss: 1.2886... Val Loss: 1.7652\n","Epoch: 25/30... Step: 18020... Loss: 1.2434... Val Loss: 1.7472\n","Epoch: 25/30... Step: 18030... Loss: 1.3124... Val Loss: 1.7434\n","Epoch: 25/30... Step: 18040... Loss: 1.2551... Val Loss: 1.7722\n","Epoch: 25/30... Step: 18050... Loss: 1.2477... Val Loss: 1.7990\n","Epoch: 25/30... Step: 18060... Loss: 1.2633... Val Loss: 1.7679\n","Epoch: 25/30... Step: 18070... Loss: 1.2512... Val Loss: 1.7800\n","Epoch: 25/30... Step: 18080... Loss: 1.2599... Val Loss: 1.7633\n","Epoch: 25/30... Step: 18090... Loss: 1.2589... Val Loss: 1.7793\n","Epoch: 25/30... Step: 18100... Loss: 1.2520... Val Loss: 1.7841\n","Epoch: 25/30... Step: 18110... Loss: 1.2496... Val Loss: 1.7953\n","Epoch: 25/30... Step: 18120... Loss: 1.2651... Val Loss: 1.7747\n","Epoch: 25/30... Step: 18130... Loss: 1.2830... Val Loss: 1.7756\n","Epoch: 25/30... Step: 18140... Loss: 1.2955... Val Loss: 1.7871\n","Epoch: 25/30... Step: 18150... Loss: 1.2692... Val Loss: 1.7796\n","Epoch: 25/30... Step: 18160... Loss: 1.2596... Val Loss: 1.8059\n","Epoch: 25/30... Step: 18170... Loss: 1.2498... Val Loss: 1.8101\n","Epoch: 25/30... Step: 18180... Loss: 1.2642... Val Loss: 1.7758\n","Epoch: 25/30... Step: 18190... Loss: 1.2682... Val Loss: 1.8003\n","Epoch: 25/30... Step: 18200... Loss: 1.2513... Val Loss: 1.7708\n","Epoch: 25/30... Step: 18210... Loss: 1.2468... Val Loss: 1.8090\n","Epoch: 25/30... Step: 18220... Loss: 1.2532... Val Loss: 1.8208\n","Epoch: 25/30... Step: 18230... Loss: 1.2574... Val Loss: 1.7776\n","Epoch: 25/30... Step: 18240... Loss: 1.2687... Val Loss: 1.7728\n","Epoch: 25/30... Step: 18250... Loss: 1.2663... Val Loss: 1.7961\n","Epoch: 25/30... Step: 18260... Loss: 1.2597... Val Loss: 1.8048\n","Epoch: 25/30... Step: 18270... Loss: 1.2850... Val Loss: 1.8030\n","Epoch: 25/30... Step: 18280... Loss: 1.2409... Val Loss: 1.8026\n","Epoch: 25/30... Step: 18290... Loss: 1.2970... Val Loss: 1.8050\n","Epoch: 25/30... Step: 18300... Loss: 1.2605... Val Loss: 1.7876\n","Epoch: 25/30... Step: 18310... Loss: 1.2728... Val Loss: 1.7830\n","Epoch: 25/30... Step: 18320... Loss: 1.2612... Val Loss: 1.7774\n","Epoch: 25/30... Step: 18330... Loss: 1.2394... Val Loss: 1.7929\n","Epoch: 25/30... Step: 18340... Loss: 1.2681... Val Loss: 1.7785\n","Epoch: 25/30... Step: 18350... Loss: 1.2363... Val Loss: 1.7666\n","Epoch: 25/30... Step: 18360... Loss: 1.2252... Val Loss: 1.7875\n","Epoch: 25/30... Step: 18370... Loss: 1.2243... Val Loss: 1.7707\n","Epoch: 25/30... Step: 18380... Loss: 1.2703... Val Loss: 1.7795\n","Epoch: 25/30... Step: 18390... Loss: 1.2538... Val Loss: 1.7953\n","Epoch: 25/30... Step: 18400... Loss: 1.2511... Val Loss: 1.7866\n","Epoch: 25/30... Step: 18410... Loss: 1.2708... Val Loss: 1.7864\n","Epoch: 25/30... Step: 18420... Loss: 1.2594... Val Loss: 1.7883\n","Epoch: 25/30... Step: 18430... Loss: 1.2638... Val Loss: 1.8036\n","Epoch: 25/30... Step: 18440... Loss: 1.2519... Val Loss: 1.7964\n","Epoch: 25/30... Step: 18450... Loss: 1.2468... Val Loss: 1.8184\n","Epoch: 25/30... Step: 18460... Loss: 1.2512... Val Loss: 1.8007\n","Epoch: 25/30... Step: 18470... Loss: 1.2550... Val Loss: 1.7940\n","Epoch: 25/30... Step: 18480... Loss: 1.2898... Val Loss: 1.7776\n","Epoch: 25/30... Step: 18490... Loss: 1.2816... Val Loss: 1.7670\n","Epoch: 25/30... Step: 18500... Loss: 1.2732... Val Loss: 1.7847\n","Epoch: 25/30... Step: 18510... Loss: 1.2480... Val Loss: 1.7952\n","Epoch: 25/30... Step: 18520... Loss: 1.2707... Val Loss: 1.7771\n","Epoch: 25/30... Step: 18530... Loss: 1.2682... Val Loss: 1.8092\n","Epoch: 25/30... Step: 18540... Loss: 1.2449... Val Loss: 1.8003\n","Epoch: 25/30... Step: 18550... Loss: 1.2400... Val Loss: 1.7964\n","Epoch: 25/30... Step: 18560... Loss: 1.2621... Val Loss: 1.8115\n","Epoch: 25/30... Step: 18570... Loss: 1.2683... Val Loss: 1.8224\n","Epoch: 25/30... Step: 18580... Loss: 1.2407... Val Loss: 1.7948\n","Epoch: 25/30... Step: 18590... Loss: 1.2612... Val Loss: 1.8026\n","Epoch: 25/30... Step: 18600... Loss: 1.2470... Val Loss: 1.8046\n","Epoch: 25/30... Step: 18610... Loss: 1.2595... Val Loss: 1.8081\n","Epoch: 25/30... Step: 18620... Loss: 1.2657... Val Loss: 1.7938\n","Epoch: 25/30... Step: 18630... Loss: 1.2521... Val Loss: 1.7735\n","Epoch: 25/30... Step: 18640... Loss: 1.2749... Val Loss: 1.7955\n","Epoch: 25/30... Step: 18650... Loss: 1.2630... Val Loss: 1.8063\n","Epoch: 25/30... Step: 18660... Loss: 1.2686... Val Loss: 1.8089\n","Epoch: 25/30... Step: 18670... Loss: 1.2543... Val Loss: 1.7921\n","Epoch: 25/30... Step: 18680... Loss: 1.2823... Val Loss: 1.7838\n","Epoch: 25/30... Step: 18690... Loss: 1.2713... Val Loss: 1.7660\n","Epoch: 25/30... Step: 18700... Loss: 1.3143... Val Loss: 1.7958\n","Epoch: 26/30... Step: 18710... Loss: 1.2508... Val Loss: 1.6912\n","Epoch: 26/30... Step: 18720... Loss: 1.2500... Val Loss: 1.6644\n","Epoch: 26/30... Step: 18730... Loss: 1.2484... Val Loss: 1.6275\n","Epoch: 26/30... Step: 18740... Loss: 1.2878... Val Loss: 1.6265\n","Epoch: 26/30... Step: 18750... Loss: 1.2667... Val Loss: 1.6389\n","Epoch: 26/30... Step: 18760... Loss: 1.2710... Val Loss: 1.6516\n","Epoch: 26/30... Step: 18770... Loss: 1.2858... Val Loss: 1.6289\n","Epoch: 26/30... Step: 18780... Loss: 1.2763... Val Loss: 1.6281\n","Epoch: 26/30... Step: 18790... Loss: 1.2518... Val Loss: 1.6537\n","Epoch: 26/30... Step: 18800... Loss: 1.2645... Val Loss: 1.6960\n","Epoch: 26/30... Step: 18810... Loss: 1.2777... Val Loss: 1.6405\n","Epoch: 26/30... Step: 18820... Loss: 1.2732... Val Loss: 1.6524\n","Epoch: 26/30... Step: 18830... Loss: 1.2569... Val Loss: 1.6519\n","Epoch: 26/30... Step: 18840... Loss: 1.2644... Val Loss: 1.6705\n","Epoch: 26/30... Step: 18850... Loss: 1.2858... Val Loss: 1.6712\n","Epoch: 26/30... Step: 18860... Loss: 1.2604... Val Loss: 1.6872\n","Epoch: 26/30... Step: 18870... Loss: 1.2650... Val Loss: 1.6368\n","Epoch: 26/30... Step: 18880... Loss: 1.2877... Val Loss: 1.6474\n","Epoch: 26/30... Step: 18890... Loss: 1.2317... Val Loss: 1.6593\n","Epoch: 26/30... Step: 18900... Loss: 1.2683... Val Loss: 1.6727\n","Epoch: 26/30... Step: 18910... Loss: 1.2590... Val Loss: 1.6811\n","Epoch: 26/30... Step: 18920... Loss: 1.2267... Val Loss: 1.6701\n","Epoch: 26/30... Step: 18930... Loss: 1.2634... Val Loss: 1.6323\n","Epoch: 26/30... Step: 18940... Loss: 1.2570... Val Loss: 1.6634\n","Epoch: 26/30... Step: 18950... Loss: 1.2712... Val Loss: 1.6419\n","Epoch: 26/30... Step: 18960... Loss: 1.2618... Val Loss: 1.6723\n","Epoch: 26/30... Step: 18970... Loss: 1.2755... Val Loss: 1.6660\n","Epoch: 26/30... Step: 18980... Loss: 1.2697... Val Loss: 1.6655\n","Epoch: 26/30... Step: 18990... Loss: 1.2764... Val Loss: 1.6607\n","Epoch: 26/30... Step: 19000... Loss: 1.2789... Val Loss: 1.6655\n","Epoch: 26/30... Step: 19010... Loss: 1.2654... Val Loss: 1.6560\n","Epoch: 26/30... Step: 19020... Loss: 1.2415... Val Loss: 1.6569\n","Epoch: 26/30... Step: 19030... Loss: 1.2535... Val Loss: 1.6902\n","Epoch: 26/30... Step: 19040... Loss: 1.2428... Val Loss: 1.6634\n","Epoch: 26/30... Step: 19050... Loss: 1.2514... Val Loss: 1.7231\n","Epoch: 26/30... Step: 19060... Loss: 1.2864... Val Loss: 1.6864\n","Epoch: 26/30... Step: 19070... Loss: 1.2375... Val Loss: 1.7057\n","Epoch: 26/30... Step: 19080... Loss: 1.2473... Val Loss: 1.7139\n","Epoch: 26/30... Step: 19090... Loss: 1.2470... Val Loss: 1.7025\n","Epoch: 26/30... Step: 19100... Loss: 1.2283... Val Loss: 1.6800\n","Epoch: 26/30... Step: 19110... Loss: 1.2627... Val Loss: 1.7128\n","Epoch: 26/30... Step: 19120... Loss: 1.2521... Val Loss: 1.6926\n","Epoch: 26/30... Step: 19130... Loss: 1.2677... Val Loss: 1.6807\n","Epoch: 26/30... Step: 19140... Loss: 1.2540... Val Loss: 1.6822\n","Epoch: 26/30... Step: 19150... Loss: 1.2461... Val Loss: 1.7002\n","Epoch: 26/30... Step: 19160... Loss: 1.2490... Val Loss: 1.6730\n","Epoch: 26/30... Step: 19170... Loss: 1.2574... Val Loss: 1.6761\n","Epoch: 26/30... Step: 19180... Loss: 1.2381... Val Loss: 1.6903\n","Epoch: 26/30... Step: 19190... Loss: 1.2272... Val Loss: 1.6874\n","Epoch: 26/30... Step: 19200... Loss: 1.2419... Val Loss: 1.6965\n","Epoch: 26/30... Step: 19210... Loss: 1.2396... Val Loss: 1.6887\n","Epoch: 26/30... Step: 19220... Loss: 1.2327... Val Loss: 1.6813\n","Epoch: 26/30... Step: 19230... Loss: 1.2572... Val Loss: 1.7074\n","Epoch: 26/30... Step: 19240... Loss: 1.2692... Val Loss: 1.6905\n","Epoch: 26/30... Step: 19250... Loss: 1.2543... Val Loss: 1.6845\n","Epoch: 26/30... Step: 19260... Loss: 1.2444... Val Loss: 1.7114\n","Epoch: 26/30... Step: 19270... Loss: 1.2534... Val Loss: 1.7304\n","Epoch: 26/30... Step: 19280... Loss: 1.2642... Val Loss: 1.6972\n","Epoch: 26/30... Step: 19290... Loss: 1.2502... Val Loss: 1.7245\n","Epoch: 26/30... Step: 19300... Loss: 1.2578... Val Loss: 1.7524\n","Epoch: 26/30... Step: 19310... Loss: 1.2504... Val Loss: 1.6954\n","Epoch: 26/30... Step: 19320... Loss: 1.2683... Val Loss: 1.6802\n","Epoch: 26/30... Step: 19330... Loss: 1.2446... Val Loss: 1.7457\n","Epoch: 26/30... Step: 19340... Loss: 1.2484... Val Loss: 1.7142\n","Epoch: 26/30... Step: 19350... Loss: 1.2531... Val Loss: 1.7277\n","Epoch: 26/30... Step: 19360... Loss: 1.2494... Val Loss: 1.7266\n","Epoch: 26/30... Step: 19370... Loss: 1.2568... Val Loss: 1.6711\n","Epoch: 26/30... Step: 19380... Loss: 1.2329... Val Loss: 1.7191\n","Epoch: 26/30... Step: 19390... Loss: 1.2423... Val Loss: 1.7047\n","Epoch: 26/30... Step: 19400... Loss: 1.2735... Val Loss: 1.7218\n","Epoch: 26/30... Step: 19410... Loss: 1.2618... Val Loss: 1.7121\n","Epoch: 26/30... Step: 19420... Loss: 1.2455... Val Loss: 1.6886\n","Epoch: 26/30... Step: 19430... Loss: 1.2480... Val Loss: 1.7156\n","Epoch: 26/30... Step: 19440... Loss: 1.2570... Val Loss: 1.6878\n","Epoch: 27/30... Step: 19450... Loss: 1.2416... Val Loss: 1.8367\n","Epoch: 27/30... Step: 19460... Loss: 1.2260... Val Loss: 1.8078\n","Epoch: 27/30... Step: 19470... Loss: 1.2718... Val Loss: 1.7531\n","Epoch: 27/30... Step: 19480... Loss: 1.2678... Val Loss: 1.7949\n","Epoch: 27/30... Step: 19490... Loss: 1.2664... Val Loss: 1.8126\n","Epoch: 27/30... Step: 19500... Loss: 1.2595... Val Loss: 1.8135\n","Epoch: 27/30... Step: 19510... Loss: 1.2513... Val Loss: 1.8145\n","Epoch: 27/30... Step: 19520... Loss: 1.2791... Val Loss: 1.8057\n","Epoch: 27/30... Step: 19530... Loss: 1.2487... Val Loss: 1.7890\n","Epoch: 27/30... Step: 19540... Loss: 1.2677... Val Loss: 1.8033\n","Epoch: 27/30... Step: 19550... Loss: 1.2594... Val Loss: 1.8058\n","Epoch: 27/30... Step: 19560... Loss: 1.2467... Val Loss: 1.8012\n","Epoch: 27/30... Step: 19570... Loss: 1.2812... Val Loss: 1.8145\n","Epoch: 27/30... Step: 19580... Loss: 1.2332... Val Loss: 1.8022\n","Epoch: 27/30... Step: 19590... Loss: 1.2523... Val Loss: 1.7947\n","Epoch: 27/30... Step: 19600... Loss: 1.2234... Val Loss: 1.8160\n","Epoch: 27/30... Step: 19610... Loss: 1.2684... Val Loss: 1.8212\n","Epoch: 27/30... Step: 19620... Loss: 1.2584... Val Loss: 1.8084\n","Epoch: 27/30... Step: 19630... Loss: 1.2331... Val Loss: 1.7939\n","Epoch: 27/30... Step: 19640... Loss: 1.2635... Val Loss: 1.8070\n","Epoch: 27/30... Step: 19650... Loss: 1.2647... Val Loss: 1.8155\n","Epoch: 27/30... Step: 19660... Loss: 1.2675... Val Loss: 1.8161\n","Epoch: 27/30... Step: 19670... Loss: 1.2289... Val Loss: 1.8282\n","Epoch: 27/30... Step: 19680... Loss: 1.2447... Val Loss: 1.7849\n","Epoch: 27/30... Step: 19690... Loss: 1.2466... Val Loss: 1.8051\n","Epoch: 27/30... Step: 19700... Loss: 1.2578... Val Loss: 1.8069\n","Epoch: 27/30... Step: 19710... Loss: 1.2426... Val Loss: 1.7927\n","Epoch: 27/30... Step: 19720... Loss: 1.2705... Val Loss: 1.8114\n","Epoch: 27/30... Step: 19730... Loss: 1.2698... Val Loss: 1.8092\n","Epoch: 27/30... Step: 19740... Loss: 1.2766... Val Loss: 1.7833\n","Epoch: 27/30... Step: 19750... Loss: 1.2519... Val Loss: 1.7922\n","Epoch: 27/30... Step: 19760... Loss: 1.2555... Val Loss: 1.7951\n","Epoch: 27/30... Step: 19770... Loss: 1.2604... Val Loss: 1.8188\n","Epoch: 27/30... Step: 19780... Loss: 1.2346... Val Loss: 1.8213\n","Epoch: 27/30... Step: 19790... Loss: 1.2652... Val Loss: 1.7783\n","Epoch: 27/30... Step: 19800... Loss: 1.2488... Val Loss: 1.8049\n","Epoch: 27/30... Step: 19810... Loss: 1.2550... Val Loss: 1.7958\n","Epoch: 27/30... Step: 19820... Loss: 1.2547... Val Loss: 1.7932\n","Epoch: 27/30... Step: 19830... Loss: 1.2701... Val Loss: 1.8241\n","Epoch: 27/30... Step: 19840... Loss: 1.2609... Val Loss: 1.8250\n","Epoch: 27/30... Step: 19850... Loss: 1.2488... Val Loss: 1.7919\n","Epoch: 27/30... Step: 19860... Loss: 1.2707... Val Loss: 1.8071\n","Epoch: 27/30... Step: 19870... Loss: 1.2576... Val Loss: 1.8003\n","Epoch: 27/30... Step: 19880... Loss: 1.2546... Val Loss: 1.7989\n","Epoch: 27/30... Step: 19890... Loss: 1.2615... Val Loss: 1.8054\n","Epoch: 27/30... Step: 19900... Loss: 1.2662... Val Loss: 1.8187\n","Epoch: 27/30... Step: 19910... Loss: 1.2711... Val Loss: 1.7980\n","Epoch: 27/30... Step: 19920... Loss: 1.2151... Val Loss: 1.7968\n","Epoch: 27/30... Step: 19930... Loss: 1.2777... Val Loss: 1.7936\n","Epoch: 27/30... Step: 19940... Loss: 1.2412... Val Loss: 1.8032\n","Epoch: 27/30... Step: 19950... Loss: 1.2572... Val Loss: 1.7977\n","Epoch: 27/30... Step: 19960... Loss: 1.2725... Val Loss: 1.7884\n","Epoch: 27/30... Step: 19970... Loss: 1.2256... Val Loss: 1.8046\n","Epoch: 27/30... Step: 19980... Loss: 1.2576... Val Loss: 1.7900\n","Epoch: 27/30... Step: 19990... Loss: 1.2535... Val Loss: 1.7980\n","Epoch: 27/30... Step: 20000... Loss: 1.2165... Val Loss: 1.8069\n","Epoch: 27/30... Step: 20010... Loss: 1.2381... Val Loss: 1.8021\n","Epoch: 27/30... Step: 20020... Loss: 1.2801... Val Loss: 1.7860\n","Epoch: 27/30... Step: 20030... Loss: 1.2421... Val Loss: 1.8020\n","Epoch: 27/30... Step: 20040... Loss: 1.2547... Val Loss: 1.8294\n","Epoch: 27/30... Step: 20050... Loss: 1.2533... Val Loss: 1.8166\n","Epoch: 27/30... Step: 20060... Loss: 1.2552... Val Loss: 1.7920\n","Epoch: 27/30... Step: 20070... Loss: 1.2422... Val Loss: 1.7922\n","Epoch: 27/30... Step: 20080... Loss: 1.2550... Val Loss: 1.8257\n","Epoch: 27/30... Step: 20090... Loss: 1.2387... Val Loss: 1.8165\n","Epoch: 27/30... Step: 20100... Loss: 1.2454... Val Loss: 1.8068\n","Epoch: 27/30... Step: 20110... Loss: 1.2321... Val Loss: 1.8192\n","Epoch: 27/30... Step: 20120... Loss: 1.2367... Val Loss: 1.8137\n","Epoch: 27/30... Step: 20130... Loss: 1.2286... Val Loss: 1.8088\n","Epoch: 27/30... Step: 20140... Loss: 1.2498... Val Loss: 1.8089\n","Epoch: 27/30... Step: 20150... Loss: 1.2211... Val Loss: 1.8204\n","Epoch: 27/30... Step: 20160... Loss: 1.2622... Val Loss: 1.8334\n","Epoch: 27/30... Step: 20170... Loss: 1.2490... Val Loss: 1.8172\n","Epoch: 27/30... Step: 20180... Loss: 1.2504... Val Loss: 1.8301\n","Epoch: 27/30... Step: 20190... Loss: 1.2322... Val Loss: 1.8154\n","Epoch: 28/30... Step: 20200... Loss: 1.2595... Val Loss: 1.7299\n","Epoch: 28/30... Step: 20210... Loss: 1.2633... Val Loss: 1.7497\n","Epoch: 28/30... Step: 20220... Loss: 1.2402... Val Loss: 1.6795\n","Epoch: 28/30... Step: 20230... Loss: 1.2697... Val Loss: 1.6706\n","Epoch: 28/30... Step: 20240... Loss: 1.2731... Val Loss: 1.6803\n","Epoch: 28/30... Step: 20250... Loss: 1.2598... Val Loss: 1.6782\n","Epoch: 28/30... Step: 20260... Loss: 1.2329... Val Loss: 1.6766\n","Epoch: 28/30... Step: 20270... Loss: 1.2516... Val Loss: 1.6638\n","Epoch: 28/30... Step: 20280... Loss: 1.2714... Val Loss: 1.6551\n","Epoch: 28/30... Step: 20290... Loss: 1.2495... Val Loss: 1.7025\n","Epoch: 28/30... Step: 20300... Loss: 1.2795... Val Loss: 1.6803\n","Epoch: 28/30... Step: 20310... Loss: 1.2476... Val Loss: 1.7012\n","Epoch: 28/30... Step: 20320... Loss: 1.2587... Val Loss: 1.6921\n","Epoch: 28/30... Step: 20330... Loss: 1.2695... Val Loss: 1.6987\n","Epoch: 28/30... Step: 20340... Loss: 1.2523... Val Loss: 1.6819\n","Epoch: 28/30... Step: 20350... Loss: 1.2489... Val Loss: 1.6972\n","Epoch: 28/30... Step: 20360... Loss: 1.2620... Val Loss: 1.7007\n","Epoch: 28/30... Step: 20370... Loss: 1.2620... Val Loss: 1.6682\n","Epoch: 28/30... Step: 20380... Loss: 1.2822... Val Loss: 1.7108\n","Epoch: 28/30... Step: 20390... Loss: 1.2281... Val Loss: 1.6966\n","Epoch: 28/30... Step: 20400... Loss: 1.2238... Val Loss: 1.7127\n","Epoch: 28/30... Step: 20410... Loss: 1.2825... Val Loss: 1.6840\n","Epoch: 28/30... Step: 20420... Loss: 1.2255... Val Loss: 1.7075\n","Epoch: 28/30... Step: 20430... Loss: 1.2532... Val Loss: 1.6717\n","Epoch: 28/30... Step: 20440... Loss: 1.2819... Val Loss: 1.6760\n","Epoch: 28/30... Step: 20450... Loss: 1.2632... Val Loss: 1.6929\n","Epoch: 28/30... Step: 20460... Loss: 1.2577... Val Loss: 1.7412\n","Epoch: 28/30... Step: 20470... Loss: 1.2735... Val Loss: 1.6837\n","Epoch: 28/30... Step: 20480... Loss: 1.2458... Val Loss: 1.6970\n","Epoch: 28/30... Step: 20490... Loss: 1.2212... Val Loss: 1.6975\n","Epoch: 28/30... Step: 20500... Loss: 1.2590... Val Loss: 1.7009\n","Epoch: 28/30... Step: 20510... Loss: 1.2405... Val Loss: 1.6742\n","Epoch: 28/30... Step: 20520... Loss: 1.2681... Val Loss: 1.6976\n","Epoch: 28/30... Step: 20530... Loss: 1.2543... Val Loss: 1.7718\n","Epoch: 28/30... Step: 20540... Loss: 1.2682... Val Loss: 1.6759\n","Epoch: 28/30... Step: 20550... Loss: 1.2605... Val Loss: 1.7647\n","Epoch: 28/30... Step: 20560... Loss: 1.2434... Val Loss: 1.7373\n","Epoch: 28/30... Step: 20570... Loss: 1.2530... Val Loss: 1.7128\n","Epoch: 28/30... Step: 20580... Loss: 1.2543... Val Loss: 1.7699\n","Epoch: 28/30... Step: 20590... Loss: 1.2491... Val Loss: 1.6853\n","Epoch: 28/30... Step: 20600... Loss: 1.2580... Val Loss: 1.6980\n","Epoch: 28/30... Step: 20610... Loss: 1.2823... Val Loss: 1.7114\n","Epoch: 28/30... Step: 20620... Loss: 1.2540... Val Loss: 1.7483\n","Epoch: 28/30... Step: 20630... Loss: 1.2362... Val Loss: 1.6994\n","Epoch: 28/30... Step: 20640... Loss: 1.2244... Val Loss: 1.7041\n","Epoch: 28/30... Step: 20650... Loss: 1.2342... Val Loss: 1.7575\n","Epoch: 28/30... Step: 20660... Loss: 1.2705... Val Loss: 1.6532\n","Epoch: 28/30... Step: 20670... Loss: 1.2542... Val Loss: 1.7326\n","Epoch: 28/30... Step: 20680... Loss: 1.2695... Val Loss: 1.7018\n","Epoch: 28/30... Step: 20690... Loss: 1.2562... Val Loss: 1.6770\n","Epoch: 28/30... Step: 20700... Loss: 1.2439... Val Loss: 1.7137\n","Epoch: 28/30... Step: 20710... Loss: 1.2896... Val Loss: 1.6977\n","Epoch: 28/30... Step: 20720... Loss: 1.2421... Val Loss: 1.7066\n","Epoch: 28/30... Step: 20730... Loss: 1.2676... Val Loss: 1.6892\n","Epoch: 28/30... Step: 20740... Loss: 1.2535... Val Loss: 1.7122\n","Epoch: 28/30... Step: 20750... Loss: 1.2438... Val Loss: 1.7216\n","Epoch: 28/30... Step: 20760... Loss: 1.2590... Val Loss: 1.7173\n","Epoch: 28/30... Step: 20770... Loss: 1.2400... Val Loss: 1.6825\n","Epoch: 28/30... Step: 20780... Loss: 1.2516... Val Loss: 1.7311\n","Epoch: 28/30... Step: 20790... Loss: 1.2700... Val Loss: 1.7284\n","Epoch: 28/30... Step: 20800... Loss: 1.2497... Val Loss: 1.7355\n","Epoch: 28/30... Step: 20810... Loss: 1.2563... Val Loss: 1.7022\n","Epoch: 28/30... Step: 20820... Loss: 1.2616... Val Loss: 1.7086\n","Epoch: 28/30... Step: 20830... Loss: 1.2552... Val Loss: 1.7448\n","Epoch: 28/30... Step: 20840... Loss: 1.2404... Val Loss: 1.6928\n","Epoch: 28/30... Step: 20850... Loss: 1.2678... Val Loss: 1.7088\n","Epoch: 28/30... Step: 20860... Loss: 1.2625... Val Loss: 1.7191\n","Epoch: 28/30... Step: 20870... Loss: 1.2327... Val Loss: 1.7066\n","Epoch: 28/30... Step: 20880... Loss: 1.2317... Val Loss: 1.6907\n","Epoch: 28/30... Step: 20890... Loss: 1.2546... Val Loss: 1.6888\n","Epoch: 28/30... Step: 20900... Loss: 1.2269... Val Loss: 1.7363\n","Epoch: 28/30... Step: 20910... Loss: 1.2347... Val Loss: 1.7005\n","Epoch: 28/30... Step: 20920... Loss: 1.2526... Val Loss: 1.6948\n","Epoch: 28/30... Step: 20930... Loss: 1.2382... Val Loss: 1.6951\n","Epoch: 28/30... Step: 20940... Loss: 1.2435... Val Loss: 1.7074\n","Epoch: 29/30... Step: 20950... Loss: 1.2490... Val Loss: 1.8302\n","Epoch: 29/30... Step: 20960... Loss: 1.2265... Val Loss: 1.8315\n","Epoch: 29/30... Step: 20970... Loss: 1.2667... Val Loss: 1.8149\n","Epoch: 29/30... Step: 20980... Loss: 1.2318... Val Loss: 1.8063\n","Epoch: 29/30... Step: 20990... Loss: 1.2752... Val Loss: 1.7939\n","Epoch: 29/30... Step: 21000... Loss: 1.2519... Val Loss: 1.8058\n","Epoch: 29/30... Step: 21010... Loss: 1.2468... Val Loss: 1.8000\n","Epoch: 29/30... Step: 21020... Loss: 1.2656... Val Loss: 1.7872\n","Epoch: 29/30... Step: 21030... Loss: 1.2625... Val Loss: 1.8102\n","Epoch: 29/30... Step: 21040... Loss: 1.2335... Val Loss: 1.8091\n","Epoch: 29/30... Step: 21050... Loss: 1.2455... Val Loss: 1.8087\n","Epoch: 29/30... Step: 21060... Loss: 1.2515... Val Loss: 1.8142\n","Epoch: 29/30... Step: 21070... Loss: 1.2435... Val Loss: 1.8168\n","Epoch: 29/30... Step: 21080... Loss: 1.2323... Val Loss: 1.8107\n","Epoch: 29/30... Step: 21090... Loss: 1.2405... Val Loss: 1.7899\n","Epoch: 29/30... Step: 21100... Loss: 1.2657... Val Loss: 1.8341\n","Epoch: 29/30... Step: 21110... Loss: 1.2541... Val Loss: 1.8259\n","Epoch: 29/30... Step: 21120... Loss: 1.2499... Val Loss: 1.8087\n","Epoch: 29/30... Step: 21130... Loss: 1.2659... Val Loss: 1.8151\n","Epoch: 29/30... Step: 21140... Loss: 1.2546... Val Loss: 1.8279\n","Epoch: 29/30... Step: 21150... Loss: 1.2726... Val Loss: 1.8315\n","Epoch: 29/30... Step: 21160... Loss: 1.2399... Val Loss: 1.8050\n","Epoch: 29/30... Step: 21170... Loss: 1.2542... Val Loss: 1.7896\n","Epoch: 29/30... Step: 21180... Loss: 1.2784... Val Loss: 1.7952\n","Epoch: 29/30... Step: 21190... Loss: 1.2604... Val Loss: 1.8067\n","Epoch: 29/30... Step: 21200... Loss: 1.2650... Val Loss: 1.8079\n","Epoch: 29/30... Step: 21210... Loss: 1.2577... Val Loss: 1.8128\n","Epoch: 29/30... Step: 21220... Loss: 1.2543... Val Loss: 1.8018\n","Epoch: 29/30... Step: 21230... Loss: 1.2541... Val Loss: 1.7972\n","Epoch: 29/30... Step: 21240... Loss: 1.2711... Val Loss: 1.8229\n","Epoch: 29/30... Step: 21250... Loss: 1.2612... Val Loss: 1.8047\n","Epoch: 29/30... Step: 21260... Loss: 1.2569... Val Loss: 1.8004\n","Epoch: 29/30... Step: 21270... Loss: 1.2479... Val Loss: 1.8084\n","Epoch: 29/30... Step: 21280... Loss: 1.2290... Val Loss: 1.8170\n","Epoch: 29/30... Step: 21290... Loss: 1.2348... Val Loss: 1.7816\n","Epoch: 29/30... Step: 21300... Loss: 1.2779... Val Loss: 1.8035\n","Epoch: 29/30... Step: 21310... Loss: 1.2484... Val Loss: 1.8193\n","Epoch: 29/30... Step: 21320... Loss: 1.2450... Val Loss: 1.7856\n","Epoch: 29/30... Step: 21330... Loss: 1.2313... Val Loss: 1.8127\n","Epoch: 29/30... Step: 21340... Loss: 1.2248... Val Loss: 1.8154\n","Epoch: 29/30... Step: 21350... Loss: 1.2159... Val Loss: 1.8138\n","Epoch: 29/30... Step: 21360... Loss: 1.2443... Val Loss: 1.7920\n","Epoch: 29/30... Step: 21370... Loss: 1.2518... Val Loss: 1.8229\n","Epoch: 29/30... Step: 21380... Loss: 1.2520... Val Loss: 1.8214\n","Epoch: 29/30... Step: 21390... Loss: 1.2390... Val Loss: 1.7847\n","Epoch: 29/30... Step: 21400... Loss: 1.2543... Val Loss: 1.8185\n","Epoch: 29/30... Step: 21410... Loss: 1.2536... Val Loss: 1.8049\n","Epoch: 29/30... Step: 21420... Loss: 1.2508... Val Loss: 1.7831\n","Epoch: 29/30... Step: 21430... Loss: 1.2324... Val Loss: 1.8152\n","Epoch: 29/30... Step: 21440... Loss: 1.2609... Val Loss: 1.8186\n","Epoch: 29/30... Step: 21450... Loss: 1.2166... Val Loss: 1.8173\n","Epoch: 29/30... Step: 21460... Loss: 1.2425... Val Loss: 1.7932\n","Epoch: 29/30... Step: 21470... Loss: 1.2399... Val Loss: 1.8125\n","Epoch: 29/30... Step: 21480... Loss: 1.2347... Val Loss: 1.8235\n","Epoch: 29/30... Step: 21490... Loss: 1.2639... Val Loss: 1.8066\n","Epoch: 29/30... Step: 21500... Loss: 1.2623... Val Loss: 1.8041\n","Epoch: 29/30... Step: 21510... Loss: 1.2291... Val Loss: 1.8039\n","Epoch: 29/30... Step: 21520... Loss: 1.2495... Val Loss: 1.7962\n","Epoch: 29/30... Step: 21530... Loss: 1.2226... Val Loss: 1.8047\n","Epoch: 29/30... Step: 21540... Loss: 1.2481... Val Loss: 1.8102\n","Epoch: 29/30... Step: 21550... Loss: 1.2472... Val Loss: 1.8139\n","Epoch: 29/30... Step: 21560... Loss: 1.2474... Val Loss: 1.7919\n","Epoch: 29/30... Step: 21570... Loss: 1.2547... Val Loss: 1.8202\n","Epoch: 29/30... Step: 21580... Loss: 1.2561... Val Loss: 1.8253\n","Epoch: 29/30... Step: 21590... Loss: 1.2684... Val Loss: 1.8091\n","Epoch: 29/30... Step: 21600... Loss: 1.2512... Val Loss: 1.8021\n","Epoch: 29/30... Step: 21610... Loss: 1.2594... Val Loss: 1.8152\n","Epoch: 29/30... Step: 21620... Loss: 1.2194... Val Loss: 1.8105\n","Epoch: 29/30... Step: 21630... Loss: 1.2379... Val Loss: 1.8145\n","Epoch: 29/30... Step: 21640... Loss: 1.2483... Val Loss: 1.8030\n","Epoch: 29/30... Step: 21650... Loss: 1.2453... Val Loss: 1.8101\n","Epoch: 29/30... Step: 21660... Loss: 1.2448... Val Loss: 1.8046\n","Epoch: 29/30... Step: 21670... Loss: 1.2036... Val Loss: 1.8027\n","Epoch: 29/30... Step: 21680... Loss: 1.2289... Val Loss: 1.7631\n","Epoch: 29/30... Step: 21690... Loss: 1.2401... Val Loss: 1.8385\n","Epoch: 30/30... Step: 21700... Loss: 1.2370... Val Loss: 1.6995\n","Epoch: 30/30... Step: 21710... Loss: 1.2570... Val Loss: 1.6836\n","Epoch: 30/30... Step: 21720... Loss: 1.2561... Val Loss: 1.6683\n","Epoch: 30/30... Step: 21730... Loss: 1.2635... Val Loss: 1.6949\n","Epoch: 30/30... Step: 21740... Loss: 1.2614... Val Loss: 1.6888\n","Epoch: 30/30... Step: 21750... Loss: 1.2802... Val Loss: 1.7042\n","Epoch: 30/30... Step: 21760... Loss: 1.2316... Val Loss: 1.7019\n","Epoch: 30/30... Step: 21770... Loss: 1.2959... Val Loss: 1.6792\n","Epoch: 30/30... Step: 21780... Loss: 1.2353... Val Loss: 1.6994\n","Epoch: 30/30... Step: 21790... Loss: 1.2346... Val Loss: 1.7385\n","Epoch: 30/30... Step: 21800... Loss: 1.2591... Val Loss: 1.6845\n","Epoch: 30/30... Step: 21810... Loss: 1.2298... Val Loss: 1.7164\n","Epoch: 30/30... Step: 21820... Loss: 1.2509... Val Loss: 1.7336\n","Epoch: 30/30... Step: 21830... Loss: 1.2423... Val Loss: 1.7428\n","Epoch: 30/30... Step: 21840... Loss: 1.2331... Val Loss: 1.7237\n","Epoch: 30/30... Step: 21850... Loss: 1.2447... Val Loss: 1.7421\n","Epoch: 30/30... Step: 21860... Loss: 1.2519... Val Loss: 1.6630\n","Epoch: 30/30... Step: 21870... Loss: 1.2717... Val Loss: 1.7327\n","Epoch: 30/30... Step: 21880... Loss: 1.2806... Val Loss: 1.7219\n","Epoch: 30/30... Step: 21890... Loss: 1.2557... Val Loss: 1.7450\n","Epoch: 30/30... Step: 21900... Loss: 1.2449... Val Loss: 1.7787\n","Epoch: 30/30... Step: 21910... Loss: 1.2397... Val Loss: 1.7852\n","Epoch: 30/30... Step: 21920... Loss: 1.2516... Val Loss: 1.7435\n","Epoch: 30/30... Step: 21930... Loss: 1.2587... Val Loss: 1.7688\n","Epoch: 30/30... Step: 21940... Loss: 1.2406... Val Loss: 1.7277\n","Epoch: 30/30... Step: 21950... Loss: 1.2395... Val Loss: 1.7062\n","Epoch: 30/30... Step: 21960... Loss: 1.2382... Val Loss: 1.7631\n","Epoch: 30/30... Step: 21970... Loss: 1.2432... Val Loss: 1.7166\n","Epoch: 30/30... Step: 21980... Loss: 1.2585... Val Loss: 1.7205\n","Epoch: 30/30... Step: 21990... Loss: 1.2539... Val Loss: 1.7617\n","Epoch: 30/30... Step: 22000... Loss: 1.2522... Val Loss: 1.7584\n","Epoch: 30/30... Step: 22010... Loss: 1.2643... Val Loss: 1.7532\n","Epoch: 30/30... Step: 22020... Loss: 1.2283... Val Loss: 1.7525\n"]}]},{"cell_type":"code","source":["def predict(net, char, h=None, top_k=None):\n","        ''' Given a character, predict the next character.\n","            Returns the predicted character and the hidden state.\n","        '''\n","        \n","        # tensor inputs\n","        x = np.array([[net.char2int[char]]])\n","        x = one_hot_encode(x, len(net.chars))\n","        inputs = torch.from_numpy(x)\n","        \n","        if(train_on_gpu):\n","            inputs = inputs.cuda()\n","        \n","        # detach hidden state from history\n","        h = tuple([each.data for each in h])\n","        # get the output of the model\n","        out, h = net(inputs, h)\n","        # get the character probabilities\n","        p = F.softmax(out, dim=1).data\n","        if(train_on_gpu):\n","            p = p.cpu() # move to cpu\n","        \n","        # get top characters\n","        if top_k is None:\n","            top_ch = np.arange(len(net.chars))\n","        else:\n","            p, top_ch = p.topk(top_k)\n","            top_ch = top_ch.numpy().squeeze()\n","        \n","        # select the likely next character with some element of randomness\n","        p = p.numpy().squeeze()\n","        char = np.random.choice(top_ch, p=p/p.sum())\n","        \n","        # return the encoded value of the predicted char and the hidden state\n","        return net.int2char[char], h\n","def sample(net, size, prime='The', top_k=None):\n","        \n","    if(train_on_gpu):\n","        net.cuda()\n","    else:\n","        net.cpu()\n","    \n","    net.eval() # eval mode\n","    \n","    # First off, run through the prime characters\n","    chars = [ch for ch in prime]\n","    h = net.init_hidden(1)\n","    for ch in prime:\n","        char, h = predict(net, ch, h, top_k=top_k)\n","    chars.append(char)\n","    \n","    # Now pass in the previous character and get a new one\n","    for ii in range(size):\n","        char, h = predict(net, chars[-1], h, top_k=top_k)\n","        chars.append(char)\n","    return ''.join(chars)"],"metadata":{"id":"lXjooCo0FPEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# change the name, for saving multiple files\n","model_name = 'poem_4_epoch.net'\n","checkpoint = {'n_hidden': net.n_hidden,\n","              'n_layers': net.n_layers,\n","              'state_dict': net.state_dict(),\n","              'tokens': net.chars}\n","with open(model_name, 'wb') as f:\n","    torch.save(checkpoint, f)"],"metadata":{"id":"LHUP2K0sFHY0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"25td9lyOqMoC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implementation ends here"],"metadata":{"id":"SB_ULU0MrynK"}},{"cell_type":"markdown","source":["##OUTPUT PROCESSING FOR GRUEN"],"metadata":{"id":"w5kSG-g-TTGa"}},{"cell_type":"code","source":["#primes used - nature, shadow, beauty, family, autumn, a, e, i, o, u"],"metadata":{"id":"UKXHtIwHT_Z8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = sample(net, 20000, prime='nature', top_k=2)"],"metadata":{"id":"2u0SO5CzKKDj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LH00OwYWKNcN","executionInfo":{"status":"ok","timestamp":1669407169197,"user_tz":300,"elapsed":46,"user":{"displayName":"Namita Shukla","userId":"15938957227034652948"}},"outputId":"05e1adc7-3bcb-4e4b-b720-7c1ce44bfd37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nature/are the street the courteous/words to see the sand$\n","in the summer star/the shape of the complaining/of the sun in the car$\n","summer rain/the shadows of the companions are/some strangers are stars$\n","the stranger stands through a star/a star of an old man was an anticipate/and so much they will be there$\n","then to their street too the same thing in the shore/and the street are the beauty to see the stars on the window/a star is to seek to think the stars of a street$\n","a street and the strain of this war that i was so tired and some somethen the same work/is an answer to this soul i have seen the state of all/the wind is something that is that i have said that they're so so funny and strength$\n","and it is a beautiful stars/and the wind was the stream of the strangers/and start to the sound of the window on the shore$\n","a strange of the straight of the stars of the shadows/of a straight of the colors of a colour of the word/and the streets on the shore of a stars are so soft$\n","the world's stars/is there the wind into the world/with a common stars$\n","that is the star to send the stars/and the cold will be a stand of the world/the winds are sometimes the streets are that$\n","a little silent the stream of the shadows of the shade/of the stars of a character of this stars/and stands are so stressed on the shore$\n","a stream of the streams of a star is to start/the song that shall be to say that the sun was a bloom and stars/to their shadows and strangers that shared to trust the wind$\n","then it was the stars of the shadow/of a strain the wind is all that shall stay/and the world with a star the world in the shower of the wind$\n","a cold streets of the shadow of an all the streams of the stars are still and some strander/than a single part of the streets and the colour in a shape/of their heads and the companion to streets to travel and the street$\n","to the stars of the street and sheet of a strange this stars that stood and the shore and stars/too many things and then the wind was a breath of the shadow/the wind is so far there's a stream of song to the shade of a company$\n","then that the street is still the street and standard stars and stars and states and shadows/of the wild showers of the will the companion/will stay through the shore to stand$\n","then in the strange of the stairs and the cares of the strangers/to the stars of a cold star the shore/of an and the sun is a silent path of the stars$\n","the street was the street the wind with the cold wind/and stars to the shorter the sun will stand/the wind and the car and to the stair$\n","the sun in the shadows and to tree/and still too stress and thing in the shadow/i have been to the streets of my son$\n","the stream of the stars in the window/and the stars and stars are so sad/and to the straight shadow of the shadow$\n","the winds of the common street/and the cold water in a shower/is something in the window$\n","the sun is all the same/the stars of the stain in the windows are/a black stars of the stranger stands to the cold$\n","then the streams are so sad/that the stars are strange that i will be/a single statue of the sound of the short$\n","the star of the window is to the car/and the stain will be a strange thing and so much/and it was an arm and stared at the same shadow$\n","a stars and the streets of the shadow of the shore/and starts a star the shore of traces/and starts and stream that should be a silence$\n","and the strains of a shadow on the street and/strange stars and shorts and showers and shadows of/to see itself and that is the same$\n","a star is a standing of a shadow/and the conversation is the stars of a stream/the world is the sad stations of the strange of the world$\n","then i stood the same shadow of a star/the contendent to the song the shadow of a chain of treasure/in a communication of their shoulders and straights and the straight on the world$\n","to the stars of the stars and the cold/the star of a star is always the street/the sun is always a single blossom$\n","and the wild stars/and the candle of the stream in the wall/of a cold waters and the cold winds$\n","a compassional thoughts/to stop the state of a bloom/and the sun will stand the shadow$\n","the soul of the way to the street and/the care is so tired and still/and they are the same the strangers$\n","and it was a beauty of a broken shadow/the stains of the wind and the street are the best friends of the shape of the stars/the summer stars to this shore$\n","and that is the band and shadow of the stars of the show/they stand to their shadows and towers and star/and stand of the world and to see the wind$\n","and then i said the stars/and shells and shadows of the stars and shapes/and stars are the sad song of the wind$\n","the stars of a street shall be the same and sometimes/the street is the saddest thing that is a little bird/and that is always there and start to the same$\n","the word is that the state of the short/is the boys and things/that is all all the sun was still the stars$\n","a little bird and the stream is the same thing/the sun will be the body of the will/that touch the stars of town and the world is to the same$\n","and the star of the soul is the stars of the shadow/of an angel of a shadow and the course/of the street the street is the same of the stars$\n","the world is still the stream of the shadow of the wind/and standing at the straight of the candle to the stains/the sun is still and straight the wind of the shadow$\n","the stars are strength of the stars/the way to see her the star/the stars of a stream their stream of stars$\n","a cold shortest shower of the short on the stars/the winds are so starting the window and the colors/of the stain they was the sad straight of the woods and shadows$\n","the soul is so stressed and still a stream of song and the shadows of the wind/and she still had to be a street and the world in the streets and stars/in a charge of sunshine is the stars of the window and the shadow is so far$\n","the words who will stand and see to the street/and to stand and still the stars and shadows are thanks/for the passion of the soul the street in the stars$\n","and the song is the strange think to the past/a straight of the shadow of the street shall see/the words that start to treat the world as that she starts and the world is so far$\n","and i have started the soul of the shore/and that's the same soul i have to see/and start to stare this thing to see you all and start the world$\n","the soul in a child was a single star/the world is a stream of the world and they was so so so funny the star/is all to stop and sometimes the winds of the strange straight the stars$\n","and it's a song of the strain that thou art the world and some thing/is a sound of some thing to stop to see/and i will stop and the streets of the soul is the same$\n","then i stand and the words are so sad that it is/a stranger to the soul of a band of the wind in a commute/to the poem and something in and stand to stay$\n","the world was the beauty of the shadow of the shower they said/that the wild shadow of to stand the show in the world when the stars are still/and that i saw the shadows of an old shade of a stars and the word that stands and strange to see/and the wind will be the best than a single shadow$\n","a star to straight and start the straight of their way to the wind/that shall see the will the changing shore is sometimes and the word/is the beauty of a street that the strain of the shapes of the shore of the woods$\n","and that the strangers are straight the streams of a brown chinese through/the car started and stand the window of the words/the woods are so strength to see that she was a silent street$\n","and i will see the stars of the stars/that start to start to the stars/to see in the shade of the shore$\n","and the stars and songs and their hearts was still and the stars/the star in a shape of the shadow of the stars in/their strange streets and to see them the show of the stars$\n","then in a stream to see the stars and the shadow/of the stars are still an angel/to stand and the sun was still a shadow of the stars$\n","to the street and the content in the strange of the woods are so strength/the women is the strain/of a state to the song of their things and so much the way$\n","i have been a stream of my love the wind was an androw of a star/the star is the same thing in the world is all/the sunshine is so sad to see it to the sound$\n","and then the sun will stop the shower to the care/and the street is the same and the sun is so sorrow/and i had to be there to be a long time$\n","a little book of the song in the wind was a breast/to sent the street the sun is so sorry to the poor/they are so so so sorry to the soul of song$\n","a little sink and stars and the cold stars/they without a straight the wild this world will be there/to see the world i have bought a street and the saddest thing in the same$\n","the sun will be the street and sometimes the sun/was a breath of the shadow of the stain of the street/the streets are still a star of the street the world$\n","and the words of the stars of the shadow is a star the world/who started to this soul of the streets and shows/as its chance of their shapes are so sad then i stared to the same$\n","and that is any thing that the surprise/is a son of the star of the stars of a black/state of the stars and this world is that i still hear to see the streets and the straight$\n","the song of the stars is the same the sun/in a child is a stream of a short of short/and to the stars and this streets are so sorry to the stars$\n","the world was and the star is a bare thanks for to see/the streets of the stars of the world is started to/the poem that is a broken shadow of the stars$\n","and too much i saw the street the stream of the star/in a chance of the wind will be and the world is so much both/that i have seen a little single show$\n","the woods that should stop the sad things and/so shall they are so the same/and the street is starting to be and that the world was a stranger$\n","and i want to see that something that i said/the shadow is a bad thing that's the barn too/much in an antician thing in the world$\n","i had an explaining and something is all the stars and souls are the same/then they said that the stars/is a pretty part of the shape of their stars than this to start to stand$\n","the wild things are to sent and the same thing/i have to be a single son the world is/a single short the world who was there the same$\n","the streams are the stars of an end/the sun is all and strange and the world/who was a street and the sun will be the best of$\n","the state of the strange shall stand and start the world/and then they will be the stream/to see the world to see the shadow$\n","the stream of the stars and shows are the stars of the shower/and the words of a station of the world in a cold stranger should have/been to their hearts and standing and strains of this straight start$\n","to the poetry of all the winds of the short shadow/of a stream that i wanted to be/a silent thought of the soul to see$\n","and the star in a shadow/is and that is a blossom/of the window as this is a stranger$\n","to some shadow of the shadow/that should be the best that i will see/and then that the streets of the world$\n","i saw the street that i will be/to sell this touchest of my heart/and still the wind is to stand$\n","and i have so much the same shining/and the sunshine is the same/that is the best of them$\n","the stars is all there is not the same and the stream/of this shall be a single shadow of the strangers/and the sunshine of the world was there$\n","that i saw the shadow of the shore of the shower/the stream of thine too there will be to the part/the street are the same thing to be a single stream$\n","and the stream is so sad and something is always/the same and some shadow of their shoulders and/this stream is so far through the shadows of song$\n","and the word of their way to straight/and the stars in the shadows of the shadow/of the shower is still a stream$\n","the songs of an easter world is/an angel of the stars of the stars/and the care is the standard of the show$\n","and the sun will see the shore of the shadow/of the walls and stars around/to send to the sound of sunshine$\n","the wind is still and straight at the same/the woods are their stream and to travel and shadow/the shore of the shore$\n","a shadow and stars and streets are soul/too many the same streams are some street to/sending the shadows$\n","the winds and stars are some star/a strain of the communication/of the sun in the stream of the shadows$\n","the sun with the stars of the street shadow/the stars and their streets that will stop/the stream that still has been$\n","a bank of a blanket is too/shall bring and stream the stream of/the cold shore of the strander$\n","a cold shadow of the stars/and shall be the street and this shoulder/and she was so far away$\n","and it is to say/it will be a book and should this/in the street that is the same$\n","the sun will be a bare/the world is a stream of the streets/and the stars of the shade of the shadow$\n","to the song on the stars/that was the stars the sun will be/the saddest thing to do$\n","a streaming show of their stars and the star/and a child is standing at the stream of the short/to stop and then the stranger was an and the wine is still the star$\n","to the stars and the car and the soul of the wind/is starting to see the woods of the world is a boy/to the song of a charging stars and the caress of the streets are the same$\n","and the stars of the shore internation is all and the same shall still have/their shades and the wind and the can translate the words that shall be a star/to their shoulder towards them the soul of a children's strange to the soul that starts to be a single stranger than the star$\n","i have so much they still stand the street and the world is standing and the station of the shore/is the same and the willow stands of the wind is so stressed/to see the stars of the street the street is to the street and the stream$\n","and the song of a stream in the windows and the shades of a stars/that was the same things that the sun is/still a cold wind and to the stars$\n","the sun is so sad and so stand and some they said/to myself to see that the street is so many/and that i will see the same$\n","then the will be/a stream in the window is the same and still/to stand and the stream of the shadow of their streets are the best things to see the sand$\n","the stars are the same the summer way to see the stream/the streets are strength of the soul of the wind and strain and/the cold shadow of a cold way the street is so fair to the soul$\n","a star took a broken show that start the shadow of the wind/and they still started the streets of an angel they will/still have to see the street and strain to treat an all the streets of too little souls$\n","the sound of the stars are strengthening and to the star/and a show of a character of the street and the cold stars/and she starts to see that shade and too start at the street$\n","and the words of this stream is to the stars on the wind/and the stairs are so thankful to the song/of their shoulder to the sound of the stars$\n","then i was a single short of the state to the can/the beautiful streams of the wind and street and star/and a stars and stars and shore of a common shadow$\n","the streets of the stars are the standing of a cold way/a companion with the streets of the stars are the same/that they are still a little boy to see him to see the wind$\n","and i hold the shadow of a barrel and the country and the color/of the shadow is a silent thing to see/and that the stream is the body$\n","thank you to my sister to stay/a star the shadow of an angel is too mary/to the stars on the world and so that the stars$\n","the wind was a stream/and the straight of a shadow is a star/a shadowy trace of the world$\n","and the street was a strange tower/and start to see and start to see the shower/they will breathe them and took to the street$\n","a shadow of the showers are still/the stars and start to stream to/the cold shadows of the shadow$\n","the wind in the shadow in the shadow/of the shower in the window and the corn/the controller shall be$\n","a state/of the street stars that was so sad and/so many things are so so sad$\n","the streets are the border of the stream/and the sun is a silence/of the star of the straight stars$\n","and then i said/i have seen the wind of the shade/of the shadow of the woods$\n","to the stream of a shadow and star/that started the can of street/and the stream of the wood is some things$\n","the stars/and the sun is something to be an end/of to their stars and stars are starting to be so full$\n","the sound of a stream in a community/is a street to see a stream/that i will be so far there$\n","a shadow and the song in the street/are their shadows of sun and the shower/in the street and soul in the shore of the wind$\n","the sunset with their stars the stars of the streets/to this shorten the sun in to the corner of the star/they with a streen the street and the strange shadows and stands$\n","a stream of this shadow/of the shore the windows of a shadow/a commuter street and the stars of the stars$\n","the street should have been a star and to the star/a shadow of the streams of the window of the wind/in the straight shadow of the shore$\n","and the sun is all the wind/in a shadow/of the streets and the common shows$\n","a couple of the short of the color/the cold streams of the stair of the stars/is all the streams and the wind is so far$\n","the stream in an orange stars and shadows/on the stars of the stars/and stans and the street shadows of the wind$\n","and the wind was still the ball of the street/they stand three things are so sad then to stop/that the strain off the shadow of the wind$\n","the sun is a street and soul to see the stream/of a star that i will stay and then i will be the sad to start/a line of soul and this way i wish that i had a shadow on the shadow$\n","a strange straight to the sound of the world is to see/they will stand and then i will stop the street/to stand and stand on their shoes they will stand a lot to see$\n","it's a standard the world is the strange the stars on the stars/the wind is still and i should be a single star/the sunset will be the boys$\n","the stranger was to their/should be the strangers that i want to bring them the world/and still want to be the beautiful stars$\n","and the world in the wind is still/a star in the short and the sun was still the world where they will/be the same the world is standing and the sun with the common$\n","and the world was a star there is/a son the window of the stairs/which is a silver to the candle$\n","the streets/and the sun was stir and the street/in the way to the street the stars$\n","to the street that starts the shadow/the stream of the wind is started the stars/and the contemplated show$\n","the sun in the street was a book/and the straight shore of the shore streams/and the stars and streams and shadows of the stars$\n","the stars and the shadow of a star/is a blood of the careful shape/of the sun the way that was the book$\n","to this time of the world/with an angel of the shore/the words of the shadows of the show$\n","the sun is so so sorrow/and that is the stream of the wind/the stream of a shadow of the shade$\n","and the sounds of the wind and the shore/and streams are strength of the wind/the strange of the shows of a shadow$\n","the stream of a star the wind is still/a cold wind is the book/of the sun and the shadow of the shape$\n","the streams of the wind with a street/a street and sound as the sun was a star/a child while the sun is a street$\n","the stranger was an all things/that the sunshine is the boy of the world/and i was a street and so much to be strength$\n","the world is so sad and something is always/the same and so shall i stop/anyway to the wind of my head$\n","and the soul in the window is still the band of the world/when i was a barn and too like a book and to see/a long time in a street and the stars of a cold street in the stars$\n","and the wild street and sounds of their shadows and towers and shades of trains/the street and songs and shadows are the same the stream/on the water of the stain of the world's shore and soul$\n","the soul is something that was a single beautiful stream/of the street th\n"]}]},{"cell_type":"code","source":["haikus = output.split('$\\n')"],"metadata":{"id":"6m9Ln5heqJTf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["haiku_syllable_wise = [];\n","for h in haikus:\n","  temp_h = h.split('/')\n","  haiku_syllable_wise.append(temp_h)\n","\n","  "],"metadata":{"id":"VKGIYF0UrFC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df = pd.DataFrame(haiku_syllable_wise)\n","df = pd.DataFrame(haiku_syllable_wise, columns =['sent_1', 'sent_2','sent_3']) \n"],"metadata":{"id":"E_Qkp0oZrqVS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"sXgw2vo_Sc8G","executionInfo":{"status":"ok","timestamp":1669406689340,"user_tz":300,"elapsed":1010,"user":{"displayName":"Namita Shukla","userId":"15938957227034652948"}},"outputId":"47bcd405-916d-45cd-be3f-e997fdc9ce7d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                     sent_1                          sent_2  \\\n","0                      uble       shadow is the best friend   \n","1           I wanna get out       of my life and I'm so sad   \n","2         I have a sendence    and I want the best for them   \n","3        I hate when people    see the same then staying at   \n","4      I won't be something          to be a star and I was   \n","..                      ...                             ...   \n","293   I'm still a stream to   the past to start the same as   \n","294       I haven't sent my        shit together to see you   \n","295      Today was the best         person to see the story   \n","296  This is the best thing         to do in my life I have   \n","297       I haven't been to                     anything an   \n","\n","                     sent_3  \n","0       and it was the same  \n","1           I want a shitty  \n","2         to be there to be  \n","3      me and stay too much  \n","4       starting to see you  \n","..                      ...  \n","293     the same time again  \n","294   a start of this shift  \n","295  I won't see this storm  \n","296     a bitch to stay too  \n","297                    None  \n","\n","[298 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-59258520-9dbb-490e-a359-545c86b2871d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent_1</th>\n","      <th>sent_2</th>\n","      <th>sent_3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>uble</td>\n","      <td>shadow is the best friend</td>\n","      <td>and it was the same</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I wanna get out</td>\n","      <td>of my life and I'm so sad</td>\n","      <td>I want a shitty</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I have a sendence</td>\n","      <td>and I want the best for them</td>\n","      <td>to be there to be</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>I hate when people</td>\n","      <td>see the same then staying at</td>\n","      <td>me and stay too much</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>I won't be something</td>\n","      <td>to be a star and I was</td>\n","      <td>starting to see you</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>293</th>\n","      <td>I'm still a stream to</td>\n","      <td>the past to start the same as</td>\n","      <td>the same time again</td>\n","    </tr>\n","    <tr>\n","      <th>294</th>\n","      <td>I haven't sent my</td>\n","      <td>shit together to see you</td>\n","      <td>a start of this shift</td>\n","    </tr>\n","    <tr>\n","      <th>295</th>\n","      <td>Today was the best</td>\n","      <td>person to see the story</td>\n","      <td>I won't see this storm</td>\n","    </tr>\n","    <tr>\n","      <th>296</th>\n","      <td>This is the best thing</td>\n","      <td>to do in my life I have</td>\n","      <td>a bitch to stay too</td>\n","    </tr>\n","    <tr>\n","      <th>297</th>\n","      <td>I haven't been to</td>\n","      <td>anything an</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>298 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59258520-9dbb-490e-a359-545c86b2871d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-59258520-9dbb-490e-a359-545c86b2871d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-59258520-9dbb-490e-a359-545c86b2871d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":139}]},{"cell_type":"code","source":["df = df[:-1]"],"metadata":{"id":"-FYJJDoSWCfD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv('/content/haiku_charrnn_output_.csv',columns = ['sent_1','sent_2','sent_3'])"],"metadata":{"id":"bqRZ6-E5sMWp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# csv_files = ['/content/haiku_charrnn_output.csv',\n","#              '/content/haiku_charrnn_output_2.csv',\n","#              '/content/haiku_charrnn_output_3.csv',\n","#              '/content/haiku_charrnn_output_4.csv',\n","#              '/content/haiku_charrnn_output_5.csv',\n","#              '/content/haiku_charrnn_output_6.csv',\n","#              '/content/haiku_charrnn_output_7.csv',\n","#              '/content/haiku_charrnn_output_8.csv',\n","#              '/content/haiku_charrnn_output_9.csv',\n","#              '/content/haiku_charrnn_output_10.csv']"],"metadata":{"id":"uiQKqSzjZtdO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df_append = pd.DataFrame()\n","# #append all files together\n","# for file in csv_files:\n","#             df_temp = pd.read_csv(file)\n","#             df_temp = df_temp[:-1]\n","#             df_append = df_append.append(df_temp, ignore_index=True)\n","# df_append"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":554},"id":"Cm3ON0hhZscJ","executionInfo":{"status":"ok","timestamp":1669406956509,"user_tz":300,"elapsed":9,"user":{"displayName":"Namita Shukla","userId":"15938957227034652948"}},"outputId":"6494ec16-9aa9-49e2-f149-5a860c568768"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Unnamed: 0                                             sent_1  \\\n","0              0                                        flowers was   \n","1              1                           a shadow and the colours   \n","2              2  and the sound of the straight strange thinks t...   \n","3              3       the sun is a silence the streets of a breeze   \n","4              4     that standing at the sun and the can of strand   \n","...          ...                                                ...   \n","1453         291                                      I wonder if I   \n","1454         292                                    I wanna see the   \n","1455         293                              I'm still a stream to   \n","1456         294                                  I haven't sent my   \n","1457         295                                 Today was the best   \n","\n","                                                 sent_2  \\\n","0                      a straight of a child and shadow   \n","1                        of a complex shaped to see the   \n","2                                is a stream that i saw   \n","3     in the windows on the stars and street and shaped   \n","4                               and so the summer stars   \n","...                                                 ...   \n","1453                          had a star to this season   \n","1454                    same thing that is all and that   \n","1455                      the past to start the same as   \n","1456                           shit together to see you   \n","1457                            person to see the story   \n","\n","                                                 sent_3  \n","0                                on a cold street stars  \n","1     stars and sheets and the streets are so sad an...  \n","2                the world to see the stars of the wind  \n","3      to start a barnes of street and the candle shade  \n","4                             and stars are still there  \n","...                                                 ...  \n","1453                               and I was still soon  \n","1454                                 I want is the same  \n","1455                                the same time again  \n","1456                              a start of this shift  \n","1457                             I won't see this storm  \n","\n","[1458 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-c21a9e85-c9af-4b17-95ee-3f92ef77e050\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>sent_1</th>\n","      <th>sent_2</th>\n","      <th>sent_3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>flowers was</td>\n","      <td>a straight of a child and shadow</td>\n","      <td>on a cold street stars</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>a shadow and the colours</td>\n","      <td>of a complex shaped to see the</td>\n","      <td>stars and sheets and the streets are so sad an...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>and the sound of the straight strange thinks t...</td>\n","      <td>is a stream that i saw</td>\n","      <td>the world to see the stars of the wind</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>the sun is a silence the streets of a breeze</td>\n","      <td>in the windows on the stars and street and shaped</td>\n","      <td>to start a barnes of street and the candle shade</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>that standing at the sun and the can of strand</td>\n","      <td>and so the summer stars</td>\n","      <td>and stars are still there</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1453</th>\n","      <td>291</td>\n","      <td>I wonder if I</td>\n","      <td>had a star to this season</td>\n","      <td>and I was still soon</td>\n","    </tr>\n","    <tr>\n","      <th>1454</th>\n","      <td>292</td>\n","      <td>I wanna see the</td>\n","      <td>same thing that is all and that</td>\n","      <td>I want is the same</td>\n","    </tr>\n","    <tr>\n","      <th>1455</th>\n","      <td>293</td>\n","      <td>I'm still a stream to</td>\n","      <td>the past to start the same as</td>\n","      <td>the same time again</td>\n","    </tr>\n","    <tr>\n","      <th>1456</th>\n","      <td>294</td>\n","      <td>I haven't sent my</td>\n","      <td>shit together to see you</td>\n","      <td>a start of this shift</td>\n","    </tr>\n","    <tr>\n","      <th>1457</th>\n","      <td>295</td>\n","      <td>Today was the best</td>\n","      <td>person to see the story</td>\n","      <td>I won't see this storm</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1458 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c21a9e85-c9af-4b17-95ee-3f92ef77e050')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c21a9e85-c9af-4b17-95ee-3f92ef77e050 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c21a9e85-c9af-4b17-95ee-3f92ef77e050');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":145}]},{"cell_type":"code","source":["# df_append.to_csv('/content/haiku_charrnn_final_output.csv',columns = ['sent_1','sent_2','sent_3'])"],"metadata":{"id":"yZs4jvbvZ-gK"},"execution_count":null,"outputs":[]}]}